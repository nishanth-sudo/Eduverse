

LLM Course documentation
Introduction
LLM Course
and get access to the augmented documentation experience
to get started
Introduction
Welcome to the ü§ó Course!
This course will teach you about large language models (LLMs) and natural language processing (NLP) using libraries from theHugging Faceecosystem ‚Äîü§ó Transformers,ü§ó Datasets,ü§ó Tokenizers, andü§ó Accelerate‚Äî as well as theHugging Face Hub.
We‚Äôll also cover libraries outside the Hugging Face ecosystem. These are amazing contributions to the AI community and incredibly useful tools.
It‚Äôs completely free and without ads.
Understanding NLP and LLMs
While this course was originally focused on NLP (Natural Language Processing), it has evolved to emphasize Large Language Models (LLMs), which represent the latest advancement in the field.
What‚Äôs the difference?
NLP (Natural Language Processing)is the broader field focused on enabling computers to understand, interpret, and generate human language. NLP encompasses many techniques and tasks such as sentiment analysis, named entity recognition, and machine translation.
LLMs (Large Language Models)are a powerful subset of NLP models characterized by their massive size, extensive training data, and ability to perform a wide range of language tasks with minimal task-specific training. Models like the Llama, GPT, or Claude series are examples of LLMs that have revolutionized what‚Äôs possible in NLP.
Throughout this course, you‚Äôll learn about both traditional NLP concepts and cutting-edge LLM techniques, as understanding the foundations of NLP is crucial for working effectively with LLMs.
What to expect?
Here is a brief overview of the course:
Chapters 1 to 4 provide an introduction to the main concepts of the ü§ó Transformers library. By the end of this part of the course, you will be familiar with how Transformer models work and will know how to use a model from theHugging Face Hub, fine-tune it on a dataset, and share your results on the Hub!
Chapters 5 to 8 teach the basics of ü§ó Datasets and ü§ó Tokenizers before diving into classic NLP tasks and LLM techniques. By the end of this part, you will be able to tackle the most common language processing challenges by yourself.
Chapter 9 goes beyond NLP to cover how to build and share demos of your models on the ü§ó Hub. By the end of this part, you will be ready to showcase your ü§ó Transformers application to the world!
Chapters 10 to 12 dive into advanced LLM topics like fine-tuning, curating high-quality datasets, and building reasoning models.
This course:
Requires a good knowledge of Python
Is better taken after an introductory deep learning course, such asfast.ai‚ÄôsPractical Deep Learning for Codersor one of the programs developed byDeepLearning.AI
Does not expect priorPyTorchorTensorFlowknowledge, though some familiarity with either of those will help
After you‚Äôve completed this course, we recommend checking out DeepLearning.AI‚ÄôsNatural Language Processing Specialization, which covers a wide range of traditional NLP models like naive Bayes and LSTMs that are well worth knowing about!
Who are we?
About the authors:
Abubakar Abidcompleted his PhD at Stanford in applied machine learning. During his PhD, he foundedGradio, an open-source Python library that has been used to build over 600,000 machine learning demos. Gradio was acquired by Hugging Face, which is where Abubakar now serves as a machine learning team lead.
Ben Burtenshawis a Machine Learning Engineer at Hugging Face. He completed his PhD in Natural Language Processing at the University of Antwerp, where he applied Transformer models to generate children stories for the purpose of improving literacy skills. Since then, he has focused on educational materials and tools for the wider community.
Matthew Carriganis a Machine Learning Engineer at Hugging Face. He lives in Dublin, Ireland and previously worked as an ML engineer at Parse.ly and before that as a post-doctoral researcher at Trinity College Dublin. He does not believe we‚Äôre going to get to AGI by scaling existing architectures, but has high hopes for robot immortality regardless.
Lysandre Debutis a Machine Learning Engineer at Hugging Face and has been working on the ü§ó Transformers library since the very early development stages. His aim is to make NLP accessible for everyone by developing tools with a very simple API.
Sylvain Guggeris a Research Engineer at Hugging Face and one of the core maintainers of the ü§ó Transformers library. Previously he was a Research Scientist at fast.ai, and he co-wroteDeep Learning for Coders with fastai and PyTorchwith Jeremy Howard. The main focus of his research is on making deep learning more accessible, by designing and improving techniques that allow models to train fast on limited resources.
Dawood Khanis a Machine Learning Engineer at Hugging Face. He‚Äôs from NYC and graduated from New York University studying Computer Science. After working as an iOS Engineer for a few years, Dawood quit to start Gradio with his fellow co-founders. Gradio was eventually acquired by Hugging Face.
Merve Noyanis a developer advocate at Hugging Face, working on developing tools and building content around them to democratize machine learning for everyone.
Lucile Saulnieris a machine learning engineer at Hugging Face, developing and supporting the use of open source tools. She is also actively involved in many research projects in the field of Natural Language Processing such as collaborative training and BigScience.
Lewis Tunstallis a machine learning engineer at Hugging Face, focused on developing open-source tools and making them accessible to the wider community. He is also a co-author of the O‚ÄôReilly bookNatural Language Processing with Transformers.
Leandro von Werrais a machine learning engineer in the open-source team at Hugging Face and also a co-author of the O‚ÄôReilly bookNatural Language Processing with Transformers. He has several years of industry experience bringing NLP projects to production by working across the whole machine learning stack..
FAQ
Here are some answers to frequently asked questions:
Does taking this course lead to a certification?Currently we do not have any certification for this course. However, we are working on a certification program for the Hugging Face ecosystem ‚Äî stay tuned!
Does taking this course lead to a certification?Currently we do not have any certification for this course. However, we are working on a certification program for the Hugging Face ecosystem ‚Äî stay tuned!
How much time should I spend on this course?Each chapter in this course is designed to be completed in 1 week, with approximately 6-8 hours of work per week. However, you can take as much time as you need to complete the course.
How much time should I spend on this course?Each chapter in this course is designed to be completed in 1 week, with approximately 6-8 hours of work per week. However, you can take as much time as you need to complete the course.
Where can I ask a question if I have one?If you have a question about any section of the course, just click on the ‚ÄùAsk a question‚Äù banner at the top of the page to be automatically redirected to the right section of theHugging Face forums:
Where can I ask a question if I have one?If you have a question about any section of the course, just click on the ‚ÄùAsk a question‚Äù banner at the top of the page to be automatically redirected to the right section of theHugging Face forums:
Note that a list ofproject ideasis also available on the forums if you wish to practice more once you have completed the course.
Where can I get the code for the course?For each section, click on the banner at the top of the page to run the code in either Google Colab or Amazon SageMaker Studio Lab:
The Jupyter notebooks containing all the code from the course are hosted on thehuggingface/notebooksrepo. If you wish to generate them locally, check out the instructions in thecourserepo on GitHub.
huggingface/notebooks
course
How can I contribute to the course?There are many ways to contribute to the course! If you find a typo or a bug, please open an issue on thecourserepo. If you would like to help translate the course into your native language, check out the instructionshere.
How can I contribute to the course?There are many ways to contribute to the course! If you find a typo or a bug, please open an issue on thecourserepo. If you would like to help translate the course into your native language, check out the instructionshere.
course
What were the choices made for each translation?Each translation has a glossary andTRANSLATING.txtfile that details the choices that were made for machine learning jargon etc. You can find an example for Germanhere.
What were the choices made for each translation?Each translation has a glossary andTRANSLATING.txtfile that details the choices that were made for machine learning jargon etc. You can find an example for Germanhere.
TRANSLATING.txt
Can I reuse this course?Of course! The course is released under the permissiveApache 2 license. This means that you must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use. If you would like to cite the course, please use the following BibTeX:
@misc{huggingfacecourse,
  author = {Hugging Face},
  title ={The Hugging Face Course, 2022},
  howpublished = "\url{https://huggingface.co/course}",
  year ={2022},
  note = "[Online; accessed<today>]"
}
Languages and translations
Thanks to our wonderful community, the course is available in many languages beyond English üî•! Check out the table below to see which languages are available and who contributed to the translations:
For some languages, thecourse YouTube videoshave subtitles in the language. You can enable them by first clicking theCCbutton in the bottom right corner of the video. Then, under the settings icon ‚öôÔ∏è, you can select the language you want by selecting theSubtitles/CCoption.
Let‚Äôs go üöÄ
Are you ready to roll? In this chapter, you will learn:
How to use thepipeline()function to solve NLP tasks such as text generation and classification
pipeline()
About the Transformer architecture
How to distinguish between encoder, decoder, and encoder-decoder architectures and use cases

The AI community building the future.
The platform where the machine learning community collaborates on models, datasets, and applications.
Models
Spaces
Generate any application with DeepSeek
Generate realistic dialogue from a script, using Dia!
Generate responses to user queries using conversation history
Edit an image based on the given instruction.
Text-to-3D and Image-to-3D Generation
Datasets
The Home of Machine Learning
Create, discover and collaborate on ML better.
The collaboration platform
Host and collaborate on unlimited public models, datasets and applications.
Move faster
With the HF Open source stack.
Explore all modalities
Text, image, video, audio or even 3D.
Build your portfolio
Share your work with the world and build your ML profile.
Accelerate your ML
We provide paid Compute and Enterprise solutions.
Compute
Deploy on optimizedInference Endpointsor update yourSpaces applicationsto a GPU in a few clicks.
Starting at $0.60/hour for GPU
Enterprise
Give your team the most advanced platform to build AI with enterprise-grade security, access controls and
			dedicated support.
Starting at $20/user/month
More than 50,000 organizations are using Hugging Face
Our Open Source
We are building the foundation of ML tooling with the community.
State-of-the-art ML for PyTorch, TensorFlow, JAX
State-of-the-art Diffusion models in PyTorch
Safe way to store/distribute neural network weights
Python client to interact with the Hugging Face Hub
Fast tokenizers optimized for research & production
Train transformers LMs with reinforcement learning
State-of-the-art ML running directly in your browser
Smol library to build great agents in Python
Parameter-efficient finetuning for large language models
Access & share datasets for any ML tasks
Serve language models with TGI optimized toolkit
Train PyTorch models with multi-GPU, TPU, mixed precision

Edit Models filters
Tasks
Libraries
Datasets
Languages
Licenses
Other
Models

Edit Datasets filters
Main
Tasks
Libraries
Languages
Licenses
Other
Datasets

Spaces
The AI App Directory
Edit an image based on the given instruction.
Describe image parts using masks
Customize characters with prompts and styles
In-browser local conversational AI inspired by 'Her'
Controllable Zero-Shot Voice Imitation
BAIDU's Reasoning LLM, https://yiyan.baidu.com/
An interactive digit classification demo
plug-and-play with visual concepts
Generate any application with DeepSeek
Generate realistic dialogue from a script, using Dia!
Generate responses to user queries using conversation history
Edit an image based on the given instruction.
Text-to-3D and Image-to-3D Generation
Describe image parts using masks
Customize characters with prompts and styles
Generate video from an image
Generate customized images using text and multiple images
Generate images from textual prompts
Try on virtual garments on person images
Replace characters in a video with characters in photos
New Ghibli EasyControl model is now released!!
Create your own AI comic with a single prompt
Remove backgrounds from images
Generate images from text prompts
Embedding Leaderboard
Execute custom code from environment variables
Generate realistic talking portrait videos from images and audio
Browse apps made with DeepSite
Generate high-quality images from text descriptions
Generate audio and video from text prompts
Generate a 3D model from an image
Generate images from text prompts

Join the community of Machine Learners and AI enthusiasts.
All HF Hub posts
mcp_server=True
launch()
importgradioasgrdefletter_counter(word, letter):"""Count the occurrences of a specific letter in a word.Args:word: The word or phrase to analyzeletter: The letter to count occurrences ofReturns:The number of times the letter appears in the word"""returnword.lower().count(letter.lower())

demo = gr.Interface(
    fn=letter_counter,
    inputs=["text","text"],
    outputs="number",
    title="Letter Counter",
    description="Count how many times a letter appears in a word")

demo.launch(mcp_server=True)
importgradioasgrdefletter_counter(word, letter):"""Count the occurrences of a specific letter in a word.Args:word: The word or phrase to analyzeletter: The letter to count occurrences ofReturns:The number of times the letter appears in the word"""returnword.lower().count(letter.lower())

demo = gr.Interface(
    fn=letter_counter,
    inputs=["text","text"],
    outputs="number",
    title="Letter Counter",
    description="Count how many times a letter appears in a word")

demo.launch(mcp_server=True)
1 reply
sdk_version
5.28
README.md
mcp_server=True
launch()
defgenerate(text, speed=1):"""Convert text to speech audio.Parameters:text (str): The input text to be converted to speech.speed (float, optional): Playback speed of the generated speech.
defgenerate(text, speed=1):"""Convert text to speech audio.Parameters:text (str): The input text to be converted to speech.speed (float, optional): Playback speed of the generated speech.
1 reply
2 replies
abidlabsAbubakar AbidFollow
AdinaYAdina YakefuFollow
ginipickginipickFollow
jsulzJared SulzdorfFollow
merveMerve NoyanFollow
merterbakMert ErbakFollow
sanaka87Ë∞¢ÈõÜFollow
lukmanajLukman Jibril AliyuFollow
fdaudensFlorent DaudensFollow
onekqYi CuiFollow

Documentations
Hub & Client Libraries
Hub
Host Git-based models, datasets, and Spaces on the HF Hub
Hub Python Library
Python client to interact with the Hugging Face Hub
Huggingface.js
JavaScript libraries for Hugging Face with built-in TS types
Tasks
Explore demos, models, and datasets for any ML tasks
Dataset viewer
API for metadata, stats, and content of HF Hub datasets
Deployment & Inference
Inference Providers
Call 200k+ models hosted by our 10+ Inference partners
Inference Endpoints (dedicated)
Deploy models on dedicated & fully managed infrastructure on HF
Amazon SageMaker
Train/deploy Transformers models with SageMaker/HF DLCs
Text Generation Inference
Serve language models with TGI optimized toolkit
Text Embeddings Inference
Serve embeddings models with TEI optimized toolkit
Core ML Libraries
Transformers
State-of-the-art ML for PyTorch, TensorFlow, JAX
Diffusers
State-of-the-art Diffusion models in PyTorch
Datasets
Access & share datasets for any ML tasks
Transformers.js
State-of-the-art ML running directly in your browser
Tokenizers
Fast tokenizers optimized for research & production
Evaluate
Evaluate and compare models performance
timm
State-of-the-art vision models: layers, optimizers, and utilities
Sentence Transformers
Embeddings, Retrieval, and Reranking
Training & Optimization
PEFT
Parameter-efficient finetuning for large language models
Accelerate
Train PyTorch models with multi-GPU, TPU, mixed precision
Optimum
Optimize HF Transformers for faster training/inference
AWS Trainium & Inferentia
Train/deploy Transformers/Diffusers on AWS
TRL
Train transformers LMs with reinforcement learning
Safetensors
Safe way to store/distribute neural network weights
Bitsandbytes
Optimize and quantize models with bitsandbytes
Lighteval
All-in-one toolkit to evaluate LLMs across multiple backends
Collaboration & Extras
Gradio
Build ML demos and web apps with a few lines of Python
smolagents
Smol library to build great agents in Python
AutoTrain
AutoTrain API and UI for seamless model training
Chat UI
Open source chat frontend powering HuggingChat
Leaderboards
Create custom Leaderboards on Hugging Face
Argilla
Collaboration tool for building high-quality datasets
Distilabel
Framework for synthetic data generation and AI feedback
Community
Blog
Learn
Discord
Forum
Github

Enterprise-ready version of the world‚Äôs leading AI platform
for $20/user/month with your Hub organization
Give your organization the most advanced platform to build AI with enterprise-grade security, access controls,
			dedicated support and more.
Single Sign-On
Connect securely to your identity provider with SSO integration.
Regions
Select, manage, and audit the location of your repository data.
Audit Logs
Stay in control with comprehensive logs that report on actions taken.
Resource Groups
Accurately manage access to repositories with granular access control.
Token Management
Centralized token control and custom approval policies for organization access.
Analytics
Track and analyze repository usage data in a single dashboard.
Advanced Compute Options
Increase scalability and performance with more compute options like ZeroGPU.
ZeroGPU Quota Boost
All organization members get 5x more ZeroGPU quota to get the most of Spaces.
Private Datasets Viewer
Enable the Dataset Viewer on your private datasets for easier collaboration.
Advanced security
Configure organization-wide security policies and default repository visibility.
Billing
Control your budget effectively with managed billing and yearly commit options.
Priority Support
Maximize your platform usage with priority support from the Hugging Face team.
Extra Private Storage
Get an additional 1 TB of private storage for each member of your organization (then $25/month per extra TB).
Join the most forward-thinking AI organizations
Everything you already know and love about Hugging Face in Enterprise mode.
Compliance & Certifications
GDPR Compliant
SOC 2 Type 2

Leveling up AI collaboration and compute.
Users and organizations already use the Hub as a collaboration platform,we‚Äôre making it easy to seamlessly and scalably launch ML compute directly from the Hub.
HF Hub
Collaborate on Machine Learning
Host unlimited public models, datasets
Create unlimited orgs with no member limits
Access the latest ML tools and open source
Community support
Forever
Pro Account
Unlock advanced HF features
ZeroGPU and Dev Mode for Spaces
Free credits across all Inference Providers
Get early access to upcoming features
Show your support with a Pro badge
Subscribe for
Enterprise Hub
Accelerate your AI roadmap
SSO and SAML support
Select data location with Storage Regions
Precise actions reviews with Audit logs
Granular access control with Resource groups
Centralized token control and approval
Dataset Viewer for private datasets
Advanced compute options for Spaces
5x more ZeroGPU quota for all org members
Deploy Inference on your own Infra
Managed billing with yearly commits
Priority support
Starting at
Spaces Hardware
Upgrade your Space compute
Free CPUs
Build more advanced Spaces
7 optimized hardware available
From CPU to GPU to Accelerators
Starting at
Inference Endpoints
Deploy models on fully managed infrastructure
Deploy dedicated Endpoints in seconds
Keep your costs low
Fully-managed autoscaling
Enterprise security
Starting at
Need support to accelerate AI in your organization? View ourExpert Support.
Hugging Face Hub
The HF Hub is the central place to explore, experiment, collaborate and build technology with Machine
					Learning.Join the open source Machine Learning movement!
Create with ML
Packed with ML features, like model eval, dataset viewer and much more.
Collaborate
Git based and designed for collaboration at its core.
Play and learn
Learn by experimenting and sharing with our awesome community.
Build your ML portfolio
Share your work with the world and build your own ML profile.
Spaces Hardware
Spaces are one of the most popular ways to share ML applications and demos with the world.Upgrade your Spaces with our selection of custom on-demand hardware:
Spaces Persistent Storage
All Spaces get ephemeral storage for free but you can upgrade and add persistent storage at any time.
Building something cool as a side project? We also offer community GPU grants.
Inference Endpoints
Inference Endpoints (dedicated) offers a secure production solution to easily deploy any ML model on dedicated
					and autoscaling infrastructure, right from the HF Hub.
Pro Account
A monthly subscription to access powerful features.
ZeroGPU: Get 5x usage quota and highest GPU queue priority
ZeroGPU: Get 5x usage quota and highest GPU queue priority
Spaces Hosting: Create ZeroGPU Spaces with A100 hardware
Spaces Hosting: Create ZeroGPU Spaces with A100 hardware
Spaces Dev Mode: Fast iterations via SSH/VS Code for Spaces
Spaces Dev Mode: Fast iterations via SSH/VS Code for Spaces
Inference Providers: Get $2 included credits across all Inference Providers
Inference Providers: Get $2 included credits across all Inference Providers
Dataset Viewer: Activate it on private datasets
Dataset Viewer: Activate it on private datasets
Blog Articles: Publish articles to the Hugging Face blog
Blog Articles: Publish articles to the Hugging Face blog
Social Posts: Share short updates with the community
Social Posts: Share short updates with the community
Features Preview: Get early access to upcoming
										features
Features Preview: Get early access to upcoming
										features
PROBadge:
										Show your support on your profile
PROBadge:
										Show your support on your profile

SSO is available forEnterpriseaccounts.

LLM Course documentation
Introduction
LLM Course
and get access to the augmented documentation experience
to get started
Introduction
Welcome to the ü§ó Course!
This course will teach you about large language models (LLMs) and natural language processing (NLP) using libraries from theHugging Faceecosystem ‚Äîü§ó Transformers,ü§ó Datasets,ü§ó Tokenizers, andü§ó Accelerate‚Äî as well as theHugging Face Hub.
We‚Äôll also cover libraries outside the Hugging Face ecosystem. These are amazing contributions to the AI community and incredibly useful tools.
It‚Äôs completely free and without ads.
Understanding NLP and LLMs
While this course was originally focused on NLP (Natural Language Processing), it has evolved to emphasize Large Language Models (LLMs), which represent the latest advancement in the field.
What‚Äôs the difference?
NLP (Natural Language Processing)is the broader field focused on enabling computers to understand, interpret, and generate human language. NLP encompasses many techniques and tasks such as sentiment analysis, named entity recognition, and machine translation.
LLMs (Large Language Models)are a powerful subset of NLP models characterized by their massive size, extensive training data, and ability to perform a wide range of language tasks with minimal task-specific training. Models like the Llama, GPT, or Claude series are examples of LLMs that have revolutionized what‚Äôs possible in NLP.
Throughout this course, you‚Äôll learn about both traditional NLP concepts and cutting-edge LLM techniques, as understanding the foundations of NLP is crucial for working effectively with LLMs.
What to expect?
Here is a brief overview of the course:
Chapters 1 to 4 provide an introduction to the main concepts of the ü§ó Transformers library. By the end of this part of the course, you will be familiar with how Transformer models work and will know how to use a model from theHugging Face Hub, fine-tune it on a dataset, and share your results on the Hub!
Chapters 5 to 8 teach the basics of ü§ó Datasets and ü§ó Tokenizers before diving into classic NLP tasks and LLM techniques. By the end of this part, you will be able to tackle the most common language processing challenges by yourself.
Chapter 9 goes beyond NLP to cover how to build and share demos of your models on the ü§ó Hub. By the end of this part, you will be ready to showcase your ü§ó Transformers application to the world!
Chapters 10 to 12 dive into advanced LLM topics like fine-tuning, curating high-quality datasets, and building reasoning models.
This course:
Requires a good knowledge of Python
Is better taken after an introductory deep learning course, such asfast.ai‚ÄôsPractical Deep Learning for Codersor one of the programs developed byDeepLearning.AI
Does not expect priorPyTorchorTensorFlowknowledge, though some familiarity with either of those will help
After you‚Äôve completed this course, we recommend checking out DeepLearning.AI‚ÄôsNatural Language Processing Specialization, which covers a wide range of traditional NLP models like naive Bayes and LSTMs that are well worth knowing about!
Who are we?
About the authors:
Abubakar Abidcompleted his PhD at Stanford in applied machine learning. During his PhD, he foundedGradio, an open-source Python library that has been used to build over 600,000 machine learning demos. Gradio was acquired by Hugging Face, which is where Abubakar now serves as a machine learning team lead.
Ben Burtenshawis a Machine Learning Engineer at Hugging Face. He completed his PhD in Natural Language Processing at the University of Antwerp, where he applied Transformer models to generate children stories for the purpose of improving literacy skills. Since then, he has focused on educational materials and tools for the wider community.
Matthew Carriganis a Machine Learning Engineer at Hugging Face. He lives in Dublin, Ireland and previously worked as an ML engineer at Parse.ly and before that as a post-doctoral researcher at Trinity College Dublin. He does not believe we‚Äôre going to get to AGI by scaling existing architectures, but has high hopes for robot immortality regardless.
Lysandre Debutis a Machine Learning Engineer at Hugging Face and has been working on the ü§ó Transformers library since the very early development stages. His aim is to make NLP accessible for everyone by developing tools with a very simple API.
Sylvain Guggeris a Research Engineer at Hugging Face and one of the core maintainers of the ü§ó Transformers library. Previously he was a Research Scientist at fast.ai, and he co-wroteDeep Learning for Coders with fastai and PyTorchwith Jeremy Howard. The main focus of his research is on making deep learning more accessible, by designing and improving techniques that allow models to train fast on limited resources.
Dawood Khanis a Machine Learning Engineer at Hugging Face. He‚Äôs from NYC and graduated from New York University studying Computer Science. After working as an iOS Engineer for a few years, Dawood quit to start Gradio with his fellow co-founders. Gradio was eventually acquired by Hugging Face.
Merve Noyanis a developer advocate at Hugging Face, working on developing tools and building content around them to democratize machine learning for everyone.
Lucile Saulnieris a machine learning engineer at Hugging Face, developing and supporting the use of open source tools. She is also actively involved in many research projects in the field of Natural Language Processing such as collaborative training and BigScience.
Lewis Tunstallis a machine learning engineer at Hugging Face, focused on developing open-source tools and making them accessible to the wider community. He is also a co-author of the O‚ÄôReilly bookNatural Language Processing with Transformers.
Leandro von Werrais a machine learning engineer in the open-source team at Hugging Face and also a co-author of the O‚ÄôReilly bookNatural Language Processing with Transformers. He has several years of industry experience bringing NLP projects to production by working across the whole machine learning stack..
FAQ
Here are some answers to frequently asked questions:
Does taking this course lead to a certification?Currently we do not have any certification for this course. However, we are working on a certification program for the Hugging Face ecosystem ‚Äî stay tuned!
Does taking this course lead to a certification?Currently we do not have any certification for this course. However, we are working on a certification program for the Hugging Face ecosystem ‚Äî stay tuned!
How much time should I spend on this course?Each chapter in this course is designed to be completed in 1 week, with approximately 6-8 hours of work per week. However, you can take as much time as you need to complete the course.
How much time should I spend on this course?Each chapter in this course is designed to be completed in 1 week, with approximately 6-8 hours of work per week. However, you can take as much time as you need to complete the course.
Where can I ask a question if I have one?If you have a question about any section of the course, just click on the ‚ÄùAsk a question‚Äù banner at the top of the page to be automatically redirected to the right section of theHugging Face forums:
Where can I ask a question if I have one?If you have a question about any section of the course, just click on the ‚ÄùAsk a question‚Äù banner at the top of the page to be automatically redirected to the right section of theHugging Face forums:
Note that a list ofproject ideasis also available on the forums if you wish to practice more once you have completed the course.
Where can I get the code for the course?For each section, click on the banner at the top of the page to run the code in either Google Colab or Amazon SageMaker Studio Lab:
The Jupyter notebooks containing all the code from the course are hosted on thehuggingface/notebooksrepo. If you wish to generate them locally, check out the instructions in thecourserepo on GitHub.
huggingface/notebooks
course
How can I contribute to the course?There are many ways to contribute to the course! If you find a typo or a bug, please open an issue on thecourserepo. If you would like to help translate the course into your native language, check out the instructionshere.
How can I contribute to the course?There are many ways to contribute to the course! If you find a typo or a bug, please open an issue on thecourserepo. If you would like to help translate the course into your native language, check out the instructionshere.
course
What were the choices made for each translation?Each translation has a glossary andTRANSLATING.txtfile that details the choices that were made for machine learning jargon etc. You can find an example for Germanhere.
What were the choices made for each translation?Each translation has a glossary andTRANSLATING.txtfile that details the choices that were made for machine learning jargon etc. You can find an example for Germanhere.
TRANSLATING.txt
Can I reuse this course?Of course! The course is released under the permissiveApache 2 license. This means that you must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use. If you would like to cite the course, please use the following BibTeX:
@misc{huggingfacecourse,
  author = {Hugging Face},
  title ={The Hugging Face Course, 2022},
  howpublished = "\url{https://huggingface.co/course}",
  year ={2022},
  note = "[Online; accessed<today>]"
}
Languages and translations
Thanks to our wonderful community, the course is available in many languages beyond English üî•! Check out the table below to see which languages are available and who contributed to the translations:
For some languages, thecourse YouTube videoshave subtitles in the language. You can enable them by first clicking theCCbutton in the bottom right corner of the video. Then, under the settings icon ‚öôÔ∏è, you can select the language you want by selecting theSubtitles/CCoption.
Let‚Äôs go üöÄ
Are you ready to roll? In this chapter, you will learn:
How to use thepipeline()function to solve NLP tasks such as text generation and classification
pipeline()
About the Transformer architecture
How to distinguish between encoder, decoder, and encoder-decoder architectures and use cases

LLM Course documentation
Natural Language Processing and Large Language Models
LLM Course
and get access to the augmented documentation experience
to get started
Natural Language Processing and Large Language Models
Before jumping into Transformer models, let‚Äôs do a quick overview of what natural language processing is, how large language models have transformed the field, and why we care about it.
What is NLP?
NLP is a field of linguistics and machine learning focused on understanding everything related to human language. The aim of NLP tasks is not only to understand single words individually, but to be able to understand the context of those words.
The following is a list of common NLP tasks, with some examples of each:
Classifying whole sentences: Getting the sentiment of a review, detecting if an email is spam, determining if a sentence is grammatically correct or whether two sentences are logically related or not
Classifying each word in a sentence: Identifying the grammatical components of a sentence (noun, verb, adjective), or the named entities (person, location, organization)
Generating text content: Completing a prompt with auto-generated text, filling in the blanks in a text with masked words
Extracting an answer from a text: Given a question and a context, extracting the answer to the question based on the information provided in the context
Generating a new sentence from an input text: Translating a text into another language, summarizing a text
NLP isn‚Äôt limited to written text though. It also tackles complex challenges in speech recognition and computer vision, such as generating a transcript of an audio sample or a description of an image.
The Rise of Large Language Models (LLMs)
In recent years, the field of NLP has been revolutionized by Large Language Models (LLMs). These models, which include architectures like GPT (Generative Pre-trained Transformer) andLlama, have transformed what‚Äôs possible in language processing.
A large language model (LLM) is an AI model trained on massive amounts of text data that can understand and generate human-like text, recognize patterns in language, and perform a wide variety of language tasks without task-specific training. They represent a significant advancement in the field of natural language processing (NLP).
LLMs are characterized by:
Scale: They contain millions, billions, or even hundreds of billions of parameters
General capabilities: They can perform multiple tasks without task-specific training
In-context learning: They can learn from examples provided in the prompt
Emergent abilities: As these models grow in size, they demonstrate capabilities that weren‚Äôt explicitly programmed or anticipated
The advent of LLMs has shifted the paradigm from building specialized models for specific NLP tasks to using a single, large model that can be prompted or fine-tuned to address a wide range of language tasks. This has made sophisticated language processing more accessible while also introducing new challenges in areas like efficiency, ethics, and deployment.
However, LLMs also have important limitations:
Hallucinations: They can generate incorrect information confidently
Lack of true understanding: They lack true understanding of the world and operate purely on statistical patterns
Bias: They may reproduce biases present in their training data or inputs.
Context windows: They have limited context windows (though this is improving)
Computational resources: They require significant computational resources
Why is language processing challenging?
Computers don‚Äôt process information in the same way as humans. For example, when we read the sentence ‚ÄúI am hungry,‚Äù we can easily understand its meaning. Similarly, given two sentences such as ‚ÄúI am hungry‚Äù and ‚ÄúI am sad,‚Äù we‚Äôre able to easily determine how similar they are. For machine learning (ML) models, such tasks are more difficult. The text needs to be processed in a way that enables the model to learn from it. And because language is complex, we need to think carefully about how this processing must be done. There has been a lot of research done on how to represent text, and we will look at some methods in the next chapter.
Even with the advances in LLMs, many fundamental challenges remain. These include understanding ambiguity, cultural context, sarcasm, and humor. LLMs address these challenges through massive training on diverse datasets, but still often fall short of human-level understanding in many complex scenarios.

LLM Course documentation
Transformers, what can they do?
LLM Course
and get access to the augmented documentation experience
to get started
Transformers, what can they do?
In this section, we will look at what Transformer models can do and use our first tool from the ü§ó Transformers library: thepipeline()function.
pipeline()
If you want to run the examples locally, we recommend taking a look at thesetup.
Transformers are everywhere!
Transformer models are used to solve all kinds of tasks across different modalities, including natural language processing (NLP), computer vision, audio processing, and more. Here are some of the companies and organizations using Hugging Face and Transformer models, who also contribute back to the community by sharing their models:
Theü§ó Transformers libraryprovides the functionality to create and use those shared models. TheModel Hubcontains millions of pretrained models that anyone can download and use. You can also upload your own models to the Hub!
‚ö†Ô∏è The Hugging Face Hub is not limited to Transformer models. Anyone can share any kind of models or datasets they want!Create a huggingface.coaccount to benefit from all available features!
Before diving into how Transformer models work under the hood, let‚Äôs look at a few examples of how they can be used to solve some interesting NLP problems.
Working with pipelines
The most basic object in the ü§ó Transformers library is thepipeline()function. It connects a model with its necessary preprocessing and postprocessing steps, allowing us to directly input any text and get an intelligible answer:
pipeline()
fromtransformersimportpipeline

classifier = pipeline("sentiment-analysis")
classifier("I've been waiting for a HuggingFace course my whole life.")
[{'label':'POSITIVE','score':0.9598047137260437}]
We can even pass several sentences!
classifier(
    ["I've been waiting for a HuggingFace course my whole life.","I hate this so much!"]
)
[{'label':'POSITIVE','score':0.9598047137260437},
 {'label':'NEGATIVE','score':0.9994558095932007}]
By default, this pipeline selects a particular pretrained model that has been fine-tuned for sentiment analysis in English. The model is downloaded and cached when you create theclassifierobject. If you rerun the command, the cached model will be used instead and there is no need to download the model again.
classifier
There are three main steps involved when you pass some text to a pipeline:
The text is preprocessed into a format the model can understand.
The preprocessed inputs are passed to the model.
The predictions of the model are post-processed, so you can make sense of them.
Available pipelines for different modalities
Thepipeline()function supports multiple modalities, allowing you to work with text, images, audio, and even multimodal tasks. In this course we‚Äôll focus on text tasks, but it‚Äôs useful to understand the transformer architecture‚Äôs potential, so we‚Äôll briefly outline it.
pipeline()
Here‚Äôs an overview of what‚Äôs available:
For a full and updated list of pipelines, see theü§ó Transformers documentation.
Text pipelines
text-generation: Generate text from a prompt
text-generation
text-classification: Classify text into predefined categories
text-classification
summarization: Create a shorter version of a text while preserving key information
summarization
translation: Translate text from one language to another
translation
zero-shot-classification: Classify text without prior training on specific labels
zero-shot-classification
feature-extraction: Extract vector representations of text
feature-extraction
Image pipelines
image-to-text: Generate text descriptions of images
image-to-text
image-classification: Identify objects in an image
image-classification
object-detection: Locate and identify objects in images
object-detection
Audio pipelines
automatic-speech-recognition: Convert speech to text
automatic-speech-recognition
audio-classification: Classify audio into categories
audio-classification
text-to-speech: Convert text to spoken audio
text-to-speech
Multimodal pipelines
image-text-to-text: Respond to an image based on a text prompt
image-text-to-text
Let‚Äôs explore some of these pipelines in more detail!
Zero-shot classification
We‚Äôll start by tackling a more challenging task where we need to classify texts that haven‚Äôt been labelled. This is a common scenario in real-world projects because annotating text is usually time-consuming and requires domain expertise. For this use case, thezero-shot-classificationpipeline is very powerful: it allows you to specify which labels to use for the classification, so you don‚Äôt have to rely on the labels of the pretrained model. You‚Äôve already seen how the model can classify a sentence as positive or negative using those two labels ‚Äî but it can also classify the text using any other set of labels you like.
zero-shot-classification
fromtransformersimportpipeline

classifier = pipeline("zero-shot-classification")
classifier("This is a course about the Transformers library",
    candidate_labels=["education","politics","business"],
)
{'sequence':'This is a course about the Transformers library','labels': ['education','business','politics'],'scores': [0.8445963859558105,0.111976258456707,0.043427448719739914]}
This pipeline is calledzero-shotbecause you don‚Äôt need to fine-tune the model on your data to use it. It can directly return probability scores for any list of labels you want!
‚úèÔ∏èTry it out!Play around with your own sequences and labels and see how the model behaves.
Text generation
Now let‚Äôs see how to use a pipeline to generate some text. The main idea here is that you provide a prompt and the model will auto-complete it by generating the remaining text. This is similar to the predictive text feature that is found on many phones. Text generation involves randomness, so it‚Äôs normal if you don‚Äôt get the same results as shown below.
fromtransformersimportpipeline

generator = pipeline("text-generation")
generator("In this course, we will teach you how to")
[{'generated_text':'In this course, we will teach you how to understand and use ''data flow and data interchange when handling user data. We ''will be working with one or more of the most commonly used ''data flows ‚Äî data flows of various types, as seen by the ''HTTP'}]
You can control how many different sequences are generated with the argumentnum_return_sequencesand the total length of the output text with the argumentmax_length.
num_return_sequences
max_length
‚úèÔ∏èTry it out!Use thenum_return_sequencesandmax_lengtharguments to generate two sentences of 15 words each.
num_return_sequences
max_length
Using any model from the Hub in a pipeline
The previous examples used the default model for the task at hand, but you can also choose a particular model from the Hub to use in a pipeline for a specific task ‚Äî say, text generation. Go to theModel Huband click on the corresponding tag on the left to display only the supported models for that task. You should get to a page likethis one.
Let‚Äôs try theHuggingFaceTB/SmolLM2-360Mmodel! Here‚Äôs how to load it in the same pipeline as before:
HuggingFaceTB/SmolLM2-360M
fromtransformersimportpipeline

generator = pipeline("text-generation", model="HuggingFaceTB/SmolLM2-360M")
generator("In this course, we will teach you how to",
    max_length=30,
    num_return_sequences=2,
)
[{'generated_text':'In this course, we will teach you how to manipulate the world and ''move your mental and physical capabilities to your advantage.'},
 {'generated_text':'In this course, we will teach you how to become an expert and ''practice realtime, and with a hands on experience on both real ''time and real'}]
You can refine your search for a model by clicking on the language tags, and pick a model that will generate text in another language. The Model Hub even contains checkpoints for multilingual models that support several languages.
Once you select a model by clicking on it, you‚Äôll see that there is a widget enabling you to try it directly online. This way you can quickly test the model‚Äôs capabilities before downloading it.
‚úèÔ∏èTry it out!Use the filters to find a text generation model for another language. Feel free to play with the widget and use it in a pipeline!
Inference Providers
All the models can be tested directly through your browser using the Inference Providers, which is available on the Hugging Facewebsite. You can play with the model directly on this page by inputting custom text and watching the model process the input data.
Inference Providers that powers the widget is also available as a paid product, which comes in handy if you need it for your workflows. See thepricing pagefor more details.
Mask filling
The next pipeline you‚Äôll try isfill-mask. The idea of this task is to fill in the blanks in a given text:
fill-mask
fromtransformersimportpipeline

unmasker = pipeline("fill-mask")
unmasker("This course will teach you all about <mask> models.", top_k=2)
[{'sequence':'This course will teach you all about mathematical models.','score':0.19619831442832947,'token':30412,'token_str':' mathematical'},
 {'sequence':'This course will teach you all about computational models.','score':0.04052725434303284,'token':38163,'token_str':' computational'}]
Thetop_kargument controls how many possibilities you want to be displayed. Note that here the model fills in the special<mask>word, which is often referred to as amask token. Other mask-filling models might have different mask tokens, so it‚Äôs always good to verify the proper mask word when exploring other models. One way to check it is by looking at the mask word used in the widget.
top_k
<mask>
‚úèÔ∏èTry it out!Search for thebert-base-casedmodel on the Hub and identify its mask word in the Inference API widget. What does this model predict for the sentence in ourpipelineexample above?
bert-base-cased
pipeline
Named entity recognition
Named entity recognition (NER) is a task where the model has to find which parts of the input text correspond to entities such as persons, locations, or organizations. Let‚Äôs look at an example:
fromtransformersimportpipeline

ner = pipeline("ner", grouped_entities=True)
ner("My name is Sylvain and I work at Hugging Face in Brooklyn.")
[{'entity_group':'PER','score':0.99816,'word':'Sylvain','start':11,'end':18}, 
 {'entity_group':'ORG','score':0.97960,'word':'Hugging Face','start':33,'end':45}, 
 {'entity_group':'LOC','score':0.99321,'word':'Brooklyn','start':49,'end':57}
]
Here the model correctly identified that Sylvain is a person (PER), Hugging Face an organization (ORG), and Brooklyn a location (LOC).
We pass the optiongrouped_entities=Truein the pipeline creation function to tell the pipeline to regroup together the parts of the sentence that correspond to the same entity: here the model correctly grouped ‚ÄúHugging‚Äù and ‚ÄúFace‚Äù as a single organization, even though the name consists of multiple words. In fact, as we will see in the next chapter, the preprocessing even splits some words into smaller parts. For instance,Sylvainis split into four pieces:S,##yl,##va, and##in. In the post-processing step, the pipeline successfully regrouped those pieces.
grouped_entities=True
Sylvain
S
##yl
##va
##in
‚úèÔ∏èTry it out!Search the Model Hub for a model able to do part-of-speech tagging (usually abbreviated as POS) in English. What does this model predict for the sentence in the example above?
Question answering
Thequestion-answeringpipeline answers questions using information from a given context:
question-answering
fromtransformersimportpipeline

question_answerer = pipeline("question-answering")
question_answerer(
    question="Where do I work?",
    context="My name is Sylvain and I work at Hugging Face in Brooklyn",
)
{'score':0.6385916471481323,'start':33,'end':45,'answer':'Hugging Face'}
Note that this pipeline works by extracting information from the provided context; it does not generate the answer.
Summarization
Summarization is the task of reducing a text into a shorter text while keeping all (or most) of the important aspects referenced in the text. Here‚Äôs an example:
fromtransformersimportpipeline

summarizer = pipeline("summarization")
summarizer("""
    America has changed dramatically during recent years. Not only has the number of 
    graduates in traditional engineering disciplines such as mechanical, civil, 
    electrical, chemical, and aeronautical engineering declined, but in most of 
    the premier American universities engineering curricula now concentrate on 
    and encourage largely the study of engineering science. As a result, there 
    are declining offerings in engineering subjects dealing with infrastructure, 
    the environment, and related issues, and greater concentration on high 
    technology subjects, largely supporting increasingly complex scientific 
    developments. While the latter is important, it should not be at the expense 
    of more traditional engineering.

    Rapidly developing economies such as China and India, as well as other 
    industrial countries in Europe and Asia, continue to encourage and advance 
    the teaching of engineering. Both China and India, respectively, graduate 
    six and eight times as many traditional engineers as does the United States. 
    Other industrial countries at minimum maintain their output, while America 
    suffers an increasingly serious decline in the number of engineering graduates 
    and a lack of well-educated engineers.
""")
[{'summary_text':' America has changed dramatically during recent years . The ''number of engineering graduates in the U.S. has declined in ''traditional engineering disciplines such as mechanical, civil '', electrical, chemical, and aeronautical engineering . Rapidly ''developing economies such as China and India, as well as other ''industrial countries in Europe and Asia, continue to encourage ''and advance engineering .'}]
Like with text generation, you can specify amax_lengthor amin_lengthfor the result.
max_length
min_length
Translation
For translation, you can use a default model if you provide a language pair in the task name (such as"translation_en_to_fr"), but the easiest way is to pick the model you want to use on theModel Hub. Here we‚Äôll try translating from French to English:
"translation_en_to_fr"
fromtransformersimportpipeline

translator = pipeline("translation", model="Helsinki-NLP/opus-mt-fr-en")
translator("Ce cours est produit par Hugging Face.")
[{'translation_text':'This course is produced by Hugging Face.'}]
Like with text generation and summarization, you can specify amax_lengthor amin_lengthfor the result.
max_length
min_length
‚úèÔ∏èTry it out!Search for translation models in other languages and try to translate the previous sentence into a few different languages.
Image and audio pipelines
Beyond text, Transformer models can also work with images and audio. Here are a few examples:
Image classification
fromtransformersimportpipeline

image_classifier = pipeline(
    task="image-classification", model="google/vit-base-patch16-224")
result = image_classifier("https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg")print(result)
[{'label':'lynx, catamount','score':0.43350091576576233},
 {'label':'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor','score':0.034796204417943954},
 {'label':'snow leopard, ounce, Panthera uncia','score':0.03240183740854263},
 {'label':'Egyptian cat','score':0.02394474856555462},
 {'label':'tiger cat','score':0.02288915030658245}]
Automatic speech recognition
fromtransformersimportpipeline

transcriber = pipeline(
    task="automatic-speech-recognition", model="openai/whisper-large-v3")
result = transcriber("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac")print(result)
{'text':' I have a dream that one day this nation will rise up and live out the true meaning of its creed.'}
Combining data from multiple sources
One powerful application of Transformer models is their ability to combine and process data from multiple sources. This is especially useful when you need to:
Search across multiple databases or repositories
Consolidate information from different formats (text, images, audio)
Create a unified view of related information
For example, you could build a system that:
Searches for information across databases in multiple modalities like text and image.
Combines results from different sources into a single coherent response. For example, from an audio file and text description.
Presents the most relevant information from a database of documents and metadata.
Conclusion
The pipelines shown in this chapter are mostly for demonstrative purposes. They were programmed for specific tasks and cannot perform variations of them. In the next chapter, you‚Äôll learn what‚Äôs inside apipeline()function and how to customize its behavior.
pipeline()

LLM Course documentation
How do Transformers work?
LLM Course
and get access to the augmented documentation experience
to get started
How do Transformers work?
In this section, we will take a look at the architecture of Transformer models and dive deeper into the concepts of attention, encoder-decoder architecture, and more.
üöÄ We‚Äôre taking things up a notch here. This section is detailed and technical, so don‚Äôt worry if you don‚Äôt understand everything right away. We‚Äôll come back to these concepts later in the course.
A bit of Transformer history
Here are some reference points in the (short) history of Transformer models:
TheTransformer architecturewas introduced in June 2017. The focus of the original research was on translation tasks. This was followed by the introduction of several influential models, including:
June 2018:GPT, the first pretrained Transformer model, used for fine-tuning on various NLP tasks and obtained state-of-the-art results
June 2018:GPT, the first pretrained Transformer model, used for fine-tuning on various NLP tasks and obtained state-of-the-art results
October 2018:BERT, another large pretrained model, this one designed to produce better summaries of sentences (more on this in the next chapter!)
October 2018:BERT, another large pretrained model, this one designed to produce better summaries of sentences (more on this in the next chapter!)
February 2019:GPT-2, an improved (and bigger) version of GPT that was not immediately publicly released due to ethical concerns
February 2019:GPT-2, an improved (and bigger) version of GPT that was not immediately publicly released due to ethical concerns
October 2019:T5, A multi-task focused implementation of the sequence-to-sequence Transformer architecture.
October 2019:T5, A multi-task focused implementation of the sequence-to-sequence Transformer architecture.
May 2020,GPT-3, an even bigger version of GPT-2 that is able to perform well on a variety of tasks without the need for fine-tuning (calledzero-shot learning)
May 2020,GPT-3, an even bigger version of GPT-2 that is able to perform well on a variety of tasks without the need for fine-tuning (calledzero-shot learning)
January 2022:InstructGPT, a version of GPT-3 that was trained to follow instructions better
This list is far from comprehensive, and is just meant to highlight a few of the different kinds of Transformer models. Broadly, they can be grouped into three categories:
January 2022:InstructGPT, a version of GPT-3 that was trained to follow instructions better
This list is far from comprehensive, and is just meant to highlight a few of the different kinds of Transformer models. Broadly, they can be grouped into three categories:
January 2023:Llama, a large language model that is able to generate text in a variety of languages.
January 2023:Llama, a large language model that is able to generate text in a variety of languages.
March 2023:Mistral, a 7-billion-parameter language model that outperforms Llama 2 13B across all evaluated benchmarks, leveraging grouped-query attention for faster inference and sliding window attention to handle sequences of arbitrary length.
March 2023:Mistral, a 7-billion-parameter language model that outperforms Llama 2 13B across all evaluated benchmarks, leveraging grouped-query attention for faster inference and sliding window attention to handle sequences of arbitrary length.
May 2024:Gemma 2, a family of lightweight, state-of-the-art open models ranging from 2B to 27B parameters that incorporate interleaved local-global attentions and group-query attention, with smaller models trained using knowledge distillation to deliver performance competitive with models 2-3 times larger.
May 2024:Gemma 2, a family of lightweight, state-of-the-art open models ranging from 2B to 27B parameters that incorporate interleaved local-global attentions and group-query attention, with smaller models trained using knowledge distillation to deliver performance competitive with models 2-3 times larger.
November 2024:SmolLM2, a state-of-the-art small language model (135 million to 1.7 billion parameters) that achieves impressive performance despite its compact size, and unlocking new possibilities for mobile and edge devices.
November 2024:SmolLM2, a state-of-the-art small language model (135 million to 1.7 billion parameters) that achieves impressive performance despite its compact size, and unlocking new possibilities for mobile and edge devices.
GPT-like (also calledauto-regressiveTransformer models)
GPT-like (also calledauto-regressiveTransformer models)
BERT-like (also calledauto-encodingTransformer models)
BERT-like (also calledauto-encodingTransformer models)
T5-like (also calledsequence-to-sequenceTransformer models)
T5-like (also calledsequence-to-sequenceTransformer models)
We will dive into these families in more depth later on.
Transformers are language models
All the Transformer models mentioned above (GPT, BERT, T5, etc.) have been trained aslanguage models. This means they have been trained on large amounts of raw text in a self-supervised fashion.
Self-supervised learning is a type of training in which the objective is automatically computed from the inputs of the model. That means that humans are not needed to label the data!
This type of model develops a statistical understanding of the language it has been trained on, but it‚Äôs less useful for specific practical tasks. Because of this, the general pretrained model then goes through a process calledtransfer learningorfine-tuning. During this process, the model is fine-tuned in a supervised way ‚Äî that is, using human-annotated labels ‚Äî on a given task.
An example of a task is predicting the next word in a sentence having read thenprevious words. This is calledcausal language modelingbecause the output depends on the past and present inputs, but not the future ones.
Another example ismasked language modeling, in which the model predicts a masked word in the sentence.
Transformers are big models
Apart from a few outliers (like DistilBERT), the general strategy to achieve better performance is by increasing the models‚Äô sizes as well as the amount of data they are pretrained on.
Unfortunately, training a model, especially a large one, requires a large amount of data. This becomes very costly in terms of time and compute resources. It even translates to environmental impact, as can be seen in the following graph.
And this is showing a project for a (very big) model led by a team consciously trying to reduce the environmental impact of pretraining. The footprint of running lots of trials to get the best hyperparameters would be even higher.
Imagine if each time a research team, a student organization, or a company wanted to train a model, it did so from scratch. This would lead to huge, unnecessary global costs!
This is why sharing language models is paramount: sharing the trained weights and building on top of already trained weights reduces the overall compute cost and carbon footprint of the community.
By the way, you can evaluate the carbon footprint of your models‚Äô training through several tools. For exampleML CO2 ImpactorCode Carbonwhich is integrated in ü§ó Transformers. To learn more about this, you can read thisblog postwhich will show you how to generate anemissions.csvfile with an estimate of the footprint of your training, as well as thedocumentationof ü§ó Transformers addressing this topic.
emissions.csv
Transfer Learning
Pretrainingis the act of training a model from scratch: the weights are randomly initialized, and the training starts without any prior knowledge.
This pretraining is usually done on very large amounts of data. Therefore, it requires a very large corpus of data, and training can take up to several weeks.
Fine-tuning, on the other hand, is the training doneaftera model has been pretrained. To perform fine-tuning, you first acquire a pretrained language model, then perform additional training with a dataset specific to your task. Wait ‚Äî why not simply train the model for your final use case from the start (scratch)? There are a couple of reasons:
The pretrained model was already trained on a dataset that has some similarities with the fine-tuning dataset. The fine-tuning process is thus able to take advantage of knowledge acquired by the initial model during pretraining (for instance, with NLP problems, the pretrained model will have some kind of statistical understanding of the language you are using for your task).
Since the pretrained model was already trained on lots of data, the fine-tuning requires way less data to get decent results.
For the same reason, the amount of time and resources needed to get good results are much lower.
For example, one could leverage a pretrained model trained on the English language and then fine-tune it on an arXiv corpus, resulting in a science/research-based model. The fine-tuning will only require a limited amount of data: the knowledge the pretrained model has acquired is ‚Äútransferred,‚Äù hence the termtransfer learning.
Fine-tuning a model therefore has lower time, data, financial, and environmental costs. It is also quicker and easier to iterate over different fine-tuning schemes, as the training is less constraining than a full pretraining.
This process will also achieve better results than training from scratch (unless you have lots of data), which is why you should always try to leverage a pretrained model ‚Äî one as close as possible to the task you have at hand ‚Äî and fine-tune it.
General Transformer architecture
In this section, we‚Äôll go over the general architecture of the Transformer model. Don‚Äôt worry if you don‚Äôt understand some of the concepts; there are detailed sections later covering each of the components.
The model is primarily composed of two blocks:
Encoder (left): The encoder receives an input and builds a representation of it (its features). This means that the model is optimized to acquire understanding from the input.
Decoder (right): The decoder uses the encoder‚Äôs representation (features) along with other inputs to generate a target sequence. This means that the model is optimized for generating outputs.
Each of these parts can be used independently, depending on the task:
Encoder-only models: Good for tasks that require understanding of the input, such as sentence classification and named entity recognition.
Decoder-only models: Good for generative tasks such as text generation.
Encoder-decoder modelsorsequence-to-sequence models: Good for generative tasks that require an input, such as translation or summarization.
We will dive into those architectures independently in later sections.
Attention layers
A key feature of Transformer models is that they are built with special layers calledattention layers. In fact, the title of the paper introducing the Transformer architecture was‚ÄúAttention Is All You Need‚Äù! We will explore the details of attention layers later in the course; for now, all you need to know is that this layer will tell the model to pay specific attention to certain words in the sentence you passed it (and more or less ignore the others) when dealing with the representation of each word.
To put this into context, consider the task of translating text from English to French. Given the input ‚ÄúYou like this course‚Äù, a translation model will need to also attend to the adjacent word ‚ÄúYou‚Äù to get the proper translation for the word ‚Äúlike‚Äù, because in French the verb ‚Äúlike‚Äù is conjugated differently depending on the subject. The rest of the sentence, however, is not useful for the translation of that word. In the same vein, when translating ‚Äúthis‚Äù the model will also need to pay attention to the word ‚Äúcourse‚Äù, because ‚Äúthis‚Äù translates differently depending on whether the associated noun is masculine or feminine. Again, the other words in the sentence will not matter for the translation of ‚Äúcourse‚Äù. With more complex sentences (and more complex grammar rules), the model would need to pay special attention to words that might appear farther away in the sentence to properly translate each word.
The same concept applies to any task associated with natural language: a word by itself has a meaning, but that meaning is deeply affected by the context, which can be any other word (or words) before or after the word being studied.
Now that you have an idea of what attention layers are all about, let‚Äôs take a closer look at the Transformer architecture.
The original architecture
The Transformer architecture was originally designed for translation. During training, the encoder receives inputs (sentences) in a certain language, while the decoder receives the same sentences in the desired target language. In the encoder, the attention layers can use all the words in a sentence (since, as we just saw, the translation of a given word can be dependent on what is after as well as before it in the sentence). The decoder, however, works sequentially and can only pay attention to the words in the sentence that it has already translated (so, only the words before the word currently being generated). For example, when we have predicted the first three words of the translated target, we give them to the decoder  which then uses all the inputs of the encoder to try to predict the fourth word.
To speed things up during training (when the model has access to target sentences), the decoder is fed the whole target, but it is not allowed to use future words (if it had access to the word at position 2 when trying to predict the word at position 2, the problem would not be very hard!). For instance, when trying to predict the fourth word, the attention layer will only have access to the words in positions 1 to 3.
The original Transformer architecture looked like this, with the encoder on the left and the decoder on the right:
Note that the first attention layer in a decoder block pays attention to all (past) inputs to the decoder, but the second attention layer uses the output of the encoder. It can thus access the whole input sentence to best predict the current word. This is very useful as different languages can have grammatical rules that put the words in different orders, or some context provided later in the sentence may be helpful to determine the best translation of a given word.
Theattention maskcan also be used in the encoder/decoder to prevent the model from paying attention to some special words ‚Äî for instance, the special padding word used to make all the inputs the same length when batching together sentences.
Architectures vs. checkpoints
As we dive into Transformer models in this course, you‚Äôll see mentions ofarchitecturesandcheckpointsas well asmodels. These terms all have slightly different meanings:
Architecture: This is the skeleton of the model ‚Äî the definition of each layer and each operation that happens within the model.
Checkpoints: These are the weights that will be loaded in a given architecture.
Model: This is an umbrella term that isn‚Äôt as precise as ‚Äúarchitecture‚Äù or ‚Äúcheckpoint‚Äù: it can mean both. This course will specifyarchitectureorcheckpointwhen it matters to reduce ambiguity.
For example, BERT is an architecture whilebert-base-cased, a set of weights trained by the Google team for the first release of BERT, is a checkpoint. However, one can say ‚Äúthe BERT model‚Äù and ‚Äúthebert-base-casedmodel.‚Äù
bert-base-cased
bert-base-cased

LLM Course documentation
How ü§ó Transformers solve tasks
LLM Course
and get access to the augmented documentation experience
to get started
How ü§ó Transformers solve tasks
InTransformers, what can they do?, you learned about natural language processing (NLP), speech and audio, computer vision tasks, and some important applications of them. This page will look closely at how models solve these tasks and explain what‚Äôs happening under the hood. There are many ways to solve a given task, some models may implement certain techniques or even approach the task from a new angle, but for Transformer models, the general idea is the same. Owing to its flexible architecture, most models are a variant of an encoder, a decoder, or an encoder-decoder structure.
Before diving into specific architectural variants, it‚Äôs helpful to understand that most tasks follow a similar pattern: input data is processed through a model, and the output is interpreted for a specific task. The differences lie in how the data is prepared, what model architecture variant is used, and how the output is processed.
To explain how tasks are solved, we‚Äôll walk through what goes on inside the model to output useful predictions. We‚Äôll cover the following models and their corresponding tasks:
Wav2Vec2for audio classification and automatic speech recognition (ASR)
Vision Transformer (ViT)andConvNeXTfor image classification
DETRfor object detection
Mask2Formerfor image segmentation
GLPNfor depth estimation
BERTfor NLP tasks like text classification, token classification and question answering that use an encoder
GPT2for NLP tasks like text generation that use a decoder
BARTfor NLP tasks like summarization and translation that use an encoder-decoder
Before you go further, it is good to have some basic knowledge of the original Transformer architecture. Knowing how encoders, decoders, and attention work will aid you in understanding how different Transformer models work. Be sure to check out ourthe previous sectionfor more information!
Transformer models for language
Language models are at the heart of modern NLP. They‚Äôre designed to understand and generate human language by learning the statistical patterns and relationships between words or tokens in text.
The Transformer was initially designed for machine translation, and since then, it has become the default architecture for solving all AI tasks. Some tasks lend themselves to the Transformer‚Äôs encoder structure, while others are better suited for the decoder. Still, other tasks make use of both the Transformer‚Äôs encoder-decoder structure.
How language models work
Language models work by being trained to predict the probability of a word given the context of surrounding words. This gives them a foundational understanding of language that can generalize to other tasks.
There are two main approaches for training a transformer model:
Masked language modeling (MLM): Used by encoder models like BERT, this approach randomly masks some tokens in the input and trains the model to predict the original tokens based on the surrounding context. This allows the model to learn bidirectional context (looking at words both before and after the masked word).
Masked language modeling (MLM): Used by encoder models like BERT, this approach randomly masks some tokens in the input and trains the model to predict the original tokens based on the surrounding context. This allows the model to learn bidirectional context (looking at words both before and after the masked word).
Causal language modeling (CLM): Used by decoder models like GPT, this approach predicts the next token based on all previous tokens in the sequence. The model can only use context from the left (previous tokens) to predict the next token.
Causal language modeling (CLM): Used by decoder models like GPT, this approach predicts the next token based on all previous tokens in the sequence. The model can only use context from the left (previous tokens) to predict the next token.
Types of language models
In the Transformers library, language models generally fall into three architectural categories:
Encoder-only models(like BERT): These models use a bidirectional approach to understand context from both directions. They‚Äôre best suited for tasks that require deep understanding of text, such as classification, named entity recognition, and question answering.
Encoder-only models(like BERT): These models use a bidirectional approach to understand context from both directions. They‚Äôre best suited for tasks that require deep understanding of text, such as classification, named entity recognition, and question answering.
Decoder-only models(like GPT, Llama): These models process text from left to right and are particularly good at text generation tasks. They can complete sentences, write essays, or even generate code based on a prompt.
Decoder-only models(like GPT, Llama): These models process text from left to right and are particularly good at text generation tasks. They can complete sentences, write essays, or even generate code based on a prompt.
Encoder-decoder models(like T5, BART): These models combine both approaches, using an encoder to understand the input and a decoder to generate output. They excel at sequence-to-sequence tasks like translation, summarization, and question answering.
Encoder-decoder models(like T5, BART): These models combine both approaches, using an encoder to understand the input and a decoder to generate output. They excel at sequence-to-sequence tasks like translation, summarization, and question answering.
As we covered in the previous section, language models are typically pretrained on large amounts of text data in a self-supervised manner (without human annotations), then fine-tuned on specific tasks. This approach, known as transfer learning, allows these models to adapt to many different NLP tasks with relatively small amounts of task-specific data.
In the following sections, we‚Äôll explore specific model architectures and how they‚Äôre applied to various tasks across speech, vision, and text domains.
Understanding which part of the Transformer architecture (encoder, decoder, or both) is best suited for a particular NLP task is key to choosing the right model. Generally, tasks requiring bidirectional context use encoders, tasks generating text use decoders, and tasks converting one sequence to another use encoder-decoders.
Text generation
Text generation involves creating coherent and contextually relevant text based on a prompt or input.
GPT-2is a decoder-only model pretrained on a large amount of text. It can generate convincing (though not always true!) text given a prompt and complete other NLP tasks like question answering despite not being explicitly trained to.
GPT-2 usesbyte pair encoding (BPE)to tokenize words and generate a token embedding. Positional encodings are added to the token embeddings to indicate the position of each token in the sequence. The input embeddings are passed through multiple decoder blocks to output some final hidden state. Within each decoder block, GPT-2 uses amasked self-attentionlayer which means GPT-2 can‚Äôt attend to future tokens. It is only allowed to attend to tokens on the left. This is different from BERT‚Äôs [mask] token because, in masked self-attention, an attention mask is used to set the score to0for future tokens.
GPT-2 usesbyte pair encoding (BPE)to tokenize words and generate a token embedding. Positional encodings are added to the token embeddings to indicate the position of each token in the sequence. The input embeddings are passed through multiple decoder blocks to output some final hidden state. Within each decoder block, GPT-2 uses amasked self-attentionlayer which means GPT-2 can‚Äôt attend to future tokens. It is only allowed to attend to tokens on the left. This is different from BERT‚Äôs [mask] token because, in masked self-attention, an attention mask is used to set the score to0for future tokens.
mask
0
The output from the decoder is passed to a language modeling head, which performs a linear transformation to convert the hidden states into logits. The label is the next token in the sequence, which are created by shifting the logits to the right by one. The cross-entropy loss is calculated between the shifted logits and the labels to output the next most likely token.
The output from the decoder is passed to a language modeling head, which performs a linear transformation to convert the hidden states into logits. The label is the next token in the sequence, which are created by shifting the logits to the right by one. The cross-entropy loss is calculated between the shifted logits and the labels to output the next most likely token.
GPT-2‚Äôs pretraining objective is based entirely oncausal language modeling, predicting the next word in a sequence. This makes GPT-2 especially good at tasks that involve generating text.
Ready to try your hand at text generation? Check out our completecausal language modeling guideto learn how to finetune DistilGPT-2 and use it for inference!
Text classification
Text classification involves assigning predefined categories to text documents, such as sentiment analysis, topic classification, or spam detection.
BERTis an encoder-only model and is the first model to effectively implement deep bidirectionality to learn richer representations of the text by attending to words on both sides.
BERT usesWordPiecetokenization to generate a token embedding of the text. To tell the difference between a single sentence and a pair of sentences, a special[SEP]token is added to differentiate them. A special[CLS]token is added to the beginning of every sequence of text. The final output with the[CLS]token is used as the input to the classification head for classification tasks. BERT also adds a segment embedding to denote whether a token belongs to the first or second sentence in a pair of sentences.
BERT usesWordPiecetokenization to generate a token embedding of the text. To tell the difference between a single sentence and a pair of sentences, a special[SEP]token is added to differentiate them. A special[CLS]token is added to the beginning of every sequence of text. The final output with the[CLS]token is used as the input to the classification head for classification tasks. BERT also adds a segment embedding to denote whether a token belongs to the first or second sentence in a pair of sentences.
[SEP]
[CLS]
[CLS]
BERT is pretrained with two objectives: masked language modeling and next-sentence prediction. In masked language modeling, some percentage of the input tokens are randomly masked, and the model needs to predict these. This solves the issue of bidirectionality, where the model could cheat and see all the words and ‚Äúpredict‚Äù the next word. The final hidden states of the predicted mask tokens are passed to a feedforward network with a softmax over the vocabulary to predict the masked word.The second pretraining object is next-sentence prediction. The model must predict whether sentence B follows sentence A. Half of the time sentence B is the next sentence, and the other half of the time, sentence B is a random sentence. The prediction, whether it is the next sentence or not, is passed to a feedforward network with a softmax over the two classes (IsNextandNotNext).
BERT is pretrained with two objectives: masked language modeling and next-sentence prediction. In masked language modeling, some percentage of the input tokens are randomly masked, and the model needs to predict these. This solves the issue of bidirectionality, where the model could cheat and see all the words and ‚Äúpredict‚Äù the next word. The final hidden states of the predicted mask tokens are passed to a feedforward network with a softmax over the vocabulary to predict the masked word.
The second pretraining object is next-sentence prediction. The model must predict whether sentence B follows sentence A. Half of the time sentence B is the next sentence, and the other half of the time, sentence B is a random sentence. The prediction, whether it is the next sentence or not, is passed to a feedforward network with a softmax over the two classes (IsNextandNotNext).
IsNext
NotNext
The input embeddings are passed through multiple encoder layers to output some final hidden states.
The input embeddings are passed through multiple encoder layers to output some final hidden states.
To use the pretrained model for text classification, add a sequence classification head on top of the base BERT model. The sequence classification head is a linear layer that accepts the final hidden states and performs a linear transformation to convert them into logits. The cross-entropy loss is calculated between the logits and target to find the most likely label.
Ready to try your hand at text classification? Check out our completetext classification guideto learn how to finetune DistilBERT and use it for inference!
Token classification
Token classification involves assigning a label to each token in a sequence, such as in named entity recognition or part-of-speech tagging.
To use BERT for token classification tasks like named entity recognition (NER), add a token classification head on top of the base BERT model. The token classification head is a linear layer that accepts the final hidden states and performs a linear transformation to convert them into logits. The cross-entropy loss is calculated between the logits and each token to find the most likely label.
Ready to try your hand at token classification? Check out our completetoken classification guideto learn how to finetune DistilBERT and use it for inference!
Question answering
Question answering involves finding the answer to a question within a given context or passage.
To use BERT for question answering, add a span classification head on top of the base BERT model. This linear layer accepts the final hidden states and performs a linear transformation to compute thespanstart and end logits corresponding to the answer. The cross-entropy loss is calculated between the logits and the label position to find the most likely span of text corresponding to the answer.
span
Ready to try your hand at question answering? Check out our completequestion answering guideto learn how to finetune DistilBERT and use it for inference!
üí° Notice how easy it is to use BERT for different tasks once it‚Äôs been pretrained. You only need to add a specific head to the pretrained model to manipulate the hidden states into your desired output!
Summarization
Summarization involves condensing a longer text into a shorter version while preserving its key information and meaning.
Encoder-decoder models likeBARTandT5are designed for the sequence-to-sequence pattern of a summarization task. We‚Äôll explain how BART works in this section, and then you can try finetuning T5 at the end.
BART‚Äôs encoder architecture is very similar to BERT and accepts a token and positional embedding of the text. BART is pretrained by corrupting the input and then reconstructing it with the decoder. Unlike other encoders with specific corruption strategies, BART can apply any type of corruption. Thetext infillingcorruption strategy works the best though. In text infilling, a number of text spans are replaced with asingle[mask] token. This is important because the model has to predict the masked tokens, and it teaches the model to predict the number of missing tokens. The input embeddings and masked spans are passed through the encoder to output some final hidden states, but unlike BERT, BART doesn‚Äôt add a final feedforward network at the end to predict a word.
BART‚Äôs encoder architecture is very similar to BERT and accepts a token and positional embedding of the text. BART is pretrained by corrupting the input and then reconstructing it with the decoder. Unlike other encoders with specific corruption strategies, BART can apply any type of corruption. Thetext infillingcorruption strategy works the best though. In text infilling, a number of text spans are replaced with asingle[mask] token. This is important because the model has to predict the masked tokens, and it teaches the model to predict the number of missing tokens. The input embeddings and masked spans are passed through the encoder to output some final hidden states, but unlike BERT, BART doesn‚Äôt add a final feedforward network at the end to predict a word.
mask
The encoder‚Äôs output is passed to the decoder, which must predict the masked tokens and any uncorrupted tokens from the encoder‚Äôs output. This gives additional context to help the decoder restore the original text. The output from the decoder is passed to a language modeling head, which performs a linear transformation to convert the hidden states into logits. The cross-entropy loss is calculated between the logits and the label, which is just the token shifted to the right.
The encoder‚Äôs output is passed to the decoder, which must predict the masked tokens and any uncorrupted tokens from the encoder‚Äôs output. This gives additional context to help the decoder restore the original text. The output from the decoder is passed to a language modeling head, which performs a linear transformation to convert the hidden states into logits. The cross-entropy loss is calculated between the logits and the label, which is just the token shifted to the right.
Ready to try your hand at summarization? Check out our completesummarization guideto learn how to finetune T5 and use it for inference!
For more information about text generation, check out thetext generation strategiesguide!
Translation
Translation involves converting text from one language to another while preserving its meaning. Translation is another example of a sequence-to-sequence task, which means you can use an encoder-decoder model likeBARTorT5to do it. We‚Äôll explain how BART works in this section, and then you can try finetuning T5 at the end.
BART adapts to translation by adding a separate randomly initialized encoder to map a source language to an input that can be decoded into the target language. This new encoder‚Äôs embeddings are passed to the pretrained encoder instead of the original word embeddings. The source encoder is trained by updating the source encoder, positional embeddings, and input embeddings with the cross-entropy loss from the model output. The model parameters are frozen in this first step, and all the model parameters are trained together in the second step.
BART has since been followed up by a multilingual version, mBART, intended for translation and pretrained on many different languages.
Ready to try your hand at translation? Check out our completetranslation guideto learn how to finetune T5 and use it for inference!
As you‚Äôve seen throughout this guide, many models follow similar patterns despite addressing different tasks. Understanding these common patterns can help you quickly grasp how new models work and how to adapt existing models to your specific needs.
Modalities beyond text
Transformers are not limited to text. They can also be applied to other modalities like speech and audio, images, and video. Of course, on this course we will focus on text, but we can briefly introduce the other modalities.
Speech and audio
Let‚Äôs start by exploring how Transformer models handle speech and audio data, which presents unique challenges compared to text or images.
Whisperis a encoder-decoder (sequence-to-sequence) transformer pretrained on 680,000 hours of labeled audio data. This amount of pretraining data enables zero-shot performance on audio tasks in English and many other languages. The decoder allows Whisper to map the encoders learned speech representations to useful outputs, such as text, without additional fine-tuning. Whisper just works out of the box.
Diagram is fromWhisper paper.
This model has two main components:
Anencoderprocesses the input audio. The raw audio is first converted into a log-Mel spectrogram. This spectrogram is then passed through a Transformer encoder network.
Anencoderprocesses the input audio. The raw audio is first converted into a log-Mel spectrogram. This spectrogram is then passed through a Transformer encoder network.
Adecodertakes the encoded audio representation and autoregressively predicts the corresponding text tokens. It‚Äôs a standard Transformer decoder trained to predict the next text token given the previous tokens and the encoder output. Special tokens are used at the beginning of the decoder input to steer the model towards specific tasks like transcription, translation, or language identification.
Adecodertakes the encoded audio representation and autoregressively predicts the corresponding text tokens. It‚Äôs a standard Transformer decoder trained to predict the next text token given the previous tokens and the encoder output. Special tokens are used at the beginning of the decoder input to steer the model towards specific tasks like transcription, translation, or language identification.
Whisper was pretrained on a massive and diverse dataset of 680,000 hours of labeled audio data collected from the web. This large-scale, weakly supervised pretraining is the key to its strong zero-shot performance across many languages and tasks.
Now that Whisper is pretrained, you can use it directly for zero-shot inference or finetune it on your data for improved performance on specific tasks like automatic speech recognition or speech translation!
The key innovation in Whisper is its training on an unprecedented scale of diverse, weakly supervised audio data from the internet. This allows it to generalize remarkably well to different languages, accents, and tasks without task-specific finetuning.
Automatic speech recognition
To use the pretrained model for automatic speech recognition, you leverage its full encoder-decoder structure. The encoder processes the audio input, and the decoder autoregressively generates the transcript token by token. When fine-tuning, the model is typically trained using a standard sequence-to-sequence loss (like cross-entropy) to predict the correct text tokens based on the audio input.
The easiest way to use a fine-tuned model for inference is within apipeline.
pipeline
fromtransformersimportpipeline

transcriber = pipeline(
    task="automatic-speech-recognition", model="openai/whisper-base.en")
transcriber("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac")# Output: {'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its creed.'}
Ready to try your hand at automatic speech recognition? Check out our completeautomatic speech recognition guideto learn how to finetune Whisper and use it for inference!
Computer vision
Now let‚Äôs move on to computer vision tasks, which deal with understanding and interpreting visual information from images or videos.
There are two ways to approach computer vision tasks:
Split an image into a sequence of patches and process them in parallel with a Transformer.
Use a modern CNN, likeConvNeXT, which relies on convolutional layers but adopts modern network designs.
A third approach mixes Transformers with convolutions (for example,Convolutional Vision TransformerorLeViT). We won‚Äôt discuss those because they just combine the two approaches we examine here.
ViT and ConvNeXT are commonly used for image classification, but for other vision tasks like object detection, segmentation, and depth estimation, we‚Äôll look at DETR, Mask2Former and GLPN, respectively; these models are better suited for those tasks.
Image classification
Image classification is one of the fundamental computer vision tasks. Let‚Äôs see how different model architectures approach this problem.
ViT and ConvNeXT can both be used for image classification; the main difference is that ViT uses an attention mechanism while ConvNeXT uses convolutions.
ViTreplaces convolutions entirely with a pure Transformer architecture. If you‚Äôre familiar with the original Transformer, then you‚Äôre already most of the way toward understanding ViT.
The main change ViT introduced was in how images are fed to a Transformer:
An image is split into square non-overlapping patches, each of which gets turned into a vector orpatch embedding. The patch embeddings are generated from a convolutional 2D layer which creates the proper input dimensions (which for a base Transformer is 768 values for each patch embedding). If you had a 224x224 pixel image, you could split it into 196 16x16 image patches. Just like how text is tokenized into words, an image is ‚Äútokenized‚Äù into a sequence of patches.
An image is split into square non-overlapping patches, each of which gets turned into a vector orpatch embedding. The patch embeddings are generated from a convolutional 2D layer which creates the proper input dimensions (which for a base Transformer is 768 values for each patch embedding). If you had a 224x224 pixel image, you could split it into 196 16x16 image patches. Just like how text is tokenized into words, an image is ‚Äútokenized‚Äù into a sequence of patches.
Alearnable embedding- a special[CLS]token - is added to the beginning of the patch embeddings just like BERT. The final hidden state of the[CLS]token is used as the input to the attached classification head; other outputs are ignored. This token helps the model learn how to encode a representation of the image.
Alearnable embedding- a special[CLS]token - is added to the beginning of the patch embeddings just like BERT. The final hidden state of the[CLS]token is used as the input to the attached classification head; other outputs are ignored. This token helps the model learn how to encode a representation of the image.
[CLS]
[CLS]
The last thing to add to the patch and learnable embeddings are theposition embeddingsbecause the model doesn‚Äôt know how the image patches are ordered. The position embeddings are also learnable and have the same size as the patch embeddings. Finally, all of the embeddings are passed to the Transformer encoder.
The last thing to add to the patch and learnable embeddings are theposition embeddingsbecause the model doesn‚Äôt know how the image patches are ordered. The position embeddings are also learnable and have the same size as the patch embeddings. Finally, all of the embeddings are passed to the Transformer encoder.
The output, specifically only the output with the[CLS]token, is passed to a multilayer perceptron head (MLP). ViT‚Äôs pretraining objective is simply classification. Like other classification heads, the MLP head converts the output into logits over the class labels and calculates the cross-entropy loss to find the most likely class.
The output, specifically only the output with the[CLS]token, is passed to a multilayer perceptron head (MLP). ViT‚Äôs pretraining objective is simply classification. Like other classification heads, the MLP head converts the output into logits over the class labels and calculates the cross-entropy loss to find the most likely class.
[CLS]
Ready to try your hand at image classification? Check out our completeimage classification guideto learn how to fine-tune ViT and use it for inference!
Notice the parallel between ViT and BERT: both use a special token ([CLS]) to capture the overall representation, both add position information to their embeddings, and both use a Transformer encoder to process the sequence of tokens/patches.
[CLS]

LLM Course documentation
Transformer Architectures
LLM Course
and get access to the augmented documentation experience
to get started
Transformer Architectures
In the previous sections, we introduced the general Transformer architecture and explored how these models can solve various tasks. Now, let‚Äôs take a closer look at the three main architectural variants of Transformer models and understand when to use each one. Then, we looked at how those architectures are applied to different language tasks.
In this section, we‚Äôre going to dive deeper into the three main architectural variants of Transformer models and understand when to use each one.
Remember that most Transformer models use one of three architectures: encoder-only, decoder-only, or encoder-decoder (sequence-to-sequence). Understanding these differences will help you choose the right model for your specific task.
Encoder models
Encoder models use only the encoder of a Transformer model. At each stage, the attention layers can access all the words in the initial sentence. These models are often characterized as having ‚Äúbi-directional‚Äù attention, and are often calledauto-encoding models.
The pretraining of these models usually revolves around somehow corrupting a given sentence (for instance, by masking random words in it) and tasking the model with finding or reconstructing the initial sentence.
Encoder models are best suited for tasks requiring an understanding of the full sentence, such as sentence classification, named entity recognition (and more generally word classification), and extractive question answering.
As we saw inHow ü§ó Transformers solve tasks, encoder models like BERT excel at understanding text because they can look at the entire context in both directions. This makes them perfect for tasks where comprehension of the whole input is important.
Representatives of this family of models include:
BERT
DistilBERT
ModernBERT
Decoder models
Decoder models use only the decoder of a Transformer model. At each stage, for a given word the attention layers can only access the words positioned before it in the sentence. These models are often calledauto-regressive models.
The pretraining of decoder models usually revolves around predicting the next word in the sentence.
These models are best suited for tasks involving text generation.
Decoder models like GPT are designed to generate text by predicting one token at a time. As we explored inHow ü§ó Transformers solve tasks, they can only see previous tokens, which makes them excellent for creative text generation but less ideal for tasks requiring bidirectional understanding.
Representatives of this family of models include:
Hugging Face SmolLM Series
Meta‚Äôs Llama Series
Google‚Äôs Gemma Series
DeepSeek‚Äôs V3
Modern Large Language Models (LLMs)
Most modern Large Language Models (LLMs) use the decoder-only architecture. These models have grown dramatically in size and capabilities over the past few years, with some of the largest models containing hundreds of billions of parameters.
Modern LLMs are typically trained in two phases:
Pretraining: The model learns to predict the next token on vast amounts of text data
Instruction tuning: The model is fine-tuned to follow instructions and generate helpful responses
This approach has led to models that can understand and generate human-like text across a wide range of topics and tasks.
Modern decoder-based LLMs have demonstrated impressive capabilities:
You can experiment with decoder-based LLMs directly in your browser via model repo pages on the Hub. Here‚Äôs an an example with the classicGPT-2(OpenAI‚Äôs finest open source model!):
Sequence-to-sequence models
Encoder-decoder models (also calledsequence-to-sequence models) use both parts of the Transformer architecture. At each stage, the attention layers of the encoder can access all the words in the initial sentence, whereas the attention layers of the decoder can only access the words positioned before a given word in the input.
The pretraining of these models can take different forms, but it often involves reconstructing a sentence for which the input has been somehow corrupted (for instance by masking random words). The pretraining of the T5 model consists of replacing random spans of text (that can contain several words) with a single mask special token, and the task is then to predict the text that this mask token replaces.
Sequence-to-sequence models are best suited for tasks revolving around generating new sentences depending on a given input, such as summarization, translation, or generative question answering.
As we saw inHow ü§ó Transformers solve tasks, encoder-decoder models like BART and T5 combine the strengths of both architectures. The encoder provides deep bidirectional understanding of the input, while the decoder generates appropriate output text. This makes them perfect for tasks that transform one sequence into another, like translation or summarization.
Practical applications
Sequence-to-sequence models excel at tasks that require transforming one form of text into another while preserving meaning. Some practical applications include:
Here‚Äôs an interactive demo of a sequence-to-sequence model for translation:
Representatives of this family of models include:
BART
mBART
Marian
T5
Choosing the right architecture
When working on a specific NLP task, how do you decide which architecture to use? Here‚Äôs a quick guide:
When in doubt about which model to use, consider:
What kind of understanding does your task need? (Bidirectional or unidirectional)
Are you generating new text or analyzing existing text?
Do you need to transform one sequence into another?
The answers to these questions will guide you toward the right architecture.
The evolution of LLMs
Large Language Models have evolved rapidly in recent years, with each generation bringing significant improvements in capabilities.
Attention mechanisms
Most transformer models use full attention in the sense that the attention matrix is square. It can be a big
computational bottleneck when you have long texts. Longformer and reformer are models that try to be more efficient and
use a sparse version of the attention matrix to speed up training.
Standard attention mechanisms have a computational complexity of O(n¬≤), where n is the sequence length. This becomes problematic for very long sequences. The specialized attention mechanisms below help address this limitation.
LSH attention
Reformeruses LSH attention. In the softmax(QK^t), only the biggest elements (in the softmax dimension) of the matrix QK^t are going to give useful contributions. So for each query q in Q, we can consider only
the keys k in K that are close to q. A hash function is used to determine if q and k are close. The attention mask is
modified to mask the current token (except at the first position), because it will give a query and a key equal (so
very similar to each other). Since the hash can be a bit random, several hash functions are used in practice
(determined by a n_rounds parameter) and then are averaged together.
Local attention
Longformeruses local attention: often, the local context (e.g., what are the two tokens to the  left and right?) is enough to take action for a given token. Also, by stacking attention layers that have a small
window, the last layer will have a receptive field of more than just the tokens in the window, allowing them to build a
representation of the whole sentence.
Some preselected input tokens are also given global attention: for those few tokens, the attention matrix can access
all tokens and this process is symmetric: all other tokens have access to those specific tokens (on top of the ones in
their local window). This is shown in Figure 2d of the paper, see below for a sample attention mask:
Using those attention matrices with less parameters then allows the model to have inputs having a bigger sequence
length.
Axial positional encodings
Reformeruses axial positional encodings: in traditional transformer models, the positional encoding
E is a matrix of sizelllbyddd,lllbeing the sequence length anddddthe dimension of the
hidden state. If you have very long texts, this matrix can be huge and take way too much space on the GPU. To alleviate
that, axial positional encodings consist of factorizing that big matrix E in two smaller matrices E1 and E2, with
dimensionsl1√ód1l_{1} \times d_{1}l1‚Äã√ód1‚Äãandl2√ód2l_{2} \times d_{2}l2‚Äã√ód2‚Äã, such thatl1√ól2=ll_{1} \times l_{2} = ll1‚Äã√ól2‚Äã=landd1+d2=dd_{1} + d_{2} = dd1‚Äã+d2‚Äã=d(with the product for the lengths, this ends up being way smaller). The embedding for time
stepjjjin E is obtained by concatenating the embeddings for timestepj%l1j \% l1j%l1in E1 andj//l1j // l1j//l1in E2.
Conclusion
In this section, we‚Äôve explored the three main Transformer architectures and some specialized attention mechanisms. Understanding these architectural differences is crucial for selecting the right model for your specific NLP task.
As we move forward in the course, you‚Äôll get hands-on experience with these different architectures and learn how to fine-tune them for your specific needs. In the next section, we‚Äôll look at some of the limitations and biases present in these models that you should be aware of when deploying them.

LLM Course documentation
Ungraded quiz
LLM Course
and get access to the augmented documentation experience
to get started
Ungraded quiz
So far, this chapter has covered a lot of ground! Don‚Äôt worry if you didn‚Äôt grasp all the details, but it‚Äôs to reflect on what you‚Äôve learned so far with a quiz.
This quiz is ungraded, so you can try it as many times as you want. If you struggle with some questions, follow the tips and revisit the material. You‚Äôll be quizzed on this material again in the certification exam.
1. Explore the Hub and look for the roberta-large-mnli checkpoint. What task does it perform?
2. What will the following code return?
fromtransformersimportpipeline

ner = pipeline("ner", grouped_entities=True)
ner("My name is Sylvain and I work at Hugging Face in Brooklyn.")
3. What should replace ‚Ä¶ in this code sample?
fromtransformersimportpipeline

filler = pipeline("fill-mask", model="bert-base-cased")
result = filler("...")
4. Why will this code fail?
fromtransformersimportpipeline

classifier = pipeline("zero-shot-classification")
result = classifier("This is a course about the Transformers library")
5. What does ‚Äútransfer learning‚Äù mean?
6. True or false? A language model usually does not need labels for its pretraining.
7. Select the sentence that best describes the terms ‚Äúmodel‚Äù, ‚Äúarchitecture‚Äù, and ‚Äúweights‚Äù.
8. Which of these types of models would you use for completing prompts with generated text?
9. Which of those types of models would you use for summarizing texts?
10. Which of these types of models would you use for classifying text inputs according to certain labels?
11. What possible source can the bias observed in a model have?

LLM Course documentation
Deep dive into Text Generation Inference with LLMs
LLM Course
and get access to the augmented documentation experience
to get started
Deep dive into Text Generation Inference with LLMs
So far, we‚Äôve explored the transformer architecture in relation to a range of discrete tasks, like text classification or summarization. However, Large Language Models are most used for text generation, and this is what we‚Äôll explore in this chapter.
In this page, we‚Äôll explore the core concepts behind LLM inference, providing a comprehensive understanding of how these models generate text and the key components involved in the inference process.
Understanding the Basics
Let‚Äôs start with the fundamentals. Inference is the process of using a trained LLM to generate human-like text from a given input prompt. Language models use their knowledge from training to formulate responses one word at a time. The model leverages learned probabilities from billions of parameters to predict and generate the next token in a sequence. This sequential generation is what allows LLMs to produce coherent and contextually relevant text.
The Role of Attention
The attention mechanism is what gives LLMs their ability to understand context and generate coherent responses. When predicting the next word, not every word in a sentence carries equal weight - for example, in the sentence‚ÄúThe capital of France is ‚Ä¶‚Äù, the words ‚ÄúFrance‚Äù and ‚Äúcapital‚Äù are crucial for determining that ‚ÄúParis‚Äù should come next. This ability to focus on relevant information is what we call attention.
This process of identifying the most relevant words to predict the next token has proven to be incredibly effective. Although the basic principle of training LLMs‚Äîpredicting the next token‚Äîhas remained generally consistent since BERT and GPT-2, there have been significant advancements in scaling neural networks and making the attention mechanism work for longer and longer sequences, at lower and lower costs.
In short, the attention mechanism is the key to LLMs being able to generate text that is both coherent and context-aware. It sets modern LLMs apart from previous generations of language models.
Context Length and Attention Span
Now that we understand attention, let‚Äôs explore how much context an LLM can actually handle. This brings us to context length, or the model‚Äôs ‚Äòattention span‚Äô.
The context length refers to the maximum number of tokens (words or parts of words) that the LLM can process at once. Think of it as the size of the model‚Äôs working memory.
These capabilities are limited by several practical factors:
The model‚Äôs architecture and size
Available computational resources
The complexity of the input and desired output
In an ideal world, we could feed unlimited context to the model, but hardware constraints and computational costs make this impractical. This is why different models are designed with different context lengths to balance capability with efficiency.
The context length is the maximum number of tokens the model can consider at once when generating a response.
The Art of Prompting
When we pass information to LLMs, we structure our input in a way that guides the generation of the LLM toward the desired output. This is calledprompting.
Understanding how LLMs process information helps us craft better prompts. Since the model‚Äôs primary task is to predict the next token by analyzing the importance of each input token, the wording of your input sequence becomes crucial.
Careful design of the prompt makes it easierto guide the generation of the LLM toward the desired output.
The Two-Phase Inference Process
Now that we understand the basic components, let‚Äôs dive into how LLMs actually generate text. The process can be broken down into two main phases: prefill and decode. These phases work together like an assembly line, each playing a crucial role in producing coherent text.
The Prefill Phase
The prefill phase is like the preparation stage in cooking - it‚Äôs where all the initial ingredients are processed and made ready. This phase involves three key steps:
Tokenization: Converting the input text into tokens (think of these as the basic building blocks the model understands)
Embedding Conversion: Transforming these tokens into numerical representations that capture their meaning
Initial Processing: Running these embeddings through the model‚Äôs neural networks to create a rich understanding of the context
This phase is computationally intensive because it needs to process all input tokens at once. Think of it as reading and understanding an entire paragraph before starting to write a response.
You can experiment with different tokenizers in the interactive playground below:
The Decode Phase
After the prefill phase has processed the input, we move to the decode phase - this is where the actual text generation happens. The model generates one token at a time in what we call an autoregressive process (where each new token depends on all previous tokens).
The decode phase involves several key steps that happen for each new token:
Attention Computation: Looking back at all previous tokens to understand context
Probability Calculation: Determining the likelihood of each possible next token
Token Selection: Choosing the next token based on these probabilities
Continuation Check: Deciding whether to continue or stop generation
This phase is memory-intensive because the model needs to keep track of all previously generated tokens and their relationships.
Sampling Strategies
Now that we understand how the model generates text, let‚Äôs explore the various ways we can control this generation process. Just like a writer might choose between being more creative or more precise, we can adjust how the model makes its token selections.
You can interact with the basic decoding process yourself with SmolLM2 in this Space (remember, it decodes until reaching anEOStoken which is<|im_end|>for this model):
Understanding Token Selection: From Probabilities to Token Choices
When the model needs to choose the next token, it starts with raw probabilities (called logits) for every word in its vocabulary. But how do we turn these probabilities into actual choices? Let‚Äôs break down the process:
Raw Logits: Think of these as the model‚Äôs initial gut feelings about each possible next word
Temperature Control: Like a creativity dial - higher settings (>1.0) make choices more random and creative, lower settings (<1.0) make them more focused and deterministic
Top-p (Nucleus) Sampling: Instead of considering all possible words, we only look at the most likely ones that add up to our chosen probability threshold (e.g., top 90%)
Top-k Filtering: An alternative approach where we only consider the k most likely next words
Managing Repetition: Keeping Output Fresh
One common challenge with LLMs is their tendency to repeat themselves - much like a speaker who keeps returning to the same points. To address this, we use two types of penalties:
Presence Penalty: A fixed penalty applied to any token that has appeared before, regardless of how often. This helps prevent the model from reusing the same words.
Frequency Penalty: A scaling penalty that increases based on how often a token has been used. The more a word appears, the less likely it is to be chosen again.
These penalties are applied early in the token selection process, adjusting the raw probabilities before other sampling strategies are applied. Think of them as gentle nudges encouraging the model to explore new vocabulary.
Controlling Generation Length: Setting Boundaries
Just as a good story needs proper pacing and length, we need ways to control how much text our LLM generates. This is crucial for practical applications - whether we‚Äôre generating a tweet-length response or a full blog post.
We can control generation length in several ways:
Token Limits: Setting minimum and maximum token counts
Stop Sequences: Defining specific patterns that signal the end of generation
End-of-Sequence Detection: Letting the model naturally conclude its response
For example, if we want to generate a single paragraph, we might set a maximum of 100 tokens and use ‚Äú\n\n‚Äù as a stop sequence. This ensures our output stays focused and appropriately sized for its purpose.
Beam Search: Looking Ahead for Better Coherence
While the strategies we‚Äôve discussed so far make decisions one token at a time, beam search takes a more holistic approach. Instead of committing to a single choice at each step, it explores multiple possible paths simultaneously - like a chess player thinking several moves ahead.
Here‚Äôs how it works:
At each step, maintain multiple candidate sequences (typically 5-10)
For each candidate, compute probabilities for the next token
Keep only the most promising combinations of sequences and next tokens
Continue this process until reaching the desired length or stop condition
Select the sequence with the highest overall probability
You can explore beam search visually here:
This approach often produces more coherent and grammatically correct text, though it requires more computational resources than simpler methods.
Practical Challenges and Optimization
As we wrap up our exploration of LLM inference, let‚Äôs look at the practical challenges you‚Äôll face when deploying these models, and how to measure and optimize their performance.
Key Performance Metrics
When working with LLMs, four critical metrics will shape your implementation decisions:
Time to First Token (TTFT): How quickly can you get the first response? This is crucial for user experience and is primarily affected by the prefill phase.
Time Per Output Token (TPOT): How fast can you generate subsequent tokens? This determines the overall generation speed.
Throughput: How many requests can you handle simultaneously? This affects scaling and cost efficiency.
VRAM Usage: How much GPU memory do you need? This often becomes the primary constraint in real-world applications.
The Context Length Challenge
One of the most significant challenges in LLM inference is managing context length effectively. Longer contexts provide more information but come with substantial costs:
Memory Usage: Grows quadratically with context length
Processing Speed: Decreases linearly with longer contexts
Resource Allocation: Requires careful balancing of VRAM usage
Recent models likeQwen2.5-1Moffer impressive 1M token context windows, but this comes at the cost of significantly slower inference times. The key is finding the right balance for your specific use case.
The KV Cache Optimization
To address these challenges, one of the most powerful optimizations is KV (Key-Value) caching. This technique significantly improves inference speed by storing and reusing intermediate calculations. This optimization:
Reduces repeated calculations
Improves generation speed
Makes long-context generation practical
The trade-off is additional memory usage, but the performance benefits usually far outweigh this cost.
Conclusion
Understanding LLM inference is crucial for effectively deploying and optimizing these powerful models. We‚Äôve covered the key components:
The fundamental role of attention and context
The two-phase inference process
Various sampling strategies for controlling generation
Practical challenges and optimizations
By mastering these concepts, you‚Äôll be better equipped to build applications that leverage LLMs effectively and efficiently.
Remember that the field of LLM inference is rapidly evolving, with new techniques and optimizations emerging regularly. Stay curious and keep experimenting with different approaches to find what works best for your specific use cases.

LLM Course documentation
Bias and limitations
LLM Course
and get access to the augmented documentation experience
to get started
Bias and limitations
If your intent is to use a pretrained model or a fine-tuned version in production, please be aware that, while these models are powerful tools, they come with limitations. The biggest of these is that, to enable pretraining on large amounts of data, researchers often scrape all the content they can find, taking the best as well as the worst of what is available on the internet.
To give a quick illustration, let‚Äôs go back the example of afill-maskpipeline with the BERT model:
fill-mask
fromtransformersimportpipeline

unmasker = pipeline("fill-mask", model="bert-base-uncased")
result = unmasker("This man works as a [MASK].")print([r["token_str"]forrinresult])

result = unmasker("This woman works as a [MASK].")print([r["token_str"]forrinresult])
['lawyer','carpenter','doctor','waiter','mechanic']
['nurse','waitress','teacher','maid','prostitute']
When asked to fill in the missing word in these two sentences, the model gives only one gender-free answer (waiter/waitress). The others are work occupations usually associated with one specific gender ‚Äî and yes, prostitute ended up in the top 5 possibilities the model associates with ‚Äúwoman‚Äù and ‚Äúwork.‚Äù This happens even though BERT is one of the rare Transformer models not built by scraping data from all over the internet, but rather using apparently neutral data (it‚Äôs trained on theEnglish WikipediaandBookCorpusdatasets).
When you use these tools, you therefore need to keep in the back of your mind that the original model you are using could very easily generate sexist, racist, or homophobic content. Fine-tuning the model on your data won‚Äôt make this intrinsic bias disappear.

LLM Course documentation
Summary
LLM Course
and get access to the augmented documentation experience
to get started
Summary
In this chapter, you‚Äôve been introduced to the fundamentals of Transformer models, Large Language Models (LLMs), and how they‚Äôre revolutionizing AI and beyond.
Key concepts covered
Natural Language Processing and LLMs
We explored what NLP is and how Large Language Models have transformed the field. You learned that:
NLP encompasses a wide range of tasks from classification to generation
LLMs are powerful models trained on massive amounts of text data
These models can perform multiple tasks within a single architecture
Despite their capabilities, LLMs have limitations including hallucinations and bias
Transformer capabilities
You saw how thepipeline()function from ü§ó Transformers makes it easy to use pre-trained models for various tasks:
pipeline()
Text classification, token classification, and question answering
Text generation and summarization
Translation and other sequence-to-sequence tasks
Speech recognition and image classification
Transformer architecture
We discussed how Transformer models work at a high level, including:
The importance of the attention mechanism
How transfer learning enables models to adapt to specific tasks
The three main architectural variants: encoder-only, decoder-only, and encoder-decoder
Model architectures and their applications
A key aspect of this chapter was understanding which architecture to use for different tasks:
Modern LLM developments
You also learned about recent developments in the field:
How LLMs have grown in size and capability over time
The concept of scaling laws and how they guide model development
Specialized attention mechanisms that help models process longer sequences
The two-phase training approach of pretraining and instruction tuning
Practical applications
Throughout the chapter, you‚Äôve seen how these models can be applied to real-world problems:
Using the Hugging Face Hub to find and use pre-trained models
Leveraging the Inference API to test models directly in your browser
Understanding which models are best suited for specific tasks
Looking ahead
Now that you have a solid understanding of what Transformer models are and how they work at a high level, you‚Äôre ready to dive deeper into how to use them effectively. In the next chapters, you‚Äôll learn how to:
Use the Transformers library to load and fine-tune models
Process different types of data for model input
Adapt pre-trained models to your specific tasks
Deploy models for practical applications
The foundation you‚Äôve built in this chapter will serve you well as you explore more advanced topics and techniques in the coming sections.