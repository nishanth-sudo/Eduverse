

The world’s leading publication for data science, AI, and ML professionals.
LinkedIn
X
Talking to Kids About AIArtificial Intelligence“This is your brain on an LLM”, and other things you shouldn’t sayStephanie KirmerMay 216 min read
Talking to Kids About AI
“This is your brain on an LLM”, and other things you shouldn’t say
Agentic AI 101: Starting Your Journey Building AI AgentsArtificial IntelligenceLearn the fundamentals of how to create AI Agents.Gustavo SantosMay 212 min read
Agentic AI 101: Starting Your Journey Building AI Agents
Learn the fundamentals of how to create AI Agents.
Latest
Rust for Python Developers: Why You Should Take a Look at the Rust Programming LanguageProgrammingDiscover how Rust complements Python with speed, safety, and control — and why it’s worth…Niklas LangMay 213 min read
Rust for Python Developers: Why You Should Take a Look at the Rust Programming Language
Discover how Rust complements Python with speed, safety, and control — and why it’s worth…
A Farewell to APMs — The Future of Observability is MCP toolsArtificial IntelligenceLike many other fields, the world of observability is about to be turned upside downRoni DoverMay 110 min read
A Farewell to APMs — The Future of Observability is MCP tools
Like many other fields, the world of observability is about to be turned upside down
Step-by-Step Guide to Build and Deploy an LLM-Powered Chat with Memory in StreamlitLarge Language ModelsAnd monitor your API usage on Google Cloud ConsoleALESSANDRA COSTAMay 117 min read
Step-by-Step Guide to Build and Deploy an LLM-Powered Chat with Memory in Streamlit
And monitor your API usage on Google Cloud Console
Practical EigenvectorsData ScienceLearn 80% about what they are and their applications with 20% effortConstantin DumitrascuMay 121 min read
Practical Eigenvectors
Learn 80% about what they are and their applications with 20% effort
How AI Is Rewriting the Day-to-Day of Data ScientistsArtificial IntelligenceIn my past articles, I have explored and compared many AI tools, for example, Google’s…Yu DongMay 112 min read
How AI Is Rewriting the Day-to-Day of Data Scientists
In my past articles, I have explored and compared many AI tools, for example, Google’s…
How Would I Learn to Code with ChatGPT if I Had to Start AgainChatGPTExploring ChatGPT in my 15 years coding learning journey — Moving beyond copy-pastingLivia EllenMay 111 min read
How Would I Learn to Code with ChatGPT if I Had to Start Again
Exploring ChatGPT in my 15 years coding learning journey — Moving beyond copy-pasting
Modern GUI Applications for Computer Vision in PythonComputer VisionCreate computer vision applications with a modern graphical user interface in Python using OpenCV and…Florian TrautweilerApr 3015 min read
Modern GUI Applications for Computer Vision in Python
Create computer vision applications with a modern graphical user interface in Python using OpenCV and…
Why Are Convolutional Neural Networks Great For Images?Deep LearningHow data symmetry informs neural network architecturesCaroline ArnoldApr 304 min read
Why Are Convolutional Neural Networks Great For Images?
How data symmetry informs neural network architectures
Beyond Glorified Curve Fitting: Exploring the Probabilistic Foundations of Machine LearningMachine LearningAn introduction to probabilistic thinking — and why it’s the foundation for robust and explainable…Sarah SchürchApr 3012 min read
Beyond Glorified Curve Fitting: Exploring the Probabilistic Foundations of Machine Learning
An introduction to probabilistic thinking — and why it’s the foundation for robust and explainable…
Editor’s Picks
From FOMO to Opportunity: Analytical AI in the Era of LLM AgentsLarge Language ModelsWhy the gold rush toward LLM agents does not make analytical AI obsoleteShuai GuoApr 2913 min read
From FOMO to Opportunity: Analytical AI in the Era of LLM Agents
Why the gold rush toward LLM agents does not make analytical AI obsolete
Building a Scalable and Accurate Audio Interview Transcription Pipeline with Google GeminiLarge Language ModelsFrom prototype to production: real-world insights into building smarter transcription pipelines with LLMs.Ugo PradèreApr 2921 min read
Building a Scalable and Accurate Audio Interview Transcription Pipeline with Google Gemini
From prototype to production: real-world insights into building smarter transcription pipelines with LLMs.
How to Ensure Your AI Solution Does What You Expect iI to DoArtificial IntelligenceA Kind Introduction to AI EvalsAnna ViaApr 2810 min read
How to Ensure Your AI Solution Does What You Expect iI to Do
A Kind Introduction to AI Evals
When OpenAI Isn’t Always the Answer: Enterprise Risks Behind Wrapper-Based AI AgentsArtificial IntelligenceData privacy, compliance, and trust gaps in today’s AI agent integrationsLivia EllenApr 288 min read
When OpenAI Isn’t Always the Answer: Enterprise Risks Behind Wrapper-Based AI Agents
Data privacy, compliance, and trust gaps in today’s AI agent integrations
Hands-on Multi Agent LLM Restaurant Simulation, with Python and OpenAILarge Language ModelsThis is how I used Large Language Models Agents to simulate an end-to-end restaurant process,…Piero PaialungaApr 2812 min read
Hands-on Multi Agent LLM Restaurant Simulation, with Python and OpenAI
This is how I used Large Language Models Agents to simulate an end-to-end restaurant process,…
Choose the Right One: Evaluating Topic Models for Business IntelligenceMachine LearningPython tutorial for evaluating top-notch bigram topic models in customer email classificationPetr KorábApr 2411 min read
Choose the Right One: Evaluating Topic Models for Business Intelligence
Python tutorial for evaluating top-notch bigram topic models in customer email classification
How to Benchmark DeepSeek-R1 Distilled Models on GPQA Using Ollama and OpenAI’s simple-evalsLarge Language ModelsSet up and run the GPQA-Diamond benchmark on DeepSeek-R1’s distilled models locally to evaluate its…Kenneth LeungApr 2312 min read
How to Benchmark DeepSeek-R1 Distilled Models on GPQA Using Ollama and OpenAI’s simple-evals
Set up and run the GPQA-Diamond benchmark on DeepSeek-R1’s distilled models locally to evaluate its…
Enterprise AI: From Build-or-Buy to Partner-and-GrowArtificial IntelligenceHow can you get started, and who should implement your first AI projects?Dr. Janna LipenkovaApr 2218 min read
Enterprise AI: From Build-or-Buy to Partner-and-Grow
How can you get started, and who should implement your first AI projects?
AI Agents Processing Time Series and Large DataframesArtificial IntelligenceBuild from Scratch using only Python & Ollama (no GPU, no APIKEY)Mauro Di PietroApr 2210 min read
AI Agents Processing Time Series and Large Dataframes
Build from Scratch using only Python & Ollama (no GPU, no APIKEY)
Featured
(Many) More TDS Contributors Are Now Eligible for Earning Through the Author Payment ProgramWritingPlus: say hello to our new stats dashboardTDS EditorsApr 223 min read
(Many) More TDS Contributors Are Now Eligible for Earning Through the Author Payment Program
Plus: say hello to our new stats dashboard
How to Format Your TDS Draft: A Quick(ish) GuideWritingEverything you need to know about creating a draft on our Contributor PortalTDS EditorsMar 2812 min read
How to Format Your TDS Draft: A Quick(ish) Guide
Everything you need to know about creating a draft on our Contributor Portal
Deep Dives
Turning Product Data into Strategic DecisionsProduct ManagementFor business leaders, product analytics isn’t just about tracking KPIs or building dashboardsWeiwei HuApr 3014 min read
Turning Product Data into Strategic Decisions
For business leaders, product analytics isn’t just about tracking KPIs or building dashboards
Graph Neural Networks Part 4: Teaching Models to Connect the DotsMachine LearningHeuristic and GNN-based approaches to Link PredictionHennie de HarderApr 2912 min read
Graph Neural Networks Part 4: Teaching Models to Connect the Dots
Heuristic and GNN-based approaches to Link Prediction
The Secret Inner Lives of AI Agents: Understanding How Evolving AI Behavior Impacts Business RisksArtificial IntelligencePart 2 in a series on rethinking AI alignment and safety in the age of…Gadi SingerApr 2918 min read
The Secret Inner Lives of AI Agents: Understanding How Evolving AI Behavior Impacts Business Risks
Part 2 in a series on rethinking AI alignment and safety in the age of…
LLM Evaluations: from Prototype to ProductionArtificial IntelligenceHow to monitor the quality of your LLM productMariya MansurovaApr 2530 min read
LLM Evaluations: from Prototype to Production
How to monitor the quality of your LLM product
Government Funding Graph RAGData ScienceGraph visualisation for UK Research and Innovation (UKRI) funding, including NetworkX, PyVis and LlamaIndex graph…Lewis JamesApr 2419 min read
Government Funding Graph RAG
Graph visualisation for UK Research and Innovation (UKRI) funding, including NetworkX, PyVis and LlamaIndex graph…
Data Science: From School to Work, Part IVProgrammingGood practices for testing your Python projectsVincent MargotApr 2317 min read
Data Science: From School to Work, Part IV
Good practices for testing your Python projects

The world’s leading publication for data science, AI, and ML professionals.
LinkedIn
X
Talking to Kids About AIArtificial Intelligence“This is your brain on an LLM”, and other things you shouldn’t sayStephanie KirmerMay 2, 202516 min read
Talking to Kids About AI
“This is your brain on an LLM”, and other things you shouldn’t say
Agentic AI 101: Starting Your Journey Building AI AgentsArtificial IntelligenceLearn the fundamentals of how to create AI Agents.Gustavo SantosMay 2, 202512 min read
Agentic AI 101: Starting Your Journey Building AI Agents
Learn the fundamentals of how to create AI Agents.
Rust for Python Developers: Why You Should Take a Look at the Rust Programming LanguageProgrammingDiscover how Rust complements Python with speed, safety, and control — and why it’s worth…Niklas LangMay 2, 202513 min read
Rust for Python Developers: Why You Should Take a Look at the Rust Programming Language
Discover how Rust complements Python with speed, safety, and control — and why it’s worth…
A Farewell to APMs — The Future of Observability is MCP toolsArtificial IntelligenceLike many other fields, the world of observability is about to be turned upside downRoni DoverMay 1, 202510 min read
A Farewell to APMs — The Future of Observability is MCP tools
Like many other fields, the world of observability is about to be turned upside down
Step-by-Step Guide to Build and Deploy an LLM-Powered Chat with Memory in StreamlitLarge Language ModelsAnd monitor your API usage on Google Cloud ConsoleALESSANDRA COSTAMay 1, 202517 min read
Step-by-Step Guide to Build and Deploy an LLM-Powered Chat with Memory in Streamlit
And monitor your API usage on Google Cloud Console
Practical EigenvectorsData ScienceLearn 80% about what they are and their applications with 20% effortConstantin DumitrascuMay 1, 202521 min read
Practical Eigenvectors
Learn 80% about what they are and their applications with 20% effort
How AI Is Rewriting the Day-to-Day of Data ScientistsArtificial IntelligenceIn my past articles, I have explored and compared many AI tools, for example, Google’s…Yu DongMay 1, 202512 min read
How AI Is Rewriting the Day-to-Day of Data Scientists
In my past articles, I have explored and compared many AI tools, for example, Google’s…
How Would I Learn to Code with ChatGPT if I Had to Start AgainChatGPTExploring ChatGPT in my 15 years coding learning journey — Moving beyond copy-pastingLivia EllenMay 1, 202511 min read
How Would I Learn to Code with ChatGPT if I Had to Start Again
Exploring ChatGPT in my 15 years coding learning journey — Moving beyond copy-pasting
Modern GUI Applications for Computer Vision in PythonComputer VisionCreate computer vision applications with a modern graphical user interface in Python using OpenCV and…Florian TrautweilerApril 30, 202515 min read
Modern GUI Applications for Computer Vision in Python
Create computer vision applications with a modern graphical user interface in Python using OpenCV and…
Why Are Convolutional Neural Networks Great For Images?Deep LearningHow data symmetry informs neural network architecturesCaroline ArnoldApril 30, 20254 min read
Why Are Convolutional Neural Networks Great For Images?
How data symmetry informs neural network architectures

The world’s leading publication for data science, AI, and ML professionals.
LinkedIn
X
Editors Pick
Rust for Python Developers: Why You Should Take a Look at the Rust Programming LanguageProgrammingDiscover how Rust complements Python with speed, safety, and control — and why it’s worth…Niklas LangMay 2, 202513 min read
Rust for Python Developers: Why You Should Take a Look at the Rust Programming Language
Discover how Rust complements Python with speed, safety, and control — and why it’s worth…
A Farewell to APMs — The Future of Observability is MCP toolsArtificial IntelligenceLike many other fields, the world of observability is about to be turned upside downRoni DoverMay 1, 202510 min read
A Farewell to APMs — The Future of Observability is MCP tools
Like many other fields, the world of observability is about to be turned upside down
Practical EigenvectorsData ScienceLearn 80% about what they are and their applications with 20% effortConstantin DumitrascuMay 1, 202521 min read
Practical Eigenvectors
Learn 80% about what they are and their applications with 20% effort
How AI Is Rewriting the Day-to-Day of Data ScientistsArtificial IntelligenceIn my past articles, I have explored and compared many AI tools, for example, Google’s…Yu DongMay 1, 202512 min read
How AI Is Rewriting the Day-to-Day of Data Scientists
In my past articles, I have explored and compared many AI tools, for example, Google’s…
Beyond Glorified Curve Fitting: Exploring the Probabilistic Foundations of Machine LearningMachine LearningAn introduction to probabilistic thinking — and why it’s the foundation for robust and explainable…Sarah SchürchApril 30, 202512 min read
Beyond Glorified Curve Fitting: Exploring the Probabilistic Foundations of Machine Learning
An introduction to probabilistic thinking — and why it’s the foundation for robust and explainable…
From FOMO to Opportunity: Analytical AI in the Era of LLM AgentsLarge Language ModelsWhy the gold rush toward LLM agents does not make analytical AI obsoleteShuai GuoApril 29, 202513 min read
From FOMO to Opportunity: Analytical AI in the Era of LLM Agents
Why the gold rush toward LLM agents does not make analytical AI obsolete
Building a Scalable and Accurate Audio Interview Transcription Pipeline with Google GeminiLarge Language ModelsFrom prototype to production: real-world insights into building smarter transcription pipelines with LLMs.Ugo PradèreApril 29, 202521 min read
Building a Scalable and Accurate Audio Interview Transcription Pipeline with Google Gemini
From prototype to production: real-world insights into building smarter transcription pipelines with LLMs.
How to Ensure Your AI Solution Does What You Expect iI to DoArtificial IntelligenceA Kind Introduction to AI EvalsAnna ViaApril 28, 202510 min read
How to Ensure Your AI Solution Does What You Expect iI to Do
A Kind Introduction to AI Evals
When OpenAI Isn’t Always the Answer: Enterprise Risks Behind Wrapper-Based AI AgentsArtificial IntelligenceData privacy, compliance, and trust gaps in today’s AI agent integrationsLivia EllenApril 28, 20258 min read
When OpenAI Isn’t Always the Answer: Enterprise Risks Behind Wrapper-Based AI Agents
Data privacy, compliance, and trust gaps in today’s AI agent integrations
Hands-on Multi Agent LLM Restaurant Simulation, with Python and OpenAILarge Language ModelsThis is how I used Large Language Models Agents to simulate an end-to-end restaurant process,…Piero PaialungaApril 28, 202512 min read
Hands-on Multi Agent LLM Restaurant Simulation, with Python and OpenAI
This is how I used Large Language Models Agents to simulate an end-to-end restaurant process,…

The world’s leading publication for data science, AI, and ML professionals.
LinkedIn
X
Deep Dives
Step-by-Step Guide to Build and Deploy an LLM-Powered Chat with Memory in StreamlitLarge Language ModelsAnd monitor your API usage on Google Cloud ConsoleALESSANDRA COSTAMay 1, 202517 min read
Step-by-Step Guide to Build and Deploy an LLM-Powered Chat with Memory in Streamlit
And monitor your API usage on Google Cloud Console
Modern GUI Applications for Computer Vision in PythonComputer VisionCreate computer vision applications with a modern graphical user interface in Python using OpenCV and…Florian TrautweilerApril 30, 202515 min read
Modern GUI Applications for Computer Vision in Python
Create computer vision applications with a modern graphical user interface in Python using OpenCV and…
Turning Product Data into Strategic DecisionsProduct ManagementFor business leaders, product analytics isn’t just about tracking KPIs or building dashboardsWeiwei HuApril 30, 202514 min read
Turning Product Data into Strategic Decisions
For business leaders, product analytics isn’t just about tracking KPIs or building dashboards
Graph Neural Networks Part 4: Teaching Models to Connect the DotsMachine LearningHeuristic and GNN-based approaches to Link PredictionHennie de HarderApril 29, 202512 min read
Graph Neural Networks Part 4: Teaching Models to Connect the Dots
Heuristic and GNN-based approaches to Link Prediction
The Secret Inner Lives of AI Agents: Understanding How Evolving AI Behavior Impacts Business RisksArtificial IntelligencePart 2 in a series on rethinking AI alignment and safety in the age of…Gadi SingerApril 29, 202518 min read
The Secret Inner Lives of AI Agents: Understanding How Evolving AI Behavior Impacts Business Risks
Part 2 in a series on rethinking AI alignment and safety in the age of…
LLM Evaluations: from Prototype to ProductionArtificial IntelligenceHow to monitor the quality of your LLM productMariya MansurovaApril 25, 202530 min read
LLM Evaluations: from Prototype to Production
How to monitor the quality of your LLM product
Government Funding Graph RAGData ScienceGraph visualisation for UK Research and Innovation (UKRI) funding, including NetworkX, PyVis and LlamaIndex graph…Lewis JamesApril 24, 202519 min read
Government Funding Graph RAG
Graph visualisation for UK Research and Innovation (UKRI) funding, including NetworkX, PyVis and LlamaIndex graph…
Data Science: From School to Work, Part IVProgrammingGood practices for testing your Python projectsVincent MargotApril 23, 202517 min read
Data Science: From School to Work, Part IV
Good practices for testing your Python projects
MapReduce: How It Powers Scalable Data ProcessingData EngineeringAn overview of the MapReduce programming model and how it can be used to optimize…Jimin KangApril 22, 202515 min read
MapReduce: How It Powers Scalable Data Processing
An overview of the MapReduce programming model and how it can be used to optimize…
How to Use Gyroscope in Presentations, or Why Take a JoyCon to DPG2025ProgrammingThis article explores how browser-based computational notebooks —particularly the WLJS Notebook —can transform static slides into dynamic,…Kirill VasinApril 21, 202519 min read
How to Use Gyroscope in Presentations, or Why Take a JoyCon to DPG2025
This article explores how browser-based computational notebooks —particularly the WLJS Notebook —can transform static slides into dynamic,…

The world’s leading publication for data science, AI, and ML professionals.
LinkedIn
X
Write for Towards Data Science
Share your concepts, ideas, and codes with a broader audience
Quick Links:
Submission Guidelines
How to get your article ready for publication!
Adding and using images
Longform posts, columns, and online books
How to submit your work
FAQ
Why become a contributor?
We are looking for writers to propose up-to-date content focused on data science, machine learning, artificial intelligence and programming. If you love to write about these topics, read on!
Reach a broader audience with your articles.We are one of the most popular data science sites in the world. TDS started as a publication on Medium, amassing more than 700k followers and becoming the most-read publication on the site.Now on a self-hosted platform, TDS is the leading destination in the data science community.
Here are a few things we do to ensure your articles reach the largest audience possible:
Our independent domain (towardsdatascience.com) provides better visibility and direct traffic to your work.
We feature our best stories on our homepage, newsletter, and social media (LinkedIn,X, and more), and provide our authors with sophisticated publishing tools to better tell their stories.
We provide editorial support to help refine and amplify high-quality submissions.
Earn money with the Towards Data Science Author Payment Program. When publishing in TDS, our authors can decide to apply to our Payment Program, which enables them to earn from their work. You can read more about ourAuthor Payment Program here.
Submission Guidelines
Before submitting your article, there are a few essential things you need to know. Make sure you read each point well, and that you understand them, as by submitting an article to TDS, you are agreeing to comply with all of them.
Please take a few minutes to familiarize yourself with ourAuthor Terms and Conditions of Use— they govern the relationship between contributors and TDS.
Any article you share with us must be entirely your own original work; you can’t take other writers’ words and present them as your own, and we also don’t allow AI-generated text, even when you’re the one who prompted its creation.
Guidelines
How to get your article ready for publication!
We aim to strike a balance between innovating, informing and philosophizing. We want to hear from you! If you are not a professional writer, consider the following points when preparing your article. We want to publish high quality, professional articles that people want to read.
1. Is your story a story that needs to be told?
Before you start writing, ask yourself: is this story a story that needs to be told?
If you have read many articles addressing the same issue or explaining the same concept, think twice before writing another one. If you have a radical, new take on an old chestnut, we want to hear from you… but, we need you to persuade us that your article is something special that distinguishes itself from the pack and speaks to our audience.
Conversely, if your article addresses an underserved area or presents a new idea or method, that’s just what we are after!
2. What is your message?
Let us know what your main message is, right from the start. Give your piece a snappy introduction that tells us:
What is your novel idea?
Why should we care?
How are you going to prove your point?
Once you’ve got that out of the way, you can be as conversational as you like, but keep calling back to the central message and give us a solid conclusion.
Remember though, Towards Data Science is not your personal blog, keep it sharp and on-topic!
3. On the internet, nobody knows you are a dog
You’ve got a new idea or a new way of doing things, you want to tell the community and start a discussion. Fantastic, that’s what we want too, but we’re not going to take for granted that you know what you are talking about or that we should uncritically believe what you say… you’ve got to persuade us (your audience) that:
The subject matter is important
There is a gap that needs to be filled
You have the answer
Your solution works
Your idea is based on a logical progression of ideas and evidence
If you are giving us a tutorial, tell us why people would need to use this tool and why your way is better than the methods already published.
You can do this by explaining the background, showing examples, providing an experiment or just laying out how data you have extracted from various sources allowed you to synthesise this new idea.
Are there arguments that counter your opinion or your findings? Explain why that interpretation conflicts with your idea and why your idea comes out on top.
4. Do you have a short title with an insightful subtitle?
If you scroll up to the top of this page, you will see an example of a title and subtitle. Your post needs to have a short title and a longer subtitle that tell readers what your article is about or why they should read it. Your header is useful for attracting potential readers and making your intentions clear. To remain consistent and give readers the best experience possible, we do not allow titles or subtitles written in all-caps. We also ask that you avoid profanity in both your title and subtitle.
When your subtitle is directly under the title and formatted correctly, it will show up in some post previews, which helps with your click-through rate.
5. What makes your post valuable to readers?
A successful post has a clearly defined and well-scoped goal, and follows through on its promise. If your title tells us you’re going to unpack a complex algorithm, show the benefits of a new library, or walk us through your own data pipeline, make sure the rest of the post delivers.
Here are a few pointers to help you plan and execute a well-crafted post:
1. Decide what your topic is — and what it isn’tIf you’re not sure what your post is going to be about, there’s very little chance your audience will when they read it. Define the problem or question your article will tackle, and stick to it: anything that doesn’t address the core of your post should stay out.
2. Create a clear planWith your topic in hand, sketch out a clear structure for your post, and keep in mind the overall structure it’ll follow. Remember that your main goal is to keep your reader engaged and well-oriented, so it’s never too early to think about formatting and how you’ll break down the topic into digestible sections. Consider adding section headings along the way to make your structure visible.
3. Use clear, action-driven languageIf you’re still finding your personal voice as a data-science author, a good place to start is keeping things clean, clear, and easy to follow.
If your article is full of neutral, generic verbs (like to be, have, go, become, make, etc.), try to mix in more precise action verbs. When it makes sense, use specific, lively descriptors instead of dull ones (for example, you could replace “easy” with “frictionless,” “accessible,” or “straightforward,” depending on the context).
There are few things editors appreciate more than a clean first draft, so don’t forget to proofread your post a couple of times before sharing it with TDS: look for spelling, punctuation, and grammar issues, and do your best to fix them. What we hope to offer to our readers are clear explanations, a smooth overall flow — pay attention to those transitions! — and a strong sense of what you’re aiming to achieve with your post.
If you’d like to expand your toolkit beyond the basics, the Internet is full of great writing resources. Here are a few ideas to help you get started:
Writing for an academic journal: 10 tips. The Guardian
6 of the Best Pieces of Advice from Successful Writers
Steven Pinker’s Sense of Style
4. Include your own images, graphs, and gifsOne of the most effective ways to get your key points across to your readers is to illustrate them with your compelling visuals.
For example, if you’re talking about a data pipeline you built, text can only take you so far; adding a diagram or flowchart could make things even clearer. If you’re covering an algorithm or another abstract concept, make it more concrete with graphs, drawings, or gifs to complement your verbal descriptions. (If you’re using images someone else created, you’ll need to source and cite them carefully — read our image guidelines below for more details.)
A strong visual component will hook your readers’ attention and guide them along as they read your post. It will also help you develop a personal style as an author, grow your following, and draw more attention on social media.
6. Are your code and equations well displayed?
TDS readers love to tinker with the ideas and workflows you share with them, which means that including a code implementation and relevant equation(s) in your post is often a great idea.
To make code snippets more accessible and usable, avoid screenshots. Use WordPress’scode blocks & inline code
To share math equations with your readers,Embed.funis a great option. Alternatively, you canuse Unicode charactersand upload an image of the resulting equation.
When you include code or an equation within your article, be sure to explain it and add some context around it so readers of all levels can follow along.
To learn more about using these embeds and others in your post, check out thisresource.
7. Check your facts
Whenever you provide a fact, if it’s not self-evident, let us know where you learned it. Tell us who your sources are and where your data originated. If we want to have a conversation we all need to be on the same page. Maybe something you say will spark a discussion, but if we want to be sure we are not at cross purposes, we need to go back to the original and read for ourselves in case we are missing a vital piece of the puzzle that makes everything you say make sense.
8. Is your conclusion to the point and not promotional?
Please make sure that you include a conclusion at the end of your article. It’s a great way to help your readers review and remember the essential points or ideas you’ve covered. You can also use your conclusion to link an original post or a few relevant articles.
Adding an extra link to your author profile or to a social media account is fine, but please avoid call-to-action (CTA) buttons.
For yourreferences, please respect this format:
[X] N. Name, Title (Year), Source
For example, your first reference should look like this:
[1] A. Pesah, A. Wehenkel and G. Louppe,Recurrent Machines for Likelihood-Free Inference(2018), NeurIPS 2018 Workshop on Meta-Learning
9. Are your tags precise enough?
The more specific your tags, the easier it is for readers to find your article and for us to classify and recommend your post to the relevant audience.
We may change one or two tags before publication. We would do this only to keep our different sections relevant to our readers. For instance, we would want to avoid tagging a post on linear regression as “Artificial Intelligence”.
10. Do you have an amazing image?
A great image attracts and excites readers. That’s why all the best newspapers always display incredible pictures.
This is what you can do to add a fantastic featured image to your post:
UseUnsplash.Most of the content on Unsplash is fine to use without asking for permission. You can learn more about their licensehere.
Take one yourself. Your phone is almost certainly good enough to capture a cool image of your surroundings. You might even already have an image on your phone that would make a great addition to your article.
Make a great graph. If your post involves data analysis, spend some time making at least one graph truly unique. You can try R, Python, D3.js or Plotly.
If you decide to purchase a license for an image to be used in your article, please note that we only allow the use of images under a license that: (i) does not expire; and (ii) that can be used for commercial purposes on the TDS Publication. You are responsible for ensuring you comply with the license terms of use. You must also include a caption below the image, as follows, or as otherwise required by the license provider: “Image via [license provider’s name] under license to [your name].” Finally, please email us a copy of a receipt or other evidence of the purchased license, along with the corresponding license terms of use.
If you’ve chosen to create images for your article using an AI tool(like DALL·E 2, DALL·E, Midjourney, or Stable Diffusion, among others), it’s your responsibility to ensure that you’ve read, understood, and followed the tool’s terms. Any image you use on TDS must be licensed for commercial use, including AI-generated images. Not all AI tools permit images to be used for commercial purposes and some require payment to permit you to use the image.
The images you generate with AI tools cannot violate the copyright of other creators. If the AI generated image resembles or is identical to an existing copyrighted image or fictional character (like Harry Potter, Fred Flinstone etc.), you are not permitted to use it on TDS. Use your best judgment and avoid AI-generated images that copy or closely emulate another work. If in doubt, use an image search tool — likeGoogle Lens,TinEye, or others — to check whether your images are too similar to an existing work. We may also ask that you provide details of the text prompts you used in the AI tool to confirm you did not use the names of copyrighted works.
Your text prompts cannot use the names of real people, nor can your images be used if they feature a real person (whether a celebrity, politician, or anyone else).
Please remember to cite the source of your images even if you aren’t legally obligated to do so.If you created an image yourself, you can add (Image by author) in the caption. Whichever way you decide to go, your image source should look like this:
Your image should both have the source and the link to that source. If you created an image yourself, you can add “Image by author”.
If you’ve created an image that was lightly inspiredby an existing image, please add the caption “Image by Author, inspired by source[include the link].”If you’ve edited an existing image, please make sure you have the right to use and edit that image and include the caption “Image by source[include the link], edited with permission by the author.”
Danger zone: Do not use images (including logos and gifs) you found online without explicit permission from the owner. Adding the source to an image doesn’t grant you the right to use it.
11. Where did you get your data?
The Towards Data Science team is committed to the creation of a respectful community of data science authors, researchers, and readers. For our authors, this means respecting the work of others, taking care to honor copyrights associated with images, published material, and data. Please always ensure that you have the right to collect, analyze, and present the data you’re using in your article.
There are plenty of great sources of data that are freely available. Try searching university databases, government open data sites, and international institutions, such as theUCI Irvine Machine Learning Repository,U.S. Government, andWorld Bank Open Data. And don’t forget about sites that hold specific data relating to fields like physics, astrophysics, earth science, sports, and politics likeCERN,NASA, andFiveThirtyEight.
TDS is a commercial publication. Before submitting your article to us,please verify your dataset islicensed for commercial use, or obtain written permission to use it. Please note that not all the datasets on the websites we’ve listed are fine to use. No matter where you obtain your data, we advise you to double-check that the dataset permits commercial use.
If you aren’t confident you have the right to use it for commercial purposes, consider contacting the owner. Many authors receive a quick, positive response to awell-constructed email. Explain how you intend to use the data, share your article or idea, and provide a link to TDS. When you receive permission, please forward a copy to us at[email protected].
This is especially important if you plan to use web scraping to create your own dataset. If the website does not explicitly allow data scraping for commercial purposes, we strongly recommend that you contact the website owner for permission. Without explicit permission, we won’t be able to publish your work, so please forward us a copy via email.
And sometimes, simple works best! If you just want a dataset to explain how an algorithm works, you can always create an artificial or simulated dataset. Here’s a quicktutorial, and anarticlethat uses a simulated dataset you might find helpful.
Please remember to add a link to the site where the dataset is stored, and credit the owner/creator in your article. Ideally, this is done on first mention of the dataset, or in a resource list at the end of the article. Please carefully follow any instructions relating to attribution that you find on the site. If you have created your own artificial or simulated dataset, it is important to mention that too.
We know interpreting a license can be challenging. It is your responsibility to be certain that you can present your data and findings in an article published with TDS, but if you’re stuck, please reach out to our editorial team for assistance. We would rather work with you in the early stages of your project than to have to decline your completed article due to a dataset license issue.
12. Is your content original?
While we do accept content that has already been published (for example, on your personal blog or website), our focus is on promoting and sharing new and original content with our readers. That means that by publishing your article in TDS first (or exclusively), you have a greater chance to be featured on our publication, our social media channels, and in our newsletter.
We love original content because it’s something that our audience hasn’t seen before. We want to give as much exposure to new material as possible and keep TDS fresh and up-to-date.
Originality also means that you (and your coauthors, if any) are the sole creator of each and every element in your post. Any time you rely on someone else’s words, you have to cite and quote them properly, otherwise we consider it an instance of plagiarism. This applies to human authors, of course, but also to AI-generated text. We generally don’t allow any language created by tools like ChatGPT on TDS; if your article discusses these tools and you wish to include examples of text you generated, please keep them to a minimum, cite their source and the prompt you used, and make it very clear (for example, by using block quotes) where the AI-generated portions begin and end.
13. Did you get any feedback before submitting your post?
Get into the habit of always asking a friend for feedback before publishing your article. Having worked so hard on that article, you wouldn’t want to let a silly mistake push readers away.
14. Has your Author profile been completed correctly?
Please include yourreal name, aphoto, and abio. We don’t publish posts from anonymous writers — it’s easier to build trust with readers when they associate your words with an actual person.
Use your profile to introduce yourself, your expertise, your and achievements — optimizing it will help you develop a meaningful relationship with your audience beyond a single post.
If you are a company and would like to publish with us, please note that we almost exclusively publish articles submitted directly from the author.
15. Are you getting better?
Take a minute to reflect on the work you have been doing so far, and the current article you wish to publish. What value are you bringing, and to whom? In which ways are this article better or worse than the ones you previously published?
Longform posts, columns, and online books
Have a lot to say? Good. We love to dive deep into complex topics, and so do our readers. Here’s how you can publish longform posts, columns, and online books on TDS.
Longform posts
We love long reads! If your article’s reading time is shorter than 25 minutes, we recommend that youdon’tbreak it into multiple pieces — keep it as-is. A single post makes it easier for readers to search and find all the information they need, and less likely that they’ll miss an important part of your argument.
To create a smoother reading experience, you can add a table of contents to orient your audience around your post. Adding high-quality images and lots of white space is always a good idea, too — a long text doesn’t have to be a wall of text.
We regularly add the most engaging and thoughtful longform posts to ourDeep Dives page.
Columns
If your post’s reading time exceeds 25 minutes, or if you plan to focus on the same topic over multiple articles and a longer stretch of time, you can create your own TDS column. All it takes are three steps:
Add a custom tag to your post.This tag needs to be unique and reflect the theme of your project. Every time you publish a post with that tag, it will be added to your column’s landing page:towardsdatascience.com/tagged/[your-tag].
Add a kicker to your post.It’s like adding a subtitle but above your title.
Link your kickerto your column’s landing page.
You can create a TDS column and invite multiple authors to contribute. Just let your colleague(s) know which tag you decided to use so that they can add the same one to their articles.Hereare some examples from our team.
Online Books
A column is a great format to use if you have an open-ended topic that you plan to write about for a while. If, on the other hand, your idea has a finite, defined scope and a clear sense of progression from one post to the next, you may want to create a series of articles that feels more like an online book. Here is the format we recommend using.
Keep the reading time of each article — or “chapter” — between 12 to 25 minutes, and aim for a series that has at least 5 articles (but probably not more than, say, 16). You can add links to previous or subsequent items from within each article — for example, in the introduction and/or conclusion.
To publish your online book, you can submit all your articles to our editorial team in one go, or one by one as you finish working on each. We’ll review them and publish them as they come along. Let us know your post is part of a planned online book project.
Please ensure that each article or online book chapter follows the same guidelines and rules as any other post that TDS publishes. If you ever decide to sell or exclusively license your book to a third party publisher, you will have to make sure you have their consent to continue to publish the book with TDS. If you do not have such consent, it is your responsibility to remove your content from the TDS publication.
How To Submit Your Work
To become a writer, please send your article usingour form.
We aim to respond to authors as quickly as possible and to let them know whether or not we’ve accepted their articles.
On rare occasions, the volume of submissions we receive makes it difficult to respond to everyone; as a general rule, if you haven’t heard from us within a week of submitting your post, it’s safe to assume we won’t move forward with publishing it.
Contribute to Towards Data Science✨
If you’re having an issue with our online form, please let us know via email ([email protected]) so we can help you complete the process. Please do not email us an article that you have already sent via our form.
FAQ
Our FAQ can be foundhere.
Written By
Topics:
Share this article:
Share on Facebook
Share on LinkedIn
Share on X
Related Articles
Talking to Kids About AIArtificial Intelligence“This is your brain on an LLM”, and other things you shouldn’t sayStephanie KirmerMay 2, 202516 min read
Talking to Kids About AI
“This is your brain on an LLM”, and other things you shouldn’t say
Agentic AI 101: Starting Your Journey Building AI AgentsArtificial IntelligenceLearn the fundamentals of how to create AI Agents.Gustavo SantosMay 2, 202512 min read
Agentic AI 101: Starting Your Journey Building AI Agents
Learn the fundamentals of how to create AI Agents.
Rust for Python Developers: Why You Should Take a Look at the Rust Programming LanguageProgrammingDiscover how Rust complements Python with speed, safety, and control — and why it’s worth…Niklas LangMay 2, 202513 min read
Rust for Python Developers: Why You Should Take a Look at the Rust Programming Language
Discover how Rust complements Python with speed, safety, and control — and why it’s worth…
A Farewell to APMs — The Future of Observability is MCP toolsArtificial IntelligenceLike many other fields, the world of observability is about to be turned upside downRoni DoverMay 1, 202510 min read
A Farewell to APMs — The Future of Observability is MCP tools
Like many other fields, the world of observability is about to be turned upside down
Step-by-Step Guide to Build and Deploy an LLM-Powered Chat with Memory in StreamlitLarge Language ModelsAnd monitor your API usage on Google Cloud ConsoleALESSANDRA COSTAMay 1, 202517 min read
Step-by-Step Guide to Build and Deploy an LLM-Powered Chat with Memory in Streamlit
And monitor your API usage on Google Cloud Console
Practical EigenvectorsData ScienceLearn 80% about what they are and their applications with 20% effortConstantin DumitrascuMay 1, 202521 min read
Practical Eigenvectors
Learn 80% about what they are and their applications with 20% effort
How AI Is Rewriting the Day-to-Day of Data ScientistsArtificial IntelligenceIn my past articles, I have explored and compared many AI tools, for example, Google’s…Yu DongMay 1, 202512 min read
How AI Is Rewriting the Day-to-Day of Data Scientists
In my past articles, I have explored and compared many AI tools, for example, Google’s…

The world’s leading publication for data science, AI, and ML professionals.
LinkedIn
X
Talking to Kids About AI
“This is your brain on an LLM”, and other things you shouldn’t say
I’ve had thepleasantopportunity recently to be involved with a program calledSkype a Scientist, which pairs scientists of various types (biologists, botanists, engineers, computer scientists, etc) with classrooms of kids to talk about our work and answer their questions. I’m pretty familiar with discussing AI and machine learning with adult audiences, but this is the first time I’ve really sat down to think about how to talk to kids about this subject matter, and it’s been an interesting challenge. Today I’m going to share a few of the ideas I’ve come up with as part of the process, which may be useful to those of you with kids in your lives in some way.
Preparing to Explain Something
I have a few rules of thumb I follow when preparing any talk, for any audience. I need to be very clear in my own mind about what information I intend to impart, and what new things the audience should know after they leave, because this shapes everything about what information I’m going to share. I also want to present my material at an appropriate level of complexity for the audience’s preexisting knowledge — not talking down, but also not way over their heads.
In my day to day life, I’m not necessarily up to speed on what kids already know (or think they know) about AI. I want to make my explanations appropriate to the level of the audience, but in this case I have somewhat limited insight about where they’re coming from already. I have been surprised in some cases that the kids were actually quite aware of things like competition in AI between companies and across international boundaries. A useful exercise when deciding how to frame the content is coming up with metaphors that use concepts or technologies the audience is already very familiar with. Thinking about this also gives you an access point to where the audience is coming from. Beyond that, be prepared to pivot and adjust your presentation approach, if you determine that you’re not hitting the right level. I like to ask kids a little bit about what they think of AI and what they know at the start, so I can start to get that clarity before I’m too far along.
Understanding the Technology
With kids in particular, I’ve got a number of themes I want to cover in my presentations. Regular readers will knowI’m a big advocate for laypersons being taught what LLMs and other AI models are trained to do, and what their training data is, because that is vital for us to set realistic expectations for what the models’ results will be. I think it’s easy for anyone, kids included, to be taken in by the anthropomorphic nature of LLM tone, voice, and even “personality” and to lose track of the limitations in reality of what these tools can do.
It’s a challenge to make it simple enough to be age-appropriate, but once you tell them about how training works, and how an LLM learns from seeing examples of written material, or a diffusion model learns from text-image pairs, they can interpolate their own intuition about what the results of that might be. As AI agents become more complex, and the underlying mechanisms get tougher to separate out, it’s important for users to know about the building blocks that lead to this capability.
For myself, I start with explaining training as a general concept, avoiding as much technical jargon as possible. When talking to kids, a little anthropomorphizing language can help make things seem a little less mysterious. For example, “we give computers lots of information and ask them to learn the patterns inside.” Next, I’ll describe examples of patterns like those in language or image pixels, because “patterns” by itself is too general and vague. Then, “those patterns it learns are written down using math, and then that math is what is inside a ‘model’. Now, when we give new information to the model, it sends us a response that is based on the patterns it learned.” From there, I give another end to end example, and walk through the process of a simplified training (usually a time series model because it’s pretty easy to visualize). After this, I’ll go into more detail about different types of model, and explain what’s different about neural networks and language models, to the degree that’s appropriate for the audience.
AI Ethics and Externalities
I also want to cover ethical issues related to AI. I think kids who are in later elementary or middle grades and up are perfectly capable of understanding theenvironmentalandsocial impactsthat AI can have. Many kids today seem to me to be quite advanced in their understanding of global climate change and the environmental crisis, so talking about how much power, water, and rare mineral usage is required to run LLMs isn’t unreasonable. It’s just important to make your explanations relatable and age appropriate. As I mentioned earlier, use examples that are relatable and connect to the lived experiences of your audience.
Here’s an example of going from kid experience to the environmental impact of AI.
“So you all have chromebooks to use for homework, right? Do you ever find that when you sit with your laptop on your lap and do work for a long time that the back gets warm? Maybe if you have a lot of files open at once, or watch a lot of videos? So that heating up is the same thing that happens in big computers called servers that run when an LLM is trained or is used, like when you go on chatGPT’s website.
The data centers that keep chatGPT going are full of servers that are all running simultaneously, and all getting pretty darn hot, which isn’t good for the machinery. So, sometimes these data centers use cool water plus some chemicals together piped through tubes that go right over all the servers, and these help cool off the machines and keep them running. However, this means that a ton of water is being used, mixed with chemicals, and heated up as it goes through these systems, and it can mean that that water isn’t available for people to use for other things like farms or drinking water.
Other times, these data centers use big air conditioners, which take a lot of electricity to run, which means there may not be enough electricity for our houses or for businesses. Electricity is also sometimes made by burning coal in power plants, which puts out exhaust into the air and increases pollution too. ”
This brings the kid’s experience into the conversation, and gives them a tangible way to relate to the concept. You can do similar kinds of discussion around copyright ethics and stealing content, using artists and creators familiar to theChildren, without having to get deep in the weeds of IP law. Deepfakes, both sexual and otherwise, are certainly a topic lots of kids know about too, and it’s important that children are aware of the risks those present to individuals and the community as they use AI.
It can be scary, especially for younger kids, when they start to grasp some of the unethical applications of AI or global challenges it creates, and realize how powerful some of this stuff can be. I’ve had kids ask “how can we fix it if someone teaches AI to do bad things?”, for example. I wish I had better answers for that, because I had to essentially say “AI already sometimes has the information to do bad things, but there are also lots of people working hard to make AI more safe and prevent it from sharing any bad information or instructions on how to do bad things.”
Unpacking the Idea of “Truth”
The anthropomorphizing of AI problem is true for adults and kids both – we tend to trust a friendly, confident voice when it tells us things. A large part of the problem is that the LLM voice telling us things is frequently friendly, confident, and wrong. The concept of media literacy has been an important topic in pedagogy for years now, andexpanding this to LLMs is a natural progression. Just like students (and adults) need to learn to be critical consumers of information generated by other people or corporations, we need to be critical and thoughtful consumers of computer-generated content.
I think this goes along with understanding the tech, too. When I explain thatan LLM’s job is to learn and replicate human language, at the simplest level by selecting the probable next word in a series based on what came before, it makes sense when I go on to say that the LLM can’t understand the idea of “truth”. Truth isn’t part of the training process, and at the same time truth is a really hard concept even for people to figure out. The LLM might get the facts right frequently, but the blind spots and potential mistakes are going to show up from time to time, by the nature of probability. As a result, kids who use it need to be very conscious of the fallibility of the tool.
This lesson actually has value beyond just the use of AI, however, because what we’re teaching is about dealing with uncertainty, ambiguity, and mistakes. AsBearman and Ajjawi (2023)note, “pedagogy for an AI-mediated world involves learning to work with opaque, partial and ambiguous situations, which reflect the entangled relationships between people and technologies.” I really like this framing, because it comes back around to something I think about a lot — that LLMs are created by humans and reflect back interpretations of human-generated content. When kids learn how models come to exist, that models are fallible, and that their output originates from human-generated input, they’re getting familiar with the blurry nature of how technology works today in our society more broadly. (In fact, I highly recommend the article in full for anyone who’s thinking about how to teach kids about AI themselves.)
A side note on images and video
As I’ve written about before, the profusion of deepfake/”AI slop” video and image content online creates a lot of difficult questions. This is another area where I think giving kids information is important, because it’s easy to absorb misinformation or outright lies through convincing visual content. This content is also one step away from the actual creation process for most kids, as a lot of this material is being shared widely on social media, and is unlikely to be labeled. Talking to kids about what tell-tale signs help to detect AI generated material can help, as well as general critical media literacy skills like “if it’s too good to be true, it’s probably fake” and “double check things you hear in this kind of post”.
Cheating
However much we explain the ethical issues and the risks that the LLM will be wrong, these AI tools are incredibly useful and seductive, so it’s understandable that some kids will resort to using them to cheat on homework and in school. I’d like to say that we need to just reason with them, and explain that learning the skills to do the homework is the point, and if they don’t learn it they’ll be missing capabilities they need for future grades and later life… but we all know that kids are very rarely that logical. Their brains are still developing, and this sort of thing is hard even for adults to reason about at times.
There are two approaches you might take, essentially: find ways to make schoolwork harder or impossible to cheat on, or incorporate AI into the classroom under the assumption that kids are going to have it at their disposal in the future. Now, monitored work in a classroom setting can give kids a chance to learn some skills they need to have without digital mediation. However, as I mentioned earlier, media literacy really has to include LLMs now, and I think supervised use of LLMs by an informed instructor can have plenty of pedagogical value. In addition, it’s really impossible to “AI-proof” homework that’s done outside of direct instructor supervision, and we should recognize that. I don’t want to make it sound like this is easy, however — see below in theFurther Readingsection for a number of scholarly articles on the broad challenges of teaching AI literacy in the classroom. Teachers have a very challenging task to try not only to keep up on the technology themselves and evolve their pedagogy to fit the times, but also to try and give their students the information they need to use AI responsibly.
Learning from the Example of Sex Ed
In the end, the question is what exactly we ought to be recommending kids do and not do in a world that contains AI, in the classroom and beyond. I’m rarely an advocate for banning or prohibition of ideas, and I think the example of science-based, age-appropriate comprehensive sexEducationpresents a good lesson. If children are not given accurate information about their bodies and sexuality, they don’t have the knowledge necessary to make informed, responsible decisions in that area.We learned this when abstinence-only sex ed made teen pregnancy ratesgo through the roof in the early 2000’s.Adults will not be present to enforce mandates when kids are making the difficult decisions about what to do in challenging circumstances, so we need to make sure the kids are equipped with the information required to make those decisions responsibly themselves, and this includes ethical guidance but also factual information.
Modeling Responsibility
One last thing that I think is important to mention is that adults should be modeling responsible behavior with AI too. If teachers, parents, and other adults in kids’ lives are not critically literate about AI, then they aren’t going to be able to teach kids to be critical and thoughtful consumers of this technology either.
Arecent New York Times storyabout how teachers use AI made me a little frustrated. The article doesn’t reflect a great understanding of AI, conflating it with some basic statistics (a teacher analyzing student data to help personalize his teaching to their levels is both not AI and not new or problematic), but it does start a conversation about how adults in kids’ lives are using AI tools, and it mentions the need for those adults to model transparent and critical uses of it. (It also briefly grazes the issue of for-profit industry pushing AI into the classroom, which seems like a problem deserving more time — maybe I’ll write about that down the road.)
To counter one assertion of the piece, I wouldn’t complain about teachers using LLMs to do a first pass at grading written material, as long as they are monitoring and validating the output. If the grading criteria are around grammar, spelling, and writing mechanics, anLlmis probably suitable based on how it’s trained. I wouldn’t want to blindly trust an LLM on this without a human taking at least a quick look, but human language is in fact what it’s designed to understand. The idea that “the student had to write it, so the teacher should have to grade it” is silly, because the purpose of the exercise is for the student to learn. Teachers already know the writing mechanics, this is not a project that is meant to force teachers to learn something that is only achievable by manually grading. I think the NYT knows this, and that the framing was mostly for clickbait purposes, but it’s worth saying clearly.
This point goes back, once again, to my earlier section about understanding the technology. If you confidently understand what the training process looks like, then you can decide whether that process would produce a tool that’s capable of managing a task, or not. But automating grading has been part of schooling for decades at least — anyone who’s filled out a scantron sheet knows that.
This technology’s development is forcing some amount of adaptation in our education system, but we can’t put that genie back in the bottle now. There are definitely some ways that AI can have positive effects on education (often cited examples are personalization and saving teachers time that can then be put towards direct student services), but as with most things I’m an advocate for a realistic view. As I believe most educators are only too well aware, education can’t just go on as it did before LLMs entered our lives.
Conclusion
Kids are smarter than we sometimes give them credit for, and I think they are capable of understanding a lot about what AI means in our world. My advice is to be transparent and forthright about the realities of the technology, including advantages and disadvantages it represents to us as individuals and to our broader society. How we use it ourselves will model to kids either positive or negative choices that they’re going to notice, so being thoughtful about our actions as well as what we say is key.
For more of my work, visitwww.stephaniekirmer.com.
If you’d like to learn more about Skype a Scientist, visithttps://www.skypeascientist.com/
Further Reading
https://www.nytimes.com/2025/04/14/us/schools-ai-teachers-writing.html
https://pmc.ncbi.nlm.nih.gov/articles/PMC3194801
https://www.nyu.edu/about/news-publications/news/2022/february/federally-funded-sex-education-programs-linked-to-decline-in-tee.html
https://www.stephaniekirmer.com/writing/environmentalimplicationsoftheaiboom
https://www.stephaniekirmer.com/writing/seeingourreflectioninllms
https://www.stephaniekirmer.com/writing/machinelearningspublicperceptionproblem
https://www.stephaniekirmer.com/writing/whatdoesitmeanwhenmachinelearningmakesamistake
https://bera-journals.onlinelibrary.wiley.com/doi/full/10.1111/bjet.13337
https://www.sciencedirect.com/science/article/pii/S2666920X21000357
https://www.stephaniekirmer.com/writing/theculturalimpactofaigeneratedcontentpart1
Additional Articles about Pedagogical Approaches to AI
For anyone who’s teaching these topics or would like a deeper dive, here are a few articles I found interesting as I was researching this.
https://bera-journals.onlinelibrary.wiley.com/doi/full/10.1111/bjet.13337
https://dl.acm.org/doi/abs/10.1145/3408877.3432530— an early college level curriculum study
https://www.sciencedirect.com/science/article/pii/S2666920X22000169— a preschool/early elementary level curriculum study
https://dl.acm.org/doi/abs/10.1145/3311890.3311904— analysis of SES and national variation in AI learning among young children
Written By
Topics:
Share this article:
Share on Facebook
Share on LinkedIn
Share on X
Related Articles
3 AI Use Cases (That Are Not a Chatbot)Machine LearningFeature engineering, structuring unstructured data, and lead scoringShaw TalebiAugust 21, 20247 min read
3 AI Use Cases (That Are Not a Chatbot)
Feature engineering, structuring unstructured data, and lead scoring
Deep Dive into LSTMs & xLSTMs by Hand ✍️Deep LearningExplore the wisdom of LSTM leading into xLSTMs - a probable competition to the present-day LLMsSrijanie Dey, PhDJuly 9, 202413 min read
Deep Dive into LSTMs & xLSTMs by Hand ✍️
Explore the wisdom of LSTM leading into xLSTMs - a probable competition to the present-day LLMs
Build Your Own Modular Audio Course on AI Ethics and SafetyArtificial IntelligenceA hand-picked “listening list” on the questions and stakes at the forefront of artificial intelligence…TDS EditorsJuly 12, 20213 min read
Build Your Own Modular Audio Course on AI Ethics and Safety
A hand-picked “listening list” on the questions and stakes at the forefront of artificial intelligence…
How to Navigate AI’s Growing Social FootprintArtificial IntelligenceOur weekly selection of must-read Editors’ Picks and original featuresTDS EditorsMarch 14, 20243 min read
How to Navigate AI’s Growing Social Footprint
Our weekly selection of must-read Editors’ Picks and original features
A Red Pill Perspective On Degrees For Data Science & Machine LearningArtificial IntelligenceThe Truth Will Set You FreeKurtis PykesAugust 15, 20214 min read
A Red Pill Perspective On Degrees For Data Science & Machine Learning
The Truth Will Set You Free
Avoiding Burnout During a Career Change into Data ScienceArtificial IntelligenceNo, you don’t need to know 27 coding languages and have 512 portfolio projectsMatt ChapmanJune 15, 202313 min read
Avoiding Burnout During a Career Change into Data Science
No, you don’t need to know 27 coding languages and have 512 portfolio projects
Turn VS Code into a One-Stop Shop for ML ExperimentsData ScienceHow to run and evaluate experiments without leaving your IDEEryk LewinsonNovember 21, 202217 min read
Turn VS Code into a One-Stop Shop for ML Experiments
How to run and evaluate experiments without leaving your IDE

The world’s leading publication for data science, AI, and ML professionals.
LinkedIn
X
Artificial Intelligence
Talking to Kids About AIArtificial Intelligence“This is your brain on an LLM”, and other things you shouldn’t sayStephanie KirmerMay 2, 202516 min read
Talking to Kids About AI
“This is your brain on an LLM”, and other things you shouldn’t say
Agentic AI 101: Starting Your Journey Building AI AgentsArtificial IntelligenceLearn the fundamentals of how to create AI Agents.Gustavo SantosMay 2, 202512 min read
Agentic AI 101: Starting Your Journey Building AI Agents
Learn the fundamentals of how to create AI Agents.
A Farewell to APMs — The Future of Observability is MCP toolsArtificial IntelligenceLike many other fields, the world of observability is about to be turned upside downRoni DoverMay 1, 202510 min read
A Farewell to APMs — The Future of Observability is MCP tools
Like many other fields, the world of observability is about to be turned upside down
Step-by-Step Guide to Build and Deploy an LLM-Powered Chat with Memory in StreamlitLarge Language ModelsAnd monitor your API usage on Google Cloud ConsoleALESSANDRA COSTAMay 1, 202517 min read
Step-by-Step Guide to Build and Deploy an LLM-Powered Chat with Memory in Streamlit
And monitor your API usage on Google Cloud Console
How AI Is Rewriting the Day-to-Day of Data ScientistsArtificial IntelligenceIn my past articles, I have explored and compared many AI tools, for example, Google’s…Yu DongMay 1, 202512 min read
How AI Is Rewriting the Day-to-Day of Data Scientists
In my past articles, I have explored and compared many AI tools, for example, Google’s…
How Would I Learn to Code with ChatGPT if I Had to Start AgainChatGPTExploring ChatGPT in my 15 years coding learning journey — Moving beyond copy-pastingLivia EllenMay 1, 202511 min read
How Would I Learn to Code with ChatGPT if I Had to Start Again
Exploring ChatGPT in my 15 years coding learning journey — Moving beyond copy-pasting
Why Are Convolutional Neural Networks Great For Images?Deep LearningHow data symmetry informs neural network architecturesCaroline ArnoldApril 30, 20254 min read
Why Are Convolutional Neural Networks Great For Images?
How data symmetry informs neural network architectures
Beyond Glorified Curve Fitting: Exploring the Probabilistic Foundations of Machine LearningMachine LearningAn introduction to probabilistic thinking — and why it’s the foundation for robust and explainable…Sarah SchürchApril 30, 202512 min read
Beyond Glorified Curve Fitting: Exploring the Probabilistic Foundations of Machine Learning
An introduction to probabilistic thinking — and why it’s the foundation for robust and explainable…
Reinforcement Learning from One Example?Machine LearningWhy 1-shot RLVR might be the breakthrough we’ve been waiting forDerrick MwitiApril 30, 20254 min read
Reinforcement Learning from One Example?
Why 1-shot RLVR might be the breakthrough we’ve been waiting for
From FOMO to Opportunity: Analytical AI in the Era of LLM AgentsLarge Language ModelsWhy the gold rush toward LLM agents does not make analytical AI obsoleteShuai GuoApril 29, 202513 min read
From FOMO to Opportunity: Analytical AI in the Era of LLM Agents
Why the gold rush toward LLM agents does not make analytical AI obsolete

The world’s leading publication for data science, AI, and ML professionals.
LinkedIn
X
Author: Stephanie Kirmer
Talking to Kids About AIArtificial Intelligence“This is your brain on an LLM”, and other things you shouldn’t sayStephanie KirmerMay 2, 202516 min read
Talking to Kids About AI
“This is your brain on an LLM”, and other things you shouldn’t say
AI in Social Research and PollingArtificial IntelligenceWhat do we do when nobody answers the phone?Stephanie KirmerApril 1, 202513 min read
AI in Social Research and Polling
What do we do when nobody answers the phone?
Generative AI and Civic InstitutionsArtificial IntelligenceShould human obsolescence be our goal?Stephanie KirmerMarch 3, 202512 min read
Generative AI and Civic Institutions
Should human obsolescence be our goal?
The Cultural Backlash Against Generative AIArtificial IntelligenceWhat’s making many people resent generative AI, and what impact does that have on the…Stephanie KirmerFebruary 1, 202511 min read
The Cultural Backlash Against Generative AI
What’s making many people resent generative AI, and what impact does that have on the…
The Cultural Impact of AI Generated Content: Part 2What can we do about the increasingly sophisticated AI generated content in our lives?Stephanie KirmerJanuary 3, 202511 min read
The Cultural Impact of AI Generated Content: Part 2
What can we do about the increasingly sophisticated AI generated content in our lives?
The Cultural Impact of AI Generated Content: Part 1Generative AIWhat happens when AI generated media becomes ubiquitous in our lives? How does this relate…Stephanie KirmerDecember 3, 20249 min read
The Cultural Impact of AI Generated Content: Part 1
What happens when AI generated media becomes ubiquitous in our lives? How does this relate…
Choosing and Implementing Hugging Face ModelsMachine LearningPulling pre-trained models out of the box for your use caseStephanie KirmerNovember 1, 202410 min read
Choosing and Implementing Hugging Face Models
Pulling pre-trained models out of the box for your use case
A Critical Look at AI Image GenerationArtWhat does image generative AI really tell us about our world?Stephanie KirmerOctober 17, 202412 min read
A Critical Look at AI Image Generation
What does image generative AI really tell us about our world?
Consent in Training AIArtificial IntelligenceShould you have control over whether information about you gets used in training generative AI?Stephanie KirmerOctober 2, 202412 min read
Consent in Training AI
Should you have control over whether information about you gets used in training generative AI?
Disability, Accessibility, and AIArtificial IntelligenceA discussion of how AI can help and harm people with disabilitiesStephanie KirmerSeptember 16, 202411 min read
Disability, Accessibility, and AI
A discussion of how AI can help and harm people with disabilities

The world’s leading publication for data science, AI, and ML professionals.
LinkedIn
X
Agentic AI 101: Starting Your Journey Building AI Agents
Learn the fundamentals of how to create AI Agents.
Introduction
The Artificial Intelligence industry is moving fast. It is impressive and many times overwhelming.
I have been studying, learning, and building my foundations in this area of Data Science because I believe that the future of Data Science is strongly correlated with the development of Generative AI.
It was just the other day when I built my firstAi Agent, and then a couple of weeks after that, there were several Python packages to choose from, not to mention the no-code options that are doing very well, liken8n.
From “mere” models that could just chat with us to atsunamiof AI Agents that are everywhere, searching the Internet, handling files, and making wholeData Scienceprojects (from EDA to modeling and evaluation), all of that happened in just a couple of years.
What?
Seeing all of that, my thought was:“I need to get on board as soon as possible”. After all, it’s better to surf the wave than be swallowed by it.
For that reason, I decided to start this series of posts where I plan to go from the fundamentals to build our first AI Agent, until more complex concepts.
Enough talk, let’s dive in.
The Basics of AI Agents
An AI Agent is created when we give theLlmthe power to interact with tools and perform useful actions for us. So, instead of being just a chatbot, now it can schedule appointments, take care of our calendar, search the internet, write social media posts, and the list goes on…
AI Agents can do useful things, not just chat.
But how can we give that power to an LLM?
The simple answer is to use an API to interact with the LLM. There are several Python packages for that nowadays. If you follow my blog, you will see that I have already tried a couple of packages to build agents: Langchain, Agno (former PhiData), and CrewAI, for instance. For this series, I will stick with Agno [1].
First, set up a virtual environment usinguv, Anaconda, or the environment handler of your preference. Next, install the packages.
uv
# Agno AI
pip install agno

# module to interact with Gemini
pip install google-generativeai

# Install these other packages that will be needed throughout the tutorial
 pip install agno groq lancedb sentence-transformers tantivy youtube-transcript-api
# Agno AI
pip install agno

# module to interact with Gemini
pip install google-generativeai

# Install these other packages that will be needed throughout the tutorial
 pip install agno groq lancedb sentence-transformers tantivy youtube-transcript-api
Quick note before we continue. Don’t forget to get a Google Gemini API Key [2].
Creating a simple agent is very simple. All the packages are very similar. They have a classAgentor something similar that allows us to select a model and start interacting with the LLM of our choice. Here are the main components of this class:
Agent
model: the connection with the LLM. Here we will choose between OpenAI, Gemini, Llama, Deepseek etc.
model
description: This argument lets us describe the behavior of the agent. This is added to thesystem_message, which is a similar argument.
description
system_message
instructions: I like to think of an agent like an employee or an assistant that we’re managing. In order to have a task done, we must provide the instructions of what needs to be done. Here is where you can do that.
instructions
expected_output: Here we can give instructions about the expected output.
expected_output
tools: This is what makes the LLM an Agent, enabling it to interact with the real world using these tools.
tools
Now, let’s create a simple agent that has no tools, but will serve to build our intuition around the structure of the code.
# Imports
from agno.agent import Agent
from agno.models.google import Gemini
import os

# Create agent
agent = Agent(
    model= Gemini(id="gemini-1.5-flash",
                  api_key = os.environ.get("GEMINI_API_KEY")),
    description= "An assistant agent",
    instructions= ["Be sucint. Answer in a maximum of 2 sentences."],
    markdown= True
    )

# Run agent
response = agent.run("What's the weather like in NYC in May?")

# Print response
print(response.content)
# Imports
from agno.agent import Agent
from agno.models.google import Gemini
import os

# Create agent
agent = Agent(
    model= Gemini(id="gemini-1.5-flash",
                  api_key = os.environ.get("GEMINI_API_KEY")),
    description= "An assistant agent",
    instructions= ["Be sucint. Answer in a maximum of 2 sentences."],
    markdown= True
    )

# Run agent
response = agent.run("What's the weather like in NYC in May?")

# Print response
print(response.content)
########### OUTPUT ###############
Expect mild temperatures in NYC during May, typically ranging from the low 50s 
to the mid-70s Fahrenheit.  
There's a chance of rain, so packing layers and an umbrella is advisable.
########### OUTPUT ###############
Expect mild temperatures in NYC during May, typically ranging from the low 50s 
to the mid-70s Fahrenheit.  
There's a chance of rain, so packing layers and an umbrella is advisable.
That is great. We are using the Gemini 1.5 model. Notice how it responds based on the data it was trained on. If we ask it to tell us the weather today, we’ll see a response saying it can’t access the internet.
Let’s explore theinstructionsandexpected_outputarguments. We now want a table with the month, season and average temperature for NYC.
instructions
expected_output
# Imports
from agno.agent import Agent
from agno.models.google import Gemini
import os

# Create agent
agent = Agent(
    model= Gemini(id="gemini-1.5-flash",
                  api_key = os.environ.get("GEMINI_API_KEY")),
    description= "An assistant agent",
    instructions= ["Be sucint. Return a markdown table"],
    expected_output= "A table with month, season and average temperature",	
    markdown= True
    )

# Run agent
response = agent.run("What's the weather like in NYC for each month of the year?")

# Print response
print(response.content)
# Imports
from agno.agent import Agent
from agno.models.google import Gemini
import os

# Create agent
agent = Agent(
    model= Gemini(id="gemini-1.5-flash",
                  api_key = os.environ.get("GEMINI_API_KEY")),
    description= "An assistant agent",
    instructions= ["Be sucint. Return a markdown table"],
    expected_output= "A table with month, season and average temperature",	
    markdown= True
    )

# Run agent
response = agent.run("What's the weather like in NYC for each month of the year?")

# Print response
print(response.content)
And there’s the result.
Tools
The previous responses are nice. But we naturally don’t want to use powerful models such as LLMs to play with a chatbot or tell us old news, right?
We want them to be a bridge to automation, productivity, and knowledge. So, theToolswill add capabilities to our AI Agents, building, therefore, the bridge with the real world. Common examples of tools for agents are: searching the web, running SQL, sending an email or calling APIs.
But more than that, we can create custom capabilities to our agents by using any Python function as a tool.
Toolsare functions that an Agent can run to achieve tasks.
In terms of code, adding a tool to the Agent is just a matter of using the argumenttoolsin theAgentclass.
tools
Agent
Imagine a solopreneur (one-person company) in the healthy living business who wants to automate their content generation. This person posts tips about healthy habits every day. I know for a fact that content generation is not as straightforward as it looks like. It demands creativity, research, and copywriting skills. So, if this could be automated, or at least part of it, that’s time saved.
So we write this code to create a very simple agent that can generate a simple Instagram post and save it to a markdown file for our review. We reduced the process from thinking > researching > writing > reviewing > posting to reviewing > posting.
# Imports
import os
from agno.agent import Agent
from agno.models.google import Gemini
from agno.tools.file import FileTools

# Create agent
agent = Agent(
    model= Gemini(id="gemini-1.5-flash",
                  api_key = os.environ.get("GEMINI_API_KEY")),
                  description= "You are a social media marketer specialized in creating engaging content.",
                  tools=[FileTools(
                      read_files=True, 
                      save_files=True
                      )],
                  show_tool_calls=True)


# Writing and saving a file
agent.print_response("""Write a short post for instagram with tips and tricks
                        that positions me as an authority in healthy eating 
                        and save it to a file named 'post.txt'.""",
                     markdown=True)
# Imports
import os
from agno.agent import Agent
from agno.models.google import Gemini
from agno.tools.file import FileTools

# Create agent
agent = Agent(
    model= Gemini(id="gemini-1.5-flash",
                  api_key = os.environ.get("GEMINI_API_KEY")),
                  description= "You are a social media marketer specialized in creating engaging content.",
                  tools=[FileTools(
                      read_files=True, 
                      save_files=True
                      )],
                  show_tool_calls=True)


# Writing and saving a file
agent.print_response("""Write a short post for instagram with tips and tricks
                        that positions me as an authority in healthy eating 
                        and save it to a file named 'post.txt'.""",
                     markdown=True)
As a result, we have the following.
Unlock Your Best Self Through Healthy Eating:

1. Prioritize whole foods: Load up on fruits, vegetables, lean proteins, and whole
 grains.  They're packed with nutrients and keep you feeling full and energized.
2. Mindful eating:  Pay attention to your body's hunger and fullness cues. 
Avoid distractions while eating.
3. Hydrate, hydrate, hydrate: Water is crucial for digestion, energy levels, 
and overall health.
4. Don't deprive yourself:  Allow for occasional treats.  
Deprivation can lead to overeating later.  Enjoy everything in moderation!
5. Plan ahead:  Prep your meals or snacks in advance to avoid unhealthy 
impulse decisions.

#healthyeating #healthylifestyle #nutrition #foodie 
#wellbeing #healthytips #eatclean #weightloss #healthyrecipes 
#nutritiontips #instahealth #healthyfood #mindfuleating #wellnessjourney 
#healthcoach
Unlock Your Best Self Through Healthy Eating:

1. Prioritize whole foods: Load up on fruits, vegetables, lean proteins, and whole
 grains.  They're packed with nutrients and keep you feeling full and energized.
2. Mindful eating:  Pay attention to your body's hunger and fullness cues. 
Avoid distractions while eating.
3. Hydrate, hydrate, hydrate: Water is crucial for digestion, energy levels, 
and overall health.
4. Don't deprive yourself:  Allow for occasional treats.  
Deprivation can lead to overeating later.  Enjoy everything in moderation!
5. Plan ahead:  Prep your meals or snacks in advance to avoid unhealthy 
impulse decisions.

#healthyeating #healthylifestyle #nutrition #foodie 
#wellbeing #healthytips #eatclean #weightloss #healthyrecipes 
#nutritiontips #instahealth #healthyfood #mindfuleating #wellnessjourney 
#healthcoach
Certainly, we could make it much more fancy by creating a crew with other agents to search a list of websites for content, a content checker and reviewer, and another one to generate an image for the post. But I believe you got the general idea of how to add atoolto anAgent.
tool
Agent
Another type of tool we can add is thefunctiontool. We can use a Python function to serve as a tool for the LLM. Just don’t forget to add the type hints likevideo_id:str, so the model knows what to use as the function’s input. Otherwise, you might see an error.
video_id:str
Let’s see briefly how that works.
We now want our Agent to be able to get a given YouTube video and summarize it. To perform such a task, we simply create a function that downloads the transcript of the video from YT and passes it to the model to summarize.
# Imports
import os
from agno.agent import Agent
from agno.models.google import Gemini
from youtube_transcript_api import YouTubeTranscriptApi

# Get YT transcript
def get_yt_transcript(video_id:str) -> str:
      
    """
    Use this function to get the transcript from a YouTube video using the video id.

    Parameters
    ----------
    video_id : str
        The id of the YouTube video.
    Returns
    -------
    str
        The transcript of the video.
    """

    # Instantiate
    ytt_api = YouTubeTranscriptApi()
    # Fetch
    yt = ytt_api.fetch(video_id)
    # Return
    return ''.join([line.text for line in yt])


# Create agent
agent = Agent(
    model= Gemini(id="gemini-1.5-flash",
                  api_key = os.environ.get("GEMINI_API_KEY")),
                  description= "You are an assistant that summarizes YouTube videos.",
                  tools=[get_yt_transcript],
                  expected_output= "A summary of the video with the 5 main points and 2 questions for me to test my understanding.",
                  markdown=True,
                  show_tool_calls=True)


# Run agent
agent.print_response("""Summarize the text of the video with the id 'hrZSfMly_Ck' """,
                     markdown=True)
# Imports
import os
from agno.agent import Agent
from agno.models.google import Gemini
from youtube_transcript_api import YouTubeTranscriptApi

# Get YT transcript
def get_yt_transcript(video_id:str) -> str:
      
    """
    Use this function to get the transcript from a YouTube video using the video id.

    Parameters
    ----------
    video_id : str
        The id of the YouTube video.
    Returns
    -------
    str
        The transcript of the video.
    """

    # Instantiate
    ytt_api = YouTubeTranscriptApi()
    # Fetch
    yt = ytt_api.fetch(video_id)
    # Return
    return ''.join([line.text for line in yt])


# Create agent
agent = Agent(
    model= Gemini(id="gemini-1.5-flash",
                  api_key = os.environ.get("GEMINI_API_KEY")),
                  description= "You are an assistant that summarizes YouTube videos.",
                  tools=[get_yt_transcript],
                  expected_output= "A summary of the video with the 5 main points and 2 questions for me to test my understanding.",
                  markdown=True,
                  show_tool_calls=True)


# Run agent
agent.print_response("""Summarize the text of the video with the id 'hrZSfMly_Ck' """,
                     markdown=True)
And then you have a result.
Agents with Reasoning
Another cool option from the Agno package is allowing us to easily create agents that can analyze the situation before answering a question. That’s the reasoning tool.
We will create a reasoning agent with Alibaba’s Qwen-qwq-32b model. Notice that the only difference here, besides the model, is that we’re adding the toolReasoningTools().
ReasoningTools()
Theadding_instructions=Truemeans providing detailed instructions to the agent, which enhances the reliability and accuracy of its tool usage, while setting this toFalseforces the agent to depend on its own reasoning, which can be more prone to errors.
adding_instructions=True
False
# Imports
import os
from agno.agent import Agent
from agno.models.groq import Groq
from agno.tools.reasoning import ReasoningTools


# Create agent with reasoning
agent = Agent(
    model= Groq(id="qwen-qwq-32b",
                  api_key = os.environ.get("GROQ_API_KEY")),
                  description= "You are an experienced math teacher.",
                  tools=[ReasoningTools(add_instructions=True)],
                  show_tool_calls=True)


# Writing and saving a file
agent.print_response("""Explain the concept of sin and cosine in simple terms.""",
                     stream=True,
                     show_full_reasoning=True,
                     markdown=True)
# Imports
import os
from agno.agent import Agent
from agno.models.groq import Groq
from agno.tools.reasoning import ReasoningTools


# Create agent with reasoning
agent = Agent(
    model= Groq(id="qwen-qwq-32b",
                  api_key = os.environ.get("GROQ_API_KEY")),
                  description= "You are an experienced math teacher.",
                  tools=[ReasoningTools(add_instructions=True)],
                  show_tool_calls=True)


# Writing and saving a file
agent.print_response("""Explain the concept of sin and cosine in simple terms.""",
                     stream=True,
                     show_full_reasoning=True,
                     markdown=True)
Follows the output.
Agent with Knowledge
This tool is the easiest way for me to create a Retrieval Augmented Generation (RAG). With this feature, you can point the agent to a website or a list of websites, and it will add the content to a vector database. Then, it becomes searchable. Once asked, the agent can use the content as part of the answer.
In this simple example, I added one page of my website and asked the agent what books are listed there.
# Imports
import os
from agno.agent import Agent
from agno.models.google import Gemini
from agno.knowledge.url import UrlKnowledge
from agno.vectordb.lancedb import LanceDb, SearchType
from agno.embedder.sentence_transformer import SentenceTransformerEmbedder

# Load webpage to the knowledge base
agent_knowledge = UrlKnowledge(
    urls=["https://gustavorsantos.me/?page_id=47"],
    vector_db=LanceDb(
        uri="tmp/lancedb",
        table_name="projects",
        search_type=SearchType.hybrid,
        # Use Sentence Transformer for embeddings
        embedder=SentenceTransformerEmbedder(),
    ),
)

# Create agent
agent = Agent(
    model=Gemini(id="gemini-2.0-flash", api_key=os.environ.get("GEMINI_API_KEY")),
    instructions=[
        "Use tables to display data.",
        "Search your knowledge before answering the question.",
        "Only inlcude the content from the agent_knowledge base table 'projects'",
        "Only include the output in your response. No other text.",
    ],
    knowledge=agent_knowledge,
    add_datetime_to_instructions=True,
    markdown=True,
)

if __name__ == "__main__":
    # Load the knowledge base, you can comment out after first run
    # Set recreate to True to recreate the knowledge base if needed
    agent.knowledge.load(recreate=False)
    agent.print_response(
        "What are the two books listed in the 'agent_knowledge'",
        stream=True,
        show_full_reasoning=True,
        stream_intermediate_steps=True,
    )
# Imports
import os
from agno.agent import Agent
from agno.models.google import Gemini
from agno.knowledge.url import UrlKnowledge
from agno.vectordb.lancedb import LanceDb, SearchType
from agno.embedder.sentence_transformer import SentenceTransformerEmbedder

# Load webpage to the knowledge base
agent_knowledge = UrlKnowledge(
    urls=["https://gustavorsantos.me/?page_id=47"],
    vector_db=LanceDb(
        uri="tmp/lancedb",
        table_name="projects",
        search_type=SearchType.hybrid,
        # Use Sentence Transformer for embeddings
        embedder=SentenceTransformerEmbedder(),
    ),
)

# Create agent
agent = Agent(
    model=Gemini(id="gemini-2.0-flash", api_key=os.environ.get("GEMINI_API_KEY")),
    instructions=[
        "Use tables to display data.",
        "Search your knowledge before answering the question.",
        "Only inlcude the content from the agent_knowledge base table 'projects'",
        "Only include the output in your response. No other text.",
    ],
    knowledge=agent_knowledge,
    add_datetime_to_instructions=True,
    markdown=True,
)

if __name__ == "__main__":
    # Load the knowledge base, you can comment out after first run
    # Set recreate to True to recreate the knowledge base if needed
    agent.knowledge.load(recreate=False)
    agent.print_response(
        "What are the two books listed in the 'agent_knowledge'",
        stream=True,
        show_full_reasoning=True,
        stream_intermediate_steps=True,
    )
Agent with Memory
The last type we will go over in this post is the agent with memory.
This type of agent can store and retrieve information about users from previous interactions, allowing it to learn user preferences and personalize its responses.
Let’s see this example where I will tell a couple of things to the agent and ask for recommendations based on that interaction.
# imports
import os
from agno.agent import Agent
from agno.memory.v2.db.sqlite import SqliteMemoryDb
from agno.memory.v2.memory import Memory
from agno.models.google import Gemini
from rich.pretty import pprint

# User Name
user_id = "data_scientist"

# Creating a memory database
memory = Memory(
    db=SqliteMemoryDb(table_name="memory", 
                      db_file="tmp/memory.db"),
    model=Gemini(id="gemini-2.0-flash", 
                 api_key=os.environ.get("GEMINI_API_KEY"))
                 )

# Clear the memory before start
memory.clear()

# Create the agent
agent = Agent(
    model=Gemini(id="gemini-2.0-flash", api_key=os.environ.get("GEMINI_API_KEY")),
    user_id=user_id,
    memory=memory,
    # Enable the Agent to dynamically create and manage user memories
    enable_agentic_memory=True,
    add_datetime_to_instructions=True,
    markdown=True,
)


# Run the code
if __name__ == "__main__":
    agent.print_response("My name is Gustavo and I am a Data Scientist learning about AI Agents.")
    memories = memory.get_user_memories(user_id=user_id)
    print(f"Memories about {user_id}:")
    pprint(memories)
    agent.print_response("What topic should I study about?")
    agent.print_response("I write articles for Towards Data Science.")
    print(f"Memories about {user_id}:")
    pprint(memories)
    agent.print_response("Where should I post my next article?")
# imports
import os
from agno.agent import Agent
from agno.memory.v2.db.sqlite import SqliteMemoryDb
from agno.memory.v2.memory import Memory
from agno.models.google import Gemini
from rich.pretty import pprint

# User Name
user_id = "data_scientist"

# Creating a memory database
memory = Memory(
    db=SqliteMemoryDb(table_name="memory", 
                      db_file="tmp/memory.db"),
    model=Gemini(id="gemini-2.0-flash", 
                 api_key=os.environ.get("GEMINI_API_KEY"))
                 )

# Clear the memory before start
memory.clear()

# Create the agent
agent = Agent(
    model=Gemini(id="gemini-2.0-flash", api_key=os.environ.get("GEMINI_API_KEY")),
    user_id=user_id,
    memory=memory,
    # Enable the Agent to dynamically create and manage user memories
    enable_agentic_memory=True,
    add_datetime_to_instructions=True,
    markdown=True,
)


# Run the code
if __name__ == "__main__":
    agent.print_response("My name is Gustavo and I am a Data Scientist learning about AI Agents.")
    memories = memory.get_user_memories(user_id=user_id)
    print(f"Memories about {user_id}:")
    pprint(memories)
    agent.print_response("What topic should I study about?")
    agent.print_response("I write articles for Towards Data Science.")
    print(f"Memories about {user_id}:")
    pprint(memories)
    agent.print_response("Where should I post my next article?")
And here we end this first post about AI Agents.
Before You Go
There’s a lot of content in this post. We climbed the first step in this ladder of learning about AI agents. I know, it is overwhelming. There is so much information out there that it becomes harder and harder to know where to start and what to study.
My suggestion is to take the same road I am taking. One step at a time, choosing just a couple of packages like Agno, CrewAI, and going deep on those, learning how to create more complex agents each time.
In this post, we started from scratch, learning how to simply interact with an LLM to creating agents with memory, or even creating a simple RAG for an AI Agent.
Obviously, there is much more you can do just with a single agent. Check out the Reference [4].
With these simple skills, be sure that you are ahead of a lot of people, and there is a lot you can do already. Just use the creativity and (why not?) ask for the help of an LLM to build something cool!
In the next post, we will learn more about agents and evaluation. Stay tuned!
GitHub Repository
https://github.com/gurezende/agno-ai-labs
Contact and Online Presence
If you liked this content, find more of my work and social media in my website:
https://gustavorsantos.me
References
[1]https://docs.agno.com/introduction
[2]https://ai.google.dev/gemini-api/docs
[3]https://pypi.org/project/youtube-transcript-api/
[4]https://github.com/agno-agi/agno/tree/main/cookbook
[5]https://docs.agno.com/introduction/agents#agent-with-knowledge
Written By
Topics:
Share this article:
Share on Facebook
Share on LinkedIn
Share on X
Related Articles
Implementing Convolutional Neural Networks in TensorFlowArtificial IntelligenceStep-by-step code guide to building a Convolutional Neural NetworkShreya RaoAugust 20, 20246 min read
Implementing Convolutional Neural Networks in TensorFlow
Step-by-step code guide to building a Convolutional Neural Network
Hands-on Time Series Anomaly Detection using Autoencoders, with PythonData ScienceHere’s how to use Autoencoders to detect signals with anomalies in a few lines of…Piero PaialungaAugust 21, 202412 min read
Hands-on Time Series Anomaly Detection using Autoencoders, with Python
Here’s how to use Autoencoders to detect signals with anomalies in a few lines of…
3 AI Use Cases (That Are Not a Chatbot)Machine LearningFeature engineering, structuring unstructured data, and lead scoringShaw TalebiAugust 21, 20247 min read
3 AI Use Cases (That Are Not a Chatbot)
Feature engineering, structuring unstructured data, and lead scoring
Back To Basics, Part Uno: Linear Regression and Cost FunctionData ScienceAn illustrated guide on essential machine learning conceptsShreya RaoFebruary 3, 20236 min read
Back To Basics, Part Uno: Linear Regression and Cost Function
An illustrated guide on essential machine learning concepts
Must-Know in Statistics: The Bivariate Normal Projection ExplainedData ScienceDerivation and practical examples of this powerful conceptLuigi BattistoniAugust 14, 20247 min read
Must-Know in Statistics: The Bivariate Normal Projection Explained
Derivation and practical examples of this powerful concept
Our ColumnsData ScienceColumns on TDS are carefully curated collections of posts on a particular idea or category…TDS EditorsNovember 14, 20204 min read
Our Columns
Columns on TDS are carefully curated collections of posts on a particular idea or category…
Optimizing Marketing Campaigns with Budgeted Multi-Armed BanditsData ScienceWith demos, our new solution, and a videoVadim ArzamasovAugust 16, 202410 min read
Optimizing Marketing Campaigns with Budgeted Multi-Armed Bandits
With demos, our new solution, and a video

The world’s leading publication for data science, AI, and ML professionals.
LinkedIn
X
Author: Gustavo Santos
Agentic AI 101: Starting Your Journey Building AI AgentsArtificial IntelligenceLearn the fundamentals of how to create AI Agents.Gustavo SantosMay 2, 202512 min read
Agentic AI 101: Starting Your Journey Building AI Agents
Learn the fundamentals of how to create AI Agents.
Creating an AI Agent to Write Blog Posts with CrewAIArtificial IntelligenceHow to assemble a crew of AI agents with CrewAI and PythonGustavo SantosApril 4, 202512 min read
Creating an AI Agent to Write Blog Posts with CrewAI
How to assemble a crew of AI agents with CrewAI and Python
LLM + RAG: Creating an AI-Powered File Reader AssistantLarge Language ModelsHow to create a chatbot to answer questions about file’s contentGustavo SantosMarch 3, 202510 min read
LLM + RAG: Creating an AI-Powered File Reader Assistant
How to create a chatbot to answer questions about file’s content
LightGBM: The Fastest Option of Gradient BoostingData ScienceLearn how to implement a fast and effective Gradient Boosting model using PythonGustavo SantosJanuary 12, 20257 min read
LightGBM: The Fastest Option of Gradient Boosting
Learn how to implement a fast and effective Gradient Boosting model using Python
How Bias and Variance Affect Your ModelData ScienceLearn the concepts and the practice. How a model behaves in each case.Gustavo SantosDecember 24, 20247 min read
How Bias and Variance Affect Your Model
Learn the concepts and the practice. How a model behaves in each case.
Synthetic Control Sample for Before and After A/B TestData ScienceLearn a simple way to use linear regression to create a synthetic control sample for…Gustavo SantosDecember 19, 202411 min read
Synthetic Control Sample for Before and After A/B Test
Learn a simple way to use linear regression to create a synthetic control sample for…
Documenting Python Projects with MkDocsUse Markdown to quickly create a beautiful documentation page for your projectsGustavo SantosNovember 22, 20249 min read
Documenting Python Projects with MkDocs
Use Markdown to quickly create a beautiful documentation page for your projects
How I Created a Data Science Project Following CRISP-DM LifecycleData ScienceAn end-to-end project using the CRISP-DM frameworkGustavo SantosNovember 13, 202426 min read
How I Created a Data Science Project Following CRISP-DM Lifecycle
An end-to-end project using the CRISP-DM framework
Make Your Way from Pandas to PySparkData ScienceLearn a few basic commands to start transitioning from Pandas to PySparkGustavo SantosSeptember 26, 20249 min read
Make Your Way from Pandas to PySpark
Learn a few basic commands to start transitioning from Pandas to PySpark
A Closer Look at Scipy’s Stats Module – Part 2Data ScienceLet’s learn the main methods from scipy.stats module in Python.Gustavo SantosSeptember 19, 20246 min read
A Closer Look at Scipy’s Stats Module – Part 2
Let’s learn the main methods from scipy.stats module in Python.

The world’s leading publication for data science, AI, and ML professionals.
LinkedIn
X
Rust for Python Developers: Why You Should Take a Look at the Rust Programming Language
Discover how Rust complements Python with speed, safety, and control — and why it's worth your attention as a modern developer.
The programminglanguage Rust is now appearing in many feeds as it offers a performant and secure way to write programs and places great emphasis on performance. If you come from the Python world ofPandas,Jupyteror Flask, you might think that Rust has a completely different focus than Python and is used more in backend development or for providing APIs. However, in recent years in particular, Rust has developed into an interesting alternative and addition to Python, which is also increasingly being used in the field ofData Science.
In this article, I will show you why it is also worth taking a look at Rust as aPythondeveloper and how the two programming languages differ. We will also build a simple example for opening CSV files in both languages to illustrate the differences in programming.
What is Rust – and why is everyone talking about it?
Rustis a modern system programming language that was created by Mozilla in 2010 to achieve the benefits of C & C++, such as security and performance, while avoiding the typical problems. These programming languages often had to deal with memory leaks, null pointer exceptions or confusing syntax. Since its invention, Rust has developed rapidly and is now enjoying increasing popularity, as it beats established programming languages, such as Python, particularly in terms of performance.
The basis of the Rust programming language is that you have maximum control over the memory management and resources of the computer and yet a pleasant developer experience is created, in which the resources do not have to be allocated manually as in C. Instead, Rust works with a so-called ownership model, which prevents many classes of errors at compile time. Roughly speaking, this works in such a way that the compiler checks a series of rules and if one of the rules is broken, the compilation process is aborted.For many programmers, this procedure is new, as they are used to programming languages in which either a garbage collection takes place, in which memory space that is no longer required is automatically released again, or the memory resources must be explicitly allocated and released again.
This unique feature makes the Rust programming language ideal for applications in which performance and speed are paramount, such as operating systems, web servers or for processing very large amounts of data.
What is Compilation?
If you come from the Python world, you are used to writing code and then simply executing it. This happens because Python is a so-called interpretable language that executes the code line by line and only translates it into machine code during execution, which can then be executed by the computer. Therefore, prototypes can be created easily and quickly, but this leads to a loss of performance, especially with repeated calculations or large amounts of data.Rust, on the other hand, is a compiled language that is translated into native machine code by a so-called compiler before the code is executed. This is a binary code that can be understood directly by the processor. This process is known as compilation.Compilation has the following advantages:
Speed: As the code is not compiled during runtime, it can be executed much faster, similar to C or C++.
Early error detection: During compilation, the code is not only compiled, but also simultaneously checked for many errors, such as type safety or memory errors.
Distribution as a binary: After compilation, the result can be passed on as an independent.exeor.binwithout the need for an interpreter or dependencies.
.exe
.bin
For many developers coming from Python, this step is unusual at first and can also lead to some frustration. However, this is a crucial point for the performance advantages of Rust.
What makes Rust special?
Compiled & Performant: In Rust, the code is translated into machine code before execution, which can be understood directly by the computer and does not have to be translated by the interpreter or a virtual machine first. This procedure is already familiar from C or C++. This gives you a real performance advantage, as the intermediate layer is removed and the code can run directly on the machine, which can lead to real performance advantages, especially for computationally intensive processes.
Memory Safety without a Garbage Collector: Compared to other languages, such as Java or Python, Rust does not use a garbage collector, which runs automatically in the background and frees up memory that is no longer required. Theownership model, on the other hand, enables the memory to be released again in good time and does not generate any performance losses, as it is already executed during the compilation process and then knows exactly which memory space is required at which point in time when the program is executed. This prevents the following errors:TheNullPointer exceptionoccurs in programming languages in which variables can have the empty valuenull. If an attempt is then made to access such a variable, the program usually crashes. In Rust, however, there are no null values, as these must be explicitly marked with the type Option.When processing data in parallel in different threads, errors can occur if both access the same data and also write data. This can lead to uncontrolled behavior and data errors, which are generally referred to asrace conditions. With Rust, on the other hand, only a single modifying access (&mut) and several read accesses (&) are permitted at the same time. This means that write processes can never occur at the same time and errors are prevented.If a variable has already been released or deleted and the program then tries to access it anyway, theuse-after-free errorcan occur, which can lead to security gaps or even a program crash. If a variable in Rust falls out of the scope of validity, the memory is automatically released and the program can no longer access it. If this does happen, the compiler aborts and does not allow the program to be compiled.
TheNullPointer exceptionoccurs in programming languages in which variables can have the empty valuenull. If an attempt is then made to access such a variable, the program usually crashes. In Rust, however, there are no null values, as these must be explicitly marked with the type Option.
null
When processing data in parallel in different threads, errors can occur if both access the same data and also write data. This can lead to uncontrolled behavior and data errors, which are generally referred to asrace conditions. With Rust, on the other hand, only a single modifying access (&mut) and several read accesses (&) are permitted at the same time. This means that write processes can never occur at the same time and errors are prevented.
&mut
&
If a variable has already been released or deleted and the program then tries to access it anyway, theuse-after-free errorcan occur, which can lead to security gaps or even a program crash. If a variable in Rust falls out of the scope of validity, the memory is automatically released and the program can no longer access it. If this does happen, the compiler aborts and does not allow the program to be compiled.
Modern Tooling: Rust is embedded in a sophisticated ecosystem with thecargotool at its center. This manages all processes that are important for the code, such as creating new projects, managing external dependencies or compiling the code. This “all-in-one tooling” allows you to focus on the actual code without having to deal with configuration problems or build scripts.
cargo
High Acceptance & Broad Community: Rust has been voted the “Most Admired” programming language by the Stack Overflow community for several years. The community is also known for its openness, detailed documentation and beginner-friendliness. It is also actively involved in further developments.
Rust vs. Python – What are the Differences?
Rust and Python are two very different programming languages that can be used for similar applications, but have major differences in their basic structure. Python is particularly popular with users for its fast prototyping and simple syntax, while Rust offers high performance and control. Specifically, the two languages differ in the following points:
Result
As the table shows, the two programming languages differ primarily in their typing and the execution speed that depends on it. Due to its syntax, Python offers an easy introduction to the programming language and can be used for prototypes, while Rust is ideal for performance-intensive systems but has a steep learning curve.
Why is Rust exciting for Python Developers?
If you come from the Python world, you are usually used to working at a high level of abstraction, as you can use powerful libraries such as Pandas or NumPy, rarely have to worry about memory management and can also implement complex programs with just a few lines of code. This is exactly where Python’s strengths lie, but it can still happen that you reach its limits at some point, for example if you want to further optimize performance or write real, parallel programs. In these cases, Rust comes into play and can be used as an exciting addition to Python.
More Control
In Rust, you are confronted with concepts such as memory management, ownership and lifetimes, as the variables have to be specifically typed and some errors are already identified during compilation. If you come from a pure Python world, this may seem very stressful and unnecessary at first, but it is the exact opposite. By actually dealing with the variables and their memory usage, you gain a better understanding of your own code and recognize direct potential for optimization. This knowledge is not only useful in Rust, but also develops your own skills in development.
High Performance
For computationally intensive tasks that may need to be highly parallelized, Python quickly reaches its limits due to the Global Interpreter Lock (GIL), as true parallelism is prevented. With Rust, precisely these bottlenecks can be avoided and high-performance modules are possible, for example for data analysis or image processing. With the help of libraries such as PyO3 or FFI, these modules can then be integrated directly into the Python project. In this way, the simplicity of Python can be combined with the performance of Rust.
Rust for the Backend
In addition to modules, entire microservices or command line tools can also be written in Rust. They are characterized above all by their robustness and also require hardly any external dependencies. In addition, powerful APIs can be written using actix-web or axum, which can be used for machine learning models or data pipelines, for example. For example, a CSV parser, a preprocessing module and a high-throughput API gateway could be built in Rust, which is then simply orchestrated in Python.
Combination of Rust & Python
As we have already seen in the previous sections, Rust and Python do not play against each other, but can be wonderfully combined, making it possible to implement very complex applications in a powerful and simple way. It therefore makes sense for many Python developers to include Rust in their toolset and use it in the right places.
Now that we have looked at the special features of Rust, it makes sense to delve deeper into development and compare two specific projects in both programming languages.
Comparison: Example project in Python and Rust
To get a concrete impression of the two languages, in this section we will look at a simple but practical task that frequently occurs in the field of data science. It involves first opening a CSV file and then determining the number of rows in a file.
In Python, we load the built-in Pythoncsvmodule, which contains extensive functions for working with CSV files. Thedata.csvfile can then be opened and is passed on in the variablef.csv.reader(f)then generates a reader that parses each line in the CSV as aPython list, whereupon all lines read are converted into a list. Finally, the length of this list corresponds to the number of lines in the CSV file:
csv
data.csv
f
csv.reader(f)
import csv 

with open('data.csv') as f: 
    reader = csv.reader(f) 
    rows = list(reader) 
    print(len(rows))
import csv 

with open('data.csv') as f: 
    reader = csv.reader(f) 
    rows = list(reader) 
    print(len(rows))
Overall, this code is quick to write and can also be easily read and understood. However, this implementation reads the entire file in memory, which can lead to problems with larger files. In addition, there is no real error handling, for example if the file cannot be found in the intended folder.
In Rust, on the other hand, we first import an error interface in order to be able to deal with various errors. We also usecsv::Reader;as an external package for importing CSV files. The actual work then takes place in themainfunction, which outputs eitherOK(())or an error if successful. The file is opened in the first step. The?at the end of the command means that any error that may occur, such as the file being missing, is passed on. All entries are then read in and counted. The number of data records is then output to the console usingprintln!:
csv::Reader;
main
OK(())
?
println!
use std::error::Error;
use csv::Reader;

fn main() -> Result<(), Box<dyn Error>> {
    let mut rdr = Reader::from_path("data.csv")?;
    let count = rdr.records().count();
    println!("Zeilen: {}", count);
    Ok(())
}
use std::error::Error;
use csv::Reader;

fn main() -> Result<(), Box<dyn Error>> {
    let mut rdr = Reader::from_path("data.csv")?;
    let count = rdr.records().count();
    println!("Zeilen: {}", count);
    Ok(())
}
This program is very robust due to its extensive error handling and does not simply crash when problems occur. It also saves memory, as the CSV file is read in line by line, which has many advantages, especially with large files.
Where can Rust shine in the Field of Data Science?
Rust started out as a programming language for “system” or “hardware-related” development that can be used for performance application development. In recent years, however, it has also emerged as a strong option for data processing. Especially in combination with Python, for example, the following use cases arise:
Data Preprocessing with Polars: Polars is a library that is on the move in both languages and can be used as an alternative to Pandas for large data sets. It was originally developed in Rust, but can be used from Python. With the help of multithreading, lazy evaluation and column-by-column executability, large amounts of data can be processed up to ten times faster than comparable processes in Pandas.
Compute-intensive tasks in PyO3: For demanding applications in Python, such as text parsing, image processing or data validation, parts of the code can simply be outsourced to Rust. These can then be integrated directly as Python modules via PyO3 or maturin. This is particularly useful in data preprocessing pipelines, which ultimately pass the processed data on to TensorFlow.
Web Backends with actix-web: For APIs and microservices, Rust offers actix-web, a high-performance framework that is not only faster than Flask or FastAPI, but also more secure, as there can be no runtime errors and static types are used.
This is what you should take with you
Rust is worthwhile for you if…
You want to do more low-level programming and don’t want to experience the difficulties of C/C++.
You enjoy working with Python, but have performance limits in some areas.
You want to develop backend services or CLI tools.
You want to better understand how programs interact with memory and the rest of the system.
You want to write robust and maintainable tools that just run.
Written By
Topics:
Share this article:
Share on Facebook
Share on LinkedIn
Share on X
Related Articles
Feature Engineering with Microsoft Fabric and Dataflow Gen2Data EngineeringFabric Madness part 3Roger NobleApril 15, 202413 min read
Feature Engineering with Microsoft Fabric and Dataflow Gen2
Fabric Madness part 3
Gauss, Imposters, and Making Room for CreativityData ScienceOur weekly selection of must-read Editors’ Picks and original featuresTDS EditorsMarch 18, 20214 min read
Gauss, Imposters, and Making Room for Creativity
Our weekly selection of must-read Editors’ Picks and original features
Python for Data Scientists: Choose Your Own AdventureData ScienceOur weekly selection of must-read Editors’ Picks and original featuresTDS EditorsAugust 11, 20223 min read
Python for Data Scientists: Choose Your Own Adventure
Our weekly selection of must-read Editors’ Picks and original features
Validate Balanced Parenthesis using SQLData ScienceCheck the well-formed-ness of a string containing open and close parenthesis using just SQLDhruv MataniJanuary 4, 20236 min read
Validate Balanced Parenthesis using SQL
Check the well-formed-ness of a string containing open and close parenthesis using just SQL
Exception Handling in C++CodingIntroduction to Error Handling in C++Sadrach Pierre, Ph.D.July 20, 202210 min read
Exception Handling in C++
Introduction to Error Handling in C++
Foundational RL: Dynamic ProgrammingData ScienceRoad to Reinforcement LearningRahul BhadaniDecember 19, 20225 min read
Foundational RL: Dynamic Programming
Road to Reinforcement Learning
5 Passive Income-Generating Products You Can Develop in Your Spare Time as a Data AnalystArtificial IntelligenceFinding your niche in these products will help you generate extra income on the side.Madison HunterSeptember 15, 20228 min read
5 Passive Income-Generating Products You Can Develop in Your Spare Time as a Data Analyst
Finding your niche in these products will help you generate extra income on the side.

The world’s leading publication for data science, AI, and ML professionals.
LinkedIn
X
Programming
Rust for Python Developers: Why You Should Take a Look at the Rust Programming LanguageProgrammingDiscover how Rust complements Python with speed, safety, and control — and why it’s worth…Niklas LangMay 2, 202513 min read
Rust for Python Developers: Why You Should Take a Look at the Rust Programming Language
Discover how Rust complements Python with speed, safety, and control — and why it’s worth…
NumExpr: The “Faster than Numpy” Library Most Data Scientists Have Never UsedProgrammingA comparative performance test with NumPyThomas ReidApril 28, 20258 min read
NumExpr: The “Faster than Numpy” Library Most Data Scientists Have Never Used
A comparative performance test with NumPy
Data Science: From School to Work, Part IVProgrammingGood practices for testing your Python projectsVincent MargotApril 23, 202517 min read
Data Science: From School to Work, Part IV
Good practices for testing your Python projects
How to Use Gyroscope in Presentations, or Why Take a JoyCon to DPG2025ProgrammingThis article explores how browser-based computational notebooks —particularly the WLJS Notebook —can transform static slides into dynamic,…Kirill VasinApril 21, 202519 min read
How to Use Gyroscope in Presentations, or Why Take a JoyCon to DPG2025
This article explores how browser-based computational notebooks —particularly the WLJS Notebook —can transform static slides into dynamic,…
How to Optimize your Python Program for SlownessProgrammingWrite a short program that finishes after the universe diesCarl KadieApril 7, 202520 min read
How to Optimize your Python Program for Slowness
Write a short program that finishes after the universe dies
How I Would Learn To Code (If I Could Start Over)ProgrammingHow to learn to code in 2025Egor HowellApril 4, 202510 min read
How I Would Learn To Code (If I Could Start Over)
How to learn to code in 2025
PyScript vs. JavaScript: A Battle of Web TitansProgrammingCan Python really replace JavaScript for web development?Pol MarinApril 2, 20255 min read
PyScript vs. JavaScript: A Battle of Web Titans
Can Python really replace JavaScript for web development?
Nine Pico PIO Wats with Rust (Part 2)ProgrammingRaspberry Pi programmable IO pitfalls illustrated with a musical exampleCarl KadieMarch 14, 202517 min read
Nine Pico PIO Wats with Rust (Part 2)
Raspberry Pi programmable IO pitfalls illustrated with a musical example
Comprehensive Guide to Dependency Management in PythonProgrammingMaster the management of virtual environmentsVyacheslav EfimovMarch 7, 20256 min read
Comprehensive Guide to Dependency Management in Python
Master the management of virtual environments
Nine Rules for SIMD Acceleration of Your Rust Code (Part 1)ProgrammingGeneral Lessons from Boosting Data Ingestion in the range-set-blaze Crate by 7xCarl KadieFebruary 26, 202520 min read
Nine Rules for SIMD Acceleration of Your Rust Code (Part 1)
General Lessons from Boosting Data Ingestion in the range-set-blaze Crate by 7x

The world’s leading publication for data science, AI, and ML professionals.
LinkedIn
X
Author: Niklas Lang
Rust for Python Developers: Why You Should Take a Look at the Rust Programming LanguageProgrammingDiscover how Rust complements Python with speed, safety, and control — and why it’s worth…Niklas LangMay 2, 202513 min read
Rust for Python Developers: Why You Should Take a Look at the Rust Programming Language
Discover how Rust complements Python with speed, safety, and control — and why it’s worth…
When Predictors Collide: Mastering VIF in Multicollinear RegressionData ScienceExplore how the Variance Inflation Factor helps detect and manage multicollinearity in your regression models.Niklas LangApril 16, 202511 min read
When Predictors Collide: Mastering VIF in Multicollinear Regression
Explore how the Variance Inflation Factor helps detect and manage multicollinearity in your regression models.
Mastering Hadoop, Part 3: Hadoop Ecosystem: Get the most out of your clusterData EngineeringExploring the Hadoop ecosystem — key tools to maximize your cluster’s potentialNiklas LangMarch 14, 202524 min read
Mastering Hadoop, Part 3: Hadoop Ecosystem: Get the most out of your cluster
Exploring the Hadoop ecosystem — key tools to maximize your cluster’s potential
Mastering Hadoop, Part 2: Getting Hands-On — Setting Up and Scaling HadoopData EngineeringUnderstanding Hadoop’s core components before installation and scalingNiklas LangMarch 13, 202523 min read
Mastering Hadoop, Part 2: Getting Hands-On — Setting Up and Scaling Hadoop
Understanding Hadoop’s core components before installation and scaling
Mastering Hadoop, Part 1: Installation, Configuration, and Modern Big Data StrategiesData EngineeringA comprehensive guide covering Hadoop setup, HDFS commands, MapReduce, debugging, advantages, challenges, and the future…Niklas LangMarch 11, 202510 min read
Mastering Hadoop, Part 1: Installation, Configuration, and Modern Big Data Strategies
A comprehensive guide covering Hadoop setup, HDFS commands, MapReduce, debugging, advantages, challenges, and the future…
Ridge Regression: A Robust Path to Reliable PredictionsData ScienceLearn how regularization reduces overfitting and improves model stability in linear regression.Niklas LangJanuary 30, 202511 min read
Ridge Regression: A Robust Path to Reliable Predictions
Learn how regularization reduces overfitting and improves model stability in linear regression.
Why Data Scientists Can’t Afford Too Many Dimensions and What They Can Do About ItDeep LearningAn in-depth article about dimensionality reduction and its most popular methodsNiklas LangJanuary 16, 202517 min read
Why Data Scientists Can’t Afford Too Many Dimensions and What They Can Do About It
An in-depth article about dimensionality reduction and its most popular methods
Activation Functions in Neural Networks: How to Choose the Right OneDeep LearningIntroduction to activation functions and an overview of the most famous functionsNiklas LangDecember 12, 202418 min read
Activation Functions in Neural Networks: How to Choose the Right One
Introduction to activation functions and an overview of the most famous functions
Why Batch Normalization Matters for Deep LearningDeep LearningDiscover the role of batch normalization in streamlining neural network training and improving model performanceNiklas LangNovember 25, 202413 min read
Why Batch Normalization Matters for Deep Learning
Discover the role of batch normalization in streamlining neural network training and improving model performance
Demystifying the Correlation Matrix in Data ScienceData ScienceUnderstanding the Connections Between Variables: A Comprehensive Guide to Correlation Matrices and Their ApplicationsNiklas LangNovember 13, 202416 min read
Demystifying the Correlation Matrix in Data Science
Understanding the Connections Between Variables: A Comprehensive Guide to Correlation Matrices and Their Applications

The world’s leading publication for data science, AI, and ML professionals.
LinkedIn
X
A Farewell to APMs — The Future of Observability is MCP tools
Like many other fields, the world of observability is about to be turned upside down
The pastyearshave been an absolute rollercoaster (or joyride) of rapidly evolving generative AI technologies. In the twenty-five years I’ve counted myself a software developer, I cannot recall a tectonic shift of a similar magnitude, one that is already fundamentally changing how software is being written.
It would be shortsighted to believe that this revolution stops at simply generating code, however. With AI agents on the loose and the ecosystem opening up to new integrations, the foundations of how we monitor, understand, and optimize software are being upended as well. The tools that served us well in a human-centric world, built around concepts such as manual alerts, datagrids, and dashboards, are becoming irrelevant and obsolete. Application Performance Monitoring (APM) platforms and, in particular, how they leverage logs, metrics, and traces, will need to acknowledge that the human user possessing the time resources required to browse, filter, and set thresholds is no longer available, the team has already delegated much of that work to AI.
Intelligent agents are becoming integral to the SDLC (Software Development Lifecycle), autonomously analyzing, diagnosing, and improving systems in real time. This emerging paradigm requires a new take on an old problem. For observability data to be incorporated to make agents and teams more productive, it must be structured for machines, not for humans. One recent technology that makes this possible is also one that has rightfully received a lot of buzz lately, theModel Context Protocol (mcp).
MCPs in a nutshell
Initially introduced byAnthropic, the Model Context Protocol (MCP) represents a communication tier between AI agents and other applications, allowing agents to access additional data sources and perform actions as they see fit. More importantly, MCPs open up new horizons for the agent to intelligently choose to act beyond its immediate scope and thereby broaden the range of use cases it can address.
The technology is not new, but the ecosystem is. In my mind, it is the equivalent of evolving from custom mobile application development to having an app store. It is not by chance that it is currently experiencing growth of Cambrian proportions, as simply having a rich and standardized ecosystem opens up the market for new opportunities. More broadly speaking, MCPs represent an agent-centric model for creating new products that can transform how applications are built and the way in which they deliver value to end users.
The limitations of a human-centric model
Most software applications are built around humans as their primary users. Generally speaking, a vendor decides to invest in developing certain product features, which it believes will be a good match to the requirements and needs of end users. The users then try to make use of that given set of features to try to fulfill their specific needs.
There are three main limitations to this approach, which are becoming more of an impediment as teams adopt AI agents to streamline their processes:
Fixed interface— Product managers have to anticipate and generalize the use case to create the right interfaces in the application. The UI or API set is fixed and cannot adapt itself to each unique need. Consequently, users may find that some features are completely useless to their specific requirements. Other times, even with a combination of features, the user can’t get everything they need.
Cognitive load— The process of interacting with the application data to get to the information the user needs requires manual effort, resources, and sometimes expertise. Taking APMs as an example, understanding the root cause of a performance issue and fixing it might take some investigation, as each issue is different. Lack of automation and reliance on voluntary manual processes often means that the data is not utilized at all.
Limited scope— Each product often only holds a part of the picture needed to solve the specific requirement. For example, the APM might have the tracing data, but no access to the code, the GitHub history, Jira trends, infrastructure data, or customer tickets. It is left to the user to triage using multiple sources to get to the root of each problem.
Agent-centric MCPs — The inverted application
With the advent of MCPs, software developers now have the choice of adopting a different model for developing software. Instead of focusing on a specific use case, trying to nail the right UI elements for hard-coded usage patterns, applications can transform into a resource for AI-driven processes. This describes a shift from supporting a handful of predefined interactions to supporting numerousemergentuse cases. Rather than investing in a specific feature, an application can now choose to lend its domain expertise to the AI agent viadataandactionsthat can be used opportunistically whenever they are relevant, even if indirectly so.
As this model scales, the agent can seamlessly consolidate data and actions from different applications and domains, such as GitHub, Jira, observability platforms, analytics tools, and the codebase itself. The agent can then automate the analysis process itself as a part ofsynthesizingthe data, removing the manual steps and the need for specialized expertise.
Let’s take a look at a practical example that can illustrate how an agent-centric model opens up new neural pathways in the engineering process.
Every developer knowscode reviewsrequire a lot of effort; to make matters worse, the reviewer is often context-switched away from their other tasks, further draining the team’s productivity. On the surface, this would seem like an opportunity for observability applications to shine. After all, the code under review has already accumulated meaningful data running in testing and pre-production environments. Theoretically, this information can help decipher more about the changes, what they are impacting, and how they have possibly altered the system behavior. Unforunately, the high cost of making sense of all of that data across multiple applications and data stream, makes it next to useless.
In an agent-centric flow, however, whenever an engineer asks an AI agent to assist in reviewing the new code, that entire process becomes completely autonomous. In the background, the agent will orchestrate the investigative steps across multiple applications and MCPs, including observability tools, to bring back actionable insights about the code changes. The agent can access relevant runtime data (e.g., traces and logs from staging runs), analytics on feature usage, GitHub commit metadata, and even Jira ticket history. It then correlates the diff with the relevant runtime spans, flags latency regressions or failed interactions, and points out recent incidents that might relate to the modified code.
In this scenario, the developer doesn’t need to sift through different tools or tabs or spend time trying to connect the dots— the agent brings it all together behind the scenes, identifying issues as well as possible fixes. As response itself is dynamically generated: it may begin with a concise textual summary, expand into a table showing metrics over time, include a link to the affected file in GitHub with highlighted changes, and even embed a chart visualizing the timeline of errors before and after the release.
While the above workflow was organically produced by an agent, some AI clients will allow the user to cement a desired workflows by adding rules to the agent’s memory. For example, this is is a memory file I am currenting using with Cursor to ensure that all code review prompts will consistently trigger checks to the test environment and check for usage based on production.
Death by a thousand use cases
The code review scenario is just one ofmanyemergentuse cases that demonstrate how AI can quietly make use of relevant MCP data to assist the user accomplish their goals. More importantly, the user does not need to be aware of the applications that were being used autonomously by the agent. From the user’s perspective, they just need to describe their need.
Emergent use cases can enhance user productivity across the board with data that cannot be made accessible otherwise. Here are a few other examples where observability data can make a huge difference, without anyone having to visit a single APM web page:
Test generationbased on real usage
Selecting the right areas torefactorbased on code issues affecting performance the most
Preventingbreaking changeswhen code is still checked out
Detectingunused code
Making observability useful to the agent, however, is a little more involved than slapping on an MCP adapter to an APM. Indeed, many of the current generation tools, in rushing to support the new technology took that very route, not taking into consideration that AI agents also have their limitations.
While smart and powerful, agents cannot instantly replace any application interacting with any data, on demand. In their current iteration, at least, they are bound by the size of the dataset and stop short of applying more complex ML algorithms or even higher-order math. If the observability tool is to become an effective data provider to the agent, it must prepare the data in advance in lieu of these limitations. More broadly speaking, this defines the role of products in the age of AI — providing islands of nontrivial domain expertise to be utilized in an AI-driven process.
There are many posts on the topic on the best way to prepare data for use by generative AI agents, and I have included some links at the end of this post. However, we can describe some of the requirements of a good MCP output in broad strokes:
Structured(schema-consistent, typed entities)
Preprocessed(aggregated, deduplicated, tagged)
Contextualized(grouped by session, lifecycle, or intent)
Linked(references across code spans, logs, commits, and tickets)
Instead of surfacing raw telemetry anMCP must feed a coherent data narrativeto the agent, post-analysis. The agent is not just a dashboard view to be rendered. At the same time, it must also  make therelevantraw data available on demand to allow further investigation, to support the agent’s autonomous reasoning actions.
Given simple access to raw data it would be next to impossible for an agent to identify an issue manifesting in the trace internals of only 5% of the millions of available traces, let alone prioritize that problem based on its system impact, or make the determination of whether that pattern is anomalous.
To bridge that gap, many products will likely evolve into ‘AI preposessors’, bringing forth dedicated ML processes and high level statistical analysis as well as domain expertise.
Farewell to APMs
Ultimately, APMs are not legacy tools — they are representative of alegacy mindsetthat is slowly but surely being replaced. It might take more time for the industry to realign, but it will ultimately impact many of the products we currently use, especially in the software industry, which is racing to adopt generative AI.
As AI becomes more dominant in developing software, it will also no longer be limited to human-initiated interactions. Generative AI reasoning will be used as a part of the CI process, and in some cases, even run indefinitely as background processes continuously checking data and performing actions. With that in mind, more and more tools will come up with their agent-centric model complement and sometimes replace their direct-to-human approach, or risk being left out of their clients new AI SLDC stack.
Links and resources
Airbyte: Normalization is key — schema consistency and relational linking improve cross-source reasoning.
Harrison Clarke: Preprocessing must hit the sweet spot — rich enough for inference, structured enough for precision.
DigitalOcean: Aggregation by semantic boundaries (user sessions, flows) unlocks better chunking and story-based reasoning.
Want to Connect?You can reach me on Twitter at@dopplewareor viaLinkedIn.Follow myMCPfor dynamic code analysis using observability athttps://github.com/digma-ai/digma-mcp-server
Written By
Topics:
Share this article:
Share on Facebook
Share on LinkedIn
Share on X
Related Articles
What Do Large Language Models “Understand”?Artificial IntelligenceA deep dive on the meaning of understanding and how it applies to LLMsTarik DzekmanAugust 21, 202431 min read
What Do Large Language Models “Understand”?
A deep dive on the meaning of understanding and how it applies to LLMs
Optimizing Marketing Campaigns with Budgeted Multi-Armed BanditsData ScienceWith demos, our new solution, and a videoVadim ArzamasovAugust 16, 202410 min read
Optimizing Marketing Campaigns with Budgeted Multi-Armed Bandits
With demos, our new solution, and a video
Deep Dive into LSTMs & xLSTMs by Hand ✍️Deep LearningExplore the wisdom of LSTM leading into xLSTMs - a probable competition to the present-day LLMsSrijanie Dey, PhDJuly 9, 202413 min read
Deep Dive into LSTMs & xLSTMs by Hand ✍️
Explore the wisdom of LSTM leading into xLSTMs - a probable competition to the present-day LLMs
Latest picks: Data Quality for Everyday AnalysisData ScienceYour daily dose of data scienceTDS EditorsNovember 17, 20201 min read
Latest picks: Data Quality for Everyday Analysis
Your daily dose of data science
Latest picks: No Free Lunch with Feature BiasData ScienceYour daily dose of data scienceTDS EditorsJanuary 25, 20211 min read
Latest picks: No Free Lunch with Feature Bias
Your daily dose of data science
Latest picks: Why & How to use the Naive Bayes algorithms in a regulated industry with sklearnData ScienceYour daily dose of data scienceTDS EditorsDecember 2, 20201 min read
Latest picks: Why & How to use the Naive Bayes algorithms in a regulated industry with sklearn
Your daily dose of data science
Latest picks: Deep Few-shot Anomaly DetectionArtificial IntelligenceYour daily dose of data scienceTDS EditorsNovember 10, 20201 min read
Latest picks: Deep Few-shot Anomaly Detection
Your daily dose of data science

The world’s leading publication for data science, AI, and ML professionals.
LinkedIn
X
Author: Roni Dover
A Farewell to APMs — The Future of Observability is MCP toolsArtificial IntelligenceLike many other fields, the world of observability is about to be turned upside downRoni DoverMay 1, 202510 min read
A Farewell to APMs — The Future of Observability is MCP tools
Like many other fields, the world of observability is about to be turned upside down

The world’s leading publication for data science, AI, and ML professionals.
LinkedIn
X
Step-by-Step Guide to Build and Deploy an LLM-Powered Chat with Memory in Streamlit
And monitor your API usage on Google Cloud Console
In this post, I’ll show you step by step how to build and deploy a chat powered with LLM —Gemini—in Streamlit and monitor the API usage on Google Cloud Console. Streamlit is a Python framework that makes it super easy to turn your Python scripts into interactive web apps, with almost no front-end work.
Recently, I built a project,bordAI— a chat assistant powered by LLM integrated with tools I developed to support embroidery projects. After that, I decided to start this series of posts to share tips I’ve learned along the way.
Here’s a quick summary of the post:
1 to 6 — Project Setup
7 to 13 — Building the Chat
14 to 15— Deploy and Monitor the app
1. Create a New GitHub repository
Go toGitHuband create a new repository.
2. Clone the repository locally
→ Execute this command in your terminal to clone it:
git clone <your-repository-url>
git clone <your-repository-url>
3. Set Up a Virtual Environment (optional)
A Virtual Environment is like a separate space on your computer where you can install a specific version of Python and libraries without affecting the rest of your system. This is useful because different projects might need different versions of the same libraries.
→ To create a virtual environment:
pyenv virtualenv 3.9.14 chat-streamlit-tutorial
pyenv virtualenv 3.9.14 chat-streamlit-tutorial
→ To activate it:
pyenv activate chat-streamlit-tutorial
pyenv activate chat-streamlit-tutorial
4. Project Structure
A project structure is just a way to organize all the files and folders for your project. Ours will look like this:
chat-streamlit-tutorial/
│
├── .env
├── .gitignore
├── app.py
├── functions.py
├── requirements.txt
└── README.md
chat-streamlit-tutorial/
│
├── .env
├── .gitignore
├── app.py
├── functions.py
├── requirements.txt
└── README.md
.env→ file where you store your API key (not pushed to GitHub)
.env
.gitignore→ file where you list the files or folders for git to ignore
.gitignore
app.py→ main streamlit app
app.py
functions.py→ custom functions to better organize the code
functions.py
requirements.txt→ list of libraries your project needs
requirements.txt
README.md→ file that explains what your project is about
README.md
→ Execute this inside your project folder to create these files:
touch .env .gitignore app.py functions.py requirements.txt
touch .env .gitignore app.py functions.py requirements.txt
→ Inside the file.gitignore, add:
.gitignore
.env
__pycache__/
.env
__pycache__/
→ Add this to therequirements.txt:
requirements.txt
streamlit
google-generativeai
python-dotenv
streamlit
google-generativeai
python-dotenv
→ Install dependencies:
pip install -r requirements.txt
pip install -r requirements.txt
5. Get API Key
An API Key is like a password that tells a service you have permission to use it. In this project, we’ll use the Gemini API because they have a free tier, so you can play around with it without spending money.
Go tohttps://aistudio.google.com/
Create or log in to your account.
Click on “Create API Key“, create it, and copy it.
Don’t set up billing if you just want to use the free tier. It should say “Free” under “Plan”, just like here:
We’ll usegemini-2.0-flashin this project. It offers a free tier, as you can see in the table below:
15 RPM = 15 Requests per minute
1,000,000 TPM = 1 Million Tokens Per Minute
1,500 RPD = 1,500 Requests Per Day
Note: These limits are accurate as of April 2025 and may change over time.
Just a heads up: if you are using the free tier, Google may use your prompts to improve their products, including human reviews, so it’s not recommended to send sensitive information. If you want to read more about this, check thislink.
6. Store your API Key
We’ll store our API Key inside a.envfile. A.envfile is a simple text file where you store secret information, so you don’t write it directly in your code. We don’t want it going to GitHub, so we have to add it to our.gitignorefile. This file determines which files git should literally ignore when you push your changes to the repository. I’ve already mentioned this in part 4, “Project Structure”, but just in case you missed it, I’m repeating it here.
.env
.env
.gitignore
This step is really important, don’t forget it!→ Add this to.gitignore:
.gitignore
.env
__pycache__/
.env
__pycache__/
→ Add the API Key to.env:
.env
API_KEY= "your-api-key"
API_KEY= "your-api-key"
If you’re running locally,.envworks fine. However, if you’re deploying in Streamlit later, you will have to usest.secrets. Here I’ve included a code that can work in both scenarios.
.env
st.secrets
→Add this function to yourfunctions.py:
functions.py
import streamlit as st
import os
from dotenv import load_dotenv

def get_secret(key):
    """
    Get a secret from Streamlit or fallback to .env for local development.

    This allows the app to run both on Streamlit Cloud and locally.
    """
    try:
        return st.secrets[key]
    except Exception:
        load_dotenv()
        return os.getenv(key)
import streamlit as st
import os
from dotenv import load_dotenv

def get_secret(key):
    """
    Get a secret from Streamlit or fallback to .env for local development.

    This allows the app to run both on Streamlit Cloud and locally.
    """
    try:
        return st.secrets[key]
    except Exception:
        load_dotenv()
        return os.getenv(key)
→ Add this to yourapp.py:
app.py
import streamlit as st
import google.generativeai as genai
from functions import get_secret

api_key = get_secret("API_KEY")
import streamlit as st
import google.generativeai as genai
from functions import get_secret

api_key = get_secret("API_KEY")
7. Choose the model
I chosegemini-2.0-flashfor this project because I think it’s a great model with a generous free tier. However, you can explore other model options that also offer free tiers and choose your preferred one.
Pro: models designed forhigh–qualityoutputs, including reasoning and creativity. Generally used for complex tasks, problem-solving, and content generation. They are multimodal — this means they can process text, image, video, and audio for input and output.
Flash: models projected forspeedandcost efficiency.Can have lower-quality answers compared to the Pro for complex tasks. Generally used for chatbots, assistants, and real-time applications like automatic phrase completion. They are multimodal for input, and for output is currently just text, other features are in development.
Lite: even faster and cheaper than Flash, but with some reduced capabilities, such as it is multimodal only for input and text-only output. Its main characteristic is that it ismore economicalthan the Flash, ideal for generating large amounts of text within cost restrictions.
Thislinkhas plenty of details about the models and their differences.
Here we are setting up the model. Just replace “gemini-2.0-flash” with the model you’ve chosen.
→ Add this to yourapp.py:
app.py
genai.configure(api_key=api_key)
model = genai.GenerativeModel("gemini-2.0-flash")
genai.configure(api_key=api_key)
model = genai.GenerativeModel("gemini-2.0-flash")
8. Build the chat
First, let’s discuss the key concepts we’ll use:
st.session_state: this works like amemoryfor your app. Streamlit reruns your script from top to bottom every time something changes — when you send a message or click a button —  so normally, all the variables would be reset. This allows Streamlit to remember values between reruns. However, if you refresh your web page you’ll lose thesession_state.
st.session_state
session_state
st.chat_message(name, avatar): Creates a chat bubble for a message in the interface. The first parameter is thenameof the message author, whichcan be“user”, “human”, “assistant”, “ai”, or str.If you use user/human and assistant/ai, it already has defaultavatarsof user and bot icons. You can change this if you want to. Check out thedocumentationfor more details.
st.chat_message(name, avatar)
st.chat_input(placeholder): Displays an input box at the bottom for the user to type messages. It has many parameters, so I recommend you check out thedocumentation.
st.chat_input(placeholder)
First, I’ll explain each part of the code separately, and after I’ll show you the whole code together.
This initial step initializes yoursession_state, the app’s “memory”, to keep all the messages within one session.
session_state
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []
Next, we’ll set the first default message. This is optional, but I like to add it. You could add some initial instructions if suitable for your context. Every time Streamlit runs the page andst.session_state.chat_historyis empty, it’ll append this message to the history with the role “assistant”.
st.session_state.chat_history
if not st.session_state.chat_history:
    st.session_state.chat_history.append(("assistant", "Hi! How can I help you?"))
if not st.session_state.chat_history:
    st.session_state.chat_history.append(("assistant", "Hi! How can I help you?"))
In my app bordAI, I added this initial message giving context and instructions for my app:
For theuserpart, the first line creates the input box. Ifuser_messagecontains content, it writes it to the interface and then appends it tochat_history.
user_message
chat_history
user_message = st.chat_input("Type your message...")

if user_message:
    st.chat_message("user").write(user_message)
    st.session_state.chat_history.append(("user", user_message))
user_message = st.chat_input("Type your message...")

if user_message:
    st.chat_message("user").write(user_message)
    st.session_state.chat_history.append(("user", user_message))
Now let’s add theassistantpart:
system_promptis the prompt sent to the model. You could just send theuser_messagein place offull_input(look at the code below). However, the output might not be precise. A prompt provides context and instructions abouthowyou want the model to behave, not justwhatyou want it to answer.A good prompt makes the model’s response more accurate, consistent, and aligned with your goals.In addition, without telling how our model should behave, it’s vulnerable toprompt injections.
system_prompt
user_message
full_input
Prompt injectionis when someone tries to manipulate the model’s prompt in order to alter its behavior. One way to mitigate this is to structure prompts clearly and delimit the user’s message within triple quotes.
We’ll start with a simple and unclearsystem_promptand in the next session we’ll make it better to compare the difference.
system_prompt
full_input: here, we’re organizing the input, delimiting the user message with triple quotes (“””). This doesn’t prevent all prompt injections, but it is one way to create better and more reliable interactions.
full_input
response: sends a request to the API, storing the output in response.
response
assistant_reply: extracts the text from the response.
assistant_reply
Finally, we usest.chat_message()combined towrite()to display the assistant reply and append it to thest.session_state.chat_history, just like we did with the user.
st.chat_message()
write()
st.session_state.chat_history
if user_message:
    st.chat_message("user").write(user_message)
    st.session_state.chat_history.append(("user", user_message))
    
    system_prompt = f"""
    You are an assistant.
    Be nice and kind in all your responses.
    """
    full_input = f"{system_prompt}\n\nUser message:\n\"\"\"{user_message}\"\"\""

    response = model.generate_content(full_input)
    assistant_reply = response.text

    st.chat_message("assistant").write(assistant_reply)
    st.session_state.chat_history.append(("assistant", assistant_reply))
if user_message:
    st.chat_message("user").write(user_message)
    st.session_state.chat_history.append(("user", user_message))
    
    system_prompt = f"""
    You are an assistant.
    Be nice and kind in all your responses.
    """
    full_input = f"{system_prompt}\n\nUser message:\n\"\"\"{user_message}\"\"\""

    response = model.generate_content(full_input)
    assistant_reply = response.text

    st.chat_message("assistant").write(assistant_reply)
    st.session_state.chat_history.append(("assistant", assistant_reply))
Now let’s see everything together!
→ Add this to yourapp.py:
app.py
import streamlit as st
import google.generativeai as genai
from functions import get_secret

api_key = get_secret("API_KEY")
genai.configure(api_key=api_key)
model = genai.GenerativeModel("gemini-2.0-flash")

if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

if not st.session_state.chat_history:
    st.session_state.chat_history.append(("assistant", "Hi! How can I help you?"))

user_message = st.chat_input("Type your message...")

if user_message:
    st.chat_message("user").write(user_message)
    st.session_state.chat_history.append(("user", user_message))

    system_prompt = f"""
    You are an assistant.
    Be nice and kind in all your responses.
    """
    full_input = f"{system_prompt}\n\nUser message:\n\"\"\"{user_message}\"\"\""

    response = model.generate_content(full_input)
    assistant_reply = response.text

    st.chat_message("assistant").write(assistant_reply)
    st.session_state.chat_history.append(("assistant", assistant_reply))
import streamlit as st
import google.generativeai as genai
from functions import get_secret

api_key = get_secret("API_KEY")
genai.configure(api_key=api_key)
model = genai.GenerativeModel("gemini-2.0-flash")

if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

if not st.session_state.chat_history:
    st.session_state.chat_history.append(("assistant", "Hi! How can I help you?"))

user_message = st.chat_input("Type your message...")

if user_message:
    st.chat_message("user").write(user_message)
    st.session_state.chat_history.append(("user", user_message))

    system_prompt = f"""
    You are an assistant.
    Be nice and kind in all your responses.
    """
    full_input = f"{system_prompt}\n\nUser message:\n\"\"\"{user_message}\"\"\""

    response = model.generate_content(full_input)
    assistant_reply = response.text

    st.chat_message("assistant").write(assistant_reply)
    st.session_state.chat_history.append(("assistant", assistant_reply))
To run and test your app locally, first navigate to the project folder, then execute the following command.
→ Execute in your terminal:
cd chat-streamlit-tutorial
streamlit run app.py
cd chat-streamlit-tutorial
streamlit run app.py
Yay!You now have a chat running in Streamlit!
9. Prompt Engineering
Prompt Engineering is a process of writing instructions to get the best possible output from an AI model.
There are plenty of techniques for prompt engineering. Here are 5 tips:
Write clear and specific instructions.
Define a role, expected behavior, and rules for the assistant.
Give the right amount of context.
Use the delimiters to indicate user input (as I explained in part 8).
Ask for the output in a specified format.
These tips can be applied to thesystem_promptor when you’re writing a prompt to interact with the chat assistant.
system_prompt
Our current system prompt is:
system_prompt = f"""
You are an assistant.
Be nice and kind in all your responses.
"""
system_prompt = f"""
You are an assistant.
Be nice and kind in all your responses.
"""
It is super vague and provides no guidance to the model.
No clear direction for the assistant, what kind of help it should provide
No specification of the role or what is the topic of the assistance
No guidelines for structuring the output
No context on whether it should be technical or casual
Lack of boundaries
We can improve our prompt based on the tips above. Here’s an example.
→ Change thesystem_promptin theapp.py:
system_prompt
app.py
system_prompt = f"""
You are a friendly and a programming tutor.
Always explain concepts in a simple and clear way, using examples when possible.
If the user asks something unrelated to programming, politely bring the conversation back to programming topics.
"""
full_input = f"{system_prompt}\n\nUser message:\n\"\"\"{user_message}\"\"\""
system_prompt = f"""
You are a friendly and a programming tutor.
Always explain concepts in a simple and clear way, using examples when possible.
If the user asks something unrelated to programming, politely bring the conversation back to programming topics.
"""
full_input = f"{system_prompt}\n\nUser message:\n\"\"\"{user_message}\"\"\""
If we ask “What is python?” to the old prompt, it just gives a generic short answer:
With the new prompt, it provides a more detailed response with examples:
Try changing thesystem_promptyourself to see the difference in the model outputs and craft the ideal prompt for your context!
system_prompt
10. Choose Generate Content Parameters
There are many parameters you can configure when generating content. Here I’ll demonstrate howtemperatureandmaxOutputTokenswork. Check thedocumentationfor more details.
temperature
maxOutputTokens
temperature: controls the randomness of the output, ranging from 0 to 2. The default is 1. Lower values produce more deterministic outputs, while higher values produce more creative ones.
temperature
maxOutputTokens: the maximum number of tokens that can be generated in the output. A token is approximately four characters.
maxOutputTokens
To change the temperature dynamically and test it, you can create a sidebar slider to control this parameter.
→ Add this toapp.py:
app.py
temperature = st.sidebar.slider(
    label="Select the temperature",
    min_value=0.0,
    max_value=2.0,
    value=1.0
)
temperature = st.sidebar.slider(
    label="Select the temperature",
    min_value=0.0,
    max_value=2.0,
    value=1.0
)
→ Change theresponsevariable to:
response
response = model.generate_content(
    full_input,
    generation_config={
        "temperature": temperature,
        "max_output_tokens": 1000
    }
)
response = model.generate_content(
    full_input,
    generation_config={
        "temperature": temperature,
        "max_output_tokens": 1000
    }
)
The sidebar will look like this:
Try adjusting the temperature to see how the output changes!
11. Display chat history
This step ensures that you keep track of all the exchanged messages in the chat, so you can see the chat history. Without this, you’d only see the latest messages from the assistant and user each time you send something.
This code accesses everything appended tochat_historyand displays it in the interface.
chat_history
→ Add this before theif user_messageinapp.py:
if user_message
app.py
for role, message in st.session_state.chat_history:
    st.chat_message(role).write(message)
for role, message in st.session_state.chat_history:
    st.chat_message(role).write(message)
Now, all the messages within one session are kept visible in the interface:
Obs: I tried to ask a non-programming question, and the assistant tried to change the subject back to programming. Our prompt is working!
12. Chat with memory
Besides having messages stored inchat_history, our model isn’t aware of the context of our conversation. It is stateless, each transaction is independent.
chat_history
To solve this, we have to pass all this context inside our prompt so the model can reference previous messages exchanged.
Createcontextwhich is a list containing all the messages exchanged until that moment. Adding lastly the most recent user message, so it doesn’t get lost in the context.
context
system_prompt = f"""
You are a friendly and knowledgeable programming tutor.
Always explain concepts in a simple and clear way, using examples when possible.
If the user asks something unrelated to programming, politely bring the conversation back to programming topics.
"""
full_input = f"{system_prompt}\n\nUser message:\n\"\"\"{user_message}\"\"\""

context = [
    *[
        {"role": role, "parts": [{"text": msg}]} for role, msg in st.session_state.chat_history
    ],
    {"role": "user", "parts": [{"text": full_input}]}
]

response = model.generate_content(
    context,
    generation_config={
        "temperature": temperature,
        "max_output_tokens": 1000
    }
)
system_prompt = f"""
You are a friendly and knowledgeable programming tutor.
Always explain concepts in a simple and clear way, using examples when possible.
If the user asks something unrelated to programming, politely bring the conversation back to programming topics.
"""
full_input = f"{system_prompt}\n\nUser message:\n\"\"\"{user_message}\"\"\""

context = [
    *[
        {"role": role, "parts": [{"text": msg}]} for role, msg in st.session_state.chat_history
    ],
    {"role": "user", "parts": [{"text": full_input}]}
]

response = model.generate_content(
    context,
    generation_config={
        "temperature": temperature,
        "max_output_tokens": 1000
    }
)
Now, I told the assistant that I was working on a project to analyze weather data. Then I asked what the theme of my project was and it correctly answered “weather data analysis”, as it now has the context of the previous messages.
If your context gets too long,you can consider summarizing itto save costs, since the more tokens you send to the API, the more you’ll pay.
13. Create a Reset Button (optional)
I like adding a reset button in case something goes wrong or the user just wants to clear the conversation.
You just need to create a function to set dechat_historyas an empty list. If you created other session states, you should set them here as False or empty, too.
chat_history
→ Add this tofunctions.py:
functions.py
def reset_chat():
    """
    Reset the Streamlit chat session state.
    """
    st.session_state.chat_history = []
    st.session_state.example = False # Add others if needed
def reset_chat():
    """
    Reset the Streamlit chat session state.
    """
    st.session_state.chat_history = []
    st.session_state.example = False # Add others if needed
→ And if you want it in the sidebar, add this toapp.py:
app.py
from functions import get_secret, reset_chat

if st.sidebar.button("Reset chat"):
    reset_chat()
from functions import get_secret, reset_chat

if st.sidebar.button("Reset chat"):
    reset_chat()
It will look like this:
Everything together:
import streamlit as st
import google.generativeai as genai
from functions import get_secret, reset_chat

api_key = get_secret("API_KEY")
genai.configure(api_key=api_key)
model = genai.GenerativeModel("gemini-2.0-flash")

temperature = st.sidebar.slider(
    label="Select the temperature",
    min_value=0.0,
    max_value=2.0,
    value=1.0
)

if st.sidebar.button("Reset chat"):
    reset_chat()

if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

if not st.session_state.chat_history:
    st.session_state.chat_history.append(("assistant", "Hi! How can I help you?"))

for role, message in st.session_state.chat_history:
    st.chat_message(role).write(message)

user_message = st.chat_input("Type your message...")

if user_message:
    st.chat_message("user").write(user_message)
    st.session_state.chat_history.append(("user", user_message))

    system_prompt = f"""
    You are a friendly and a programming tutor.
    Always explain concepts in a simple and clear way, using examples when possible.
    If the user asks something unrelated to programming, politely bring the conversation back to programming topics.
    """
    full_input = f"{system_prompt}\n\nUser message:\n\"\"\"{user_message}\"\"\""

    context = [
        *[
            {"role": role, "parts": [{"text": msg}]} for role, msg in st.session_state.chat_history
        ],
        {"role": "user", "parts": [{"text": full_input}]}
    ]

    response = model.generate_content(
        context,
        generation_config={
            "temperature": temperature,
            "max_output_tokens": 1000
        }
    )
    assistant_reply = response.text

    st.chat_message("assistant").write(assistant_reply)
    st.session_state.chat_history.append(("assistant", assistant_reply))
import streamlit as st
import google.generativeai as genai
from functions import get_secret, reset_chat

api_key = get_secret("API_KEY")
genai.configure(api_key=api_key)
model = genai.GenerativeModel("gemini-2.0-flash")

temperature = st.sidebar.slider(
    label="Select the temperature",
    min_value=0.0,
    max_value=2.0,
    value=1.0
)

if st.sidebar.button("Reset chat"):
    reset_chat()

if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

if not st.session_state.chat_history:
    st.session_state.chat_history.append(("assistant", "Hi! How can I help you?"))

for role, message in st.session_state.chat_history:
    st.chat_message(role).write(message)

user_message = st.chat_input("Type your message...")

if user_message:
    st.chat_message("user").write(user_message)
    st.session_state.chat_history.append(("user", user_message))

    system_prompt = f"""
    You are a friendly and a programming tutor.
    Always explain concepts in a simple and clear way, using examples when possible.
    If the user asks something unrelated to programming, politely bring the conversation back to programming topics.
    """
    full_input = f"{system_prompt}\n\nUser message:\n\"\"\"{user_message}\"\"\""

    context = [
        *[
            {"role": role, "parts": [{"text": msg}]} for role, msg in st.session_state.chat_history
        ],
        {"role": "user", "parts": [{"text": full_input}]}
    ]

    response = model.generate_content(
        context,
        generation_config={
            "temperature": temperature,
            "max_output_tokens": 1000
        }
    )
    assistant_reply = response.text

    st.chat_message("assistant").write(assistant_reply)
    st.session_state.chat_history.append(("assistant", assistant_reply))
14. Deploy
If your repository is public, you can deploy with Streamlit for free.
MAKE SURE YOU DO NOT HAVE API KEYS ON YOUR PUBLIC REPOSITORY.
First, save and push your code to the repository.
→ Execute in your terminal:
git add .
git commit -m "tutorial chat streamlit"
git push origin main
git add .
git commit -m "tutorial chat streamlit"
git push origin main
Pushing directly into themainisn’t a best practice, but since it’s just a simple tutorial, we’ll do it for convenience.
main
Go to your streamlit app that is running locally.
Click on “Deploy” at the top right.
In Streamlit Community Cloud, click “Deploy now”.
Fill out the information.
5. Click on “Advanced settings” and writeAPI_KEY="your-api-key", just like you did with the.envfile.
API_KEY="your-api-key"
.env
6. Click “Deploy”.
All done! If you’d like, check out my apphere! 🎉
15. Monitor API usage on Google Console
The last part of this post shows you how to monitor API usage on the Google Cloud Console. This is important if you deploy your app publicly, so you don’t have any surprises.
AccessGoogle Cloud Console.
Go to “APIs and services”.
Click on “Generative Language API”.
Requests:how many times your API was called. In our case, the API is called each time we runmodel.generate_content(context).
model.generate_content(context)
Error (%):the percentage of requests that failed. Errors can have the code4xxwhich is usually the user’s/requester’s fault — for instance,400forbad input,and429means you’rehitting the API too frequently. In addition, errors with the code5xxare usually the system’s/server’s fault and are less common. Google typically retries internally or recommends retrying after a few seconds — e.g.500forInternal Server Errorand503forService Unavailable.
Latency, median (ms): This shows how long (in milliseconds) it takes for your service to respond, at the 50th percentile — meaning half the requests are faster and half are slower. It’s a good general measure of your service’s speed, answering the question, “How fast is it normally?”.
Latency, 95% (ms): This shows the response time at the 95th percentile — meaning 95% of requests are faster than this time, and only 5% slower. It helps to identify how your system behaves under heavy load or with slower cases, answering the question, “How bad is it getting for some users?”.
A quick example of the difference between Latency median and Latency p95:Imagine your service usually responds in200ms:
Median latency = 200ms (good!)
p95 latency = 220ms (also good)
Now under heavy load:
Median latency = 220ms (still looks OK)
p95 latency = 1200ms (notgood)
The metric p95 shows that5% of your users are waiting more than 1.2 seconds— a much worse experience. If we had looked just at the median, we’d assume everything was fine, but p95 shows hidden problems.
Continuing in the “Metrics” page, you’ll find graphs and, at the bottom, the methods called by the API. Also, in “Quotas & System Limits”, you can monitor the API usage compared to the free tier limit.
Click “Show usage chart” to compare usage day by day.
I hope you enjoyed this tutorial.
You can find all the code for this project on myGitHub.
I’d love to hear your thoughts! Let me know in the comments what you think.
Follow me on:
Linkedin
GitHub
Youtube
Written By
Topics:
Share this article:
Share on Facebook
Share on LinkedIn
Share on X
Related Articles
What Do Large Language Models “Understand”?Artificial IntelligenceA deep dive on the meaning of understanding and how it applies to LLMsTarik DzekmanAugust 21, 202431 min read
What Do Large Language Models “Understand”?
A deep dive on the meaning of understanding and how it applies to LLMs
Making Sense of the Promise (and Risks) of Large Language ModelsArtificial IntelligenceOur weekly selection of must-read Editors’ Picks and original featuresTDS EditorsApril 27, 20234 min read
Making Sense of the Promise (and Risks) of Large Language Models
Our weekly selection of must-read Editors’ Picks and original features
Beware of Unreliable Data in Model Evaluation: A LLM Prompt Selection case study with Flan-T5Artificial IntelligenceYou may choose suboptimal prompts for your LLM (or make other suboptimal choices via model…Chris MauckJune 16, 202312 min read
Beware of Unreliable Data in Model Evaluation: A LLM Prompt Selection case study with Flan-T5
You may choose suboptimal prompts for your LLM (or make other suboptimal choices via model…
Beating ChatGPT 4 in Chess with a Hybrid AI modelArtificial IntelligenceBetter but not the best, GPT4 cheated but lost the match !Octavio SantiagoJanuary 22, 20247 min read
Beating ChatGPT 4 in Chess with a Hybrid AI model
Better but not the best, GPT4 cheated but lost the match !
Semantic Search Engine for Emojis in 50+ Languages Using AI 😊🌍🚀Artificial IntelligenceIf you are on social media like Twitter or LinkedIn, you have probably noticed that…Badr Alabsi, PhDJuly 17, 202414 min read
Semantic Search Engine for Emojis in 50+ Languages Using AI 😊🌍🚀
If you are on social media like Twitter or LinkedIn, you have probably noticed that…
If Oral and Written Communication Made Humans Develop Intelligence… What’s Up with Language Models?Artificial IntelligenceEssay A discussion at the frontier between science and fiction. Human intelligence, with its extraordinary…Luciano AbriataJune 30, 202315 min read
If Oral and Written Communication Made Humans Develop Intelligence… What’s Up with Language Models?
Essay A discussion at the frontier between science and fiction. Human intelligence, with its extraordinary…
Throwing a Cat Among the Pigeons? Augmenting Human Computation With Large Language ModelsArtificial IntelligenceI have always been fascinated by etymology. More often than not, there is an intriguing…Ujwal GadirajuJuly 21, 202314 min read
Throwing a Cat Among the Pigeons? Augmenting Human Computation With Large Language Models
I have always been fascinated by etymology. More often than not, there is an intriguing…

The world’s leading publication for data science, AI, and ML professionals.
LinkedIn
X
Large Language Models
Step-by-Step Guide to Build and Deploy an LLM-Powered Chat with Memory in StreamlitLarge Language ModelsAnd monitor your API usage on Google Cloud ConsoleALESSANDRA COSTAMay 1, 202517 min read
Step-by-Step Guide to Build and Deploy an LLM-Powered Chat with Memory in Streamlit
And monitor your API usage on Google Cloud Console
From FOMO to Opportunity: Analytical AI in the Era of LLM AgentsLarge Language ModelsWhy the gold rush toward LLM agents does not make analytical AI obsoleteShuai GuoApril 29, 202513 min read
From FOMO to Opportunity: Analytical AI in the Era of LLM Agents
Why the gold rush toward LLM agents does not make analytical AI obsolete
Building a Scalable and Accurate Audio Interview Transcription Pipeline with Google GeminiLarge Language ModelsFrom prototype to production: real-world insights into building smarter transcription pipelines with LLMs.Ugo PradèreApril 29, 202521 min read
Building a Scalable and Accurate Audio Interview Transcription Pipeline with Google Gemini
From prototype to production: real-world insights into building smarter transcription pipelines with LLMs.
Hands-on Multi Agent LLM Restaurant Simulation, with Python and OpenAILarge Language ModelsThis is how I used Large Language Models Agents to simulate an end-to-end restaurant process,…Piero PaialungaApril 28, 202512 min read
Hands-on Multi Agent LLM Restaurant Simulation, with Python and OpenAI
This is how I used Large Language Models Agents to simulate an end-to-end restaurant process,…
A Step-By-Step Guide To Powering Your Application With LLMsLarge Language ModelsExplore a hands-on guide to integrating large language models into real-world apps, not just read…Prasann Pradeep PatilApril 25, 20258 min read
A Step-By-Step Guide To Powering Your Application With LLMs
Explore a hands-on guide to integrating large language models into real-world apps, not just read…
Behind the Magic: How Tensors Drive TransformersLarge Language ModelsThe workflow Of tensors Inside TransformersZiad SALLOUMApril 25, 20254 min read
Behind the Magic: How Tensors Drive Transformers
The workflow Of tensors Inside Transformers
How to Benchmark DeepSeek-R1 Distilled Models on GPQA Using Ollama and OpenAI’s simple-evalsLarge Language ModelsSet up and run the GPQA-Diamond benchmark on DeepSeek-R1’s distilled models locally to evaluate its…Kenneth LeungApril 23, 202512 min read
How to Benchmark DeepSeek-R1 Distilled Models on GPQA Using Ollama and OpenAI’s simple-evals
Set up and run the GPQA-Diamond benchmark on DeepSeek-R1’s distilled models locally to evaluate its…
Retrieval Augmented Generation (RAG) — An IntroductionLarge Language ModelsGiven the critical need to keep models updated in a time and cost effective way,…Carolina BentoApril 21, 20257 min read
Retrieval Augmented Generation (RAG) — An Introduction
Given the critical need to keep models updated in a time and cost effective way,…
Load-Testing LLMs Using LLMPerfLarge Language ModelsBenchmark Claude 3 Sonnet on Amazon BedrockRam VegirajuApril 18, 20258 min read
Load-Testing LLMs Using LLMPerf
Benchmark Claude 3 Sonnet on Amazon Bedrock
An Unbiased Review of Snowflake’s Document AILarge Language ModelsOr, how we spared a human from manually inspecting 10,000 flu shot documents.Ben TengelsenApril 15, 20258 min read
An Unbiased Review of Snowflake’s Document AI
Or, how we spared a human from manually inspecting 10,000 flu shot documents.

The world’s leading publication for data science, AI, and ML professionals.
LinkedIn
X
Author: ALESSANDRA COSTA
Step-by-Step Guide to Build and Deploy an LLM-Powered Chat with Memory in StreamlitLarge Language ModelsAnd monitor your API usage on Google Cloud ConsoleALESSANDRA COSTAMay 1, 202517 min read
Step-by-Step Guide to Build and Deploy an LLM-Powered Chat with Memory in Streamlit
And monitor your API usage on Google Cloud Console

The world’s leading publication for data science, AI, and ML professionals.
LinkedIn
X
Practical Eigenvectors
Learn 80% about what they are and their applications with 20% effort
Motivation
Eigenvectors are a central concept of linear algebra with a wide range of exciting applications. However, they may be non-intuitive and intimidating, and linear algebra defines these concepts in very rigorous and generic terms spanning hundreds of pages. Moreover, the information on what they are and how they’re used in various applications is scattered in different places.
This article makes eigenvectors friendlier with simple visualizations and exciting applications.
We assume that the reader is familiar with the basic matrix addition and multiplication operations. We only discuss finite-dimensional vector spaces over ℝ (real numbers)[1].
Vectors and bases
In the N-dimensional space,a vector\(v\) is a list of N scalars: \[v=\begin{bmatrix}x_1 \\ x_2 \\ \vdots \\ x_N\end{bmatrix}\]
The standard basis(S) is a special group of N vectors \(s_1, s_2, \dots, s_N\) such that \(s_k\) has 1 on kthposition and 0 otherwise.By default, every vector is defined with respect to the standard basis. In other words, the meaning of \(v\) above is that \(v = x_1 \cdot s_1 + x_2 \cdot s_2 + \dots + x_N \cdot s_N\). To make the basis explicit, we subscript it: \(v=v_S\).
Geometrically, a vector is an arrow starting from the fixed origin of the N-dimensional space and ending in the point identified by its components.
The image below depicts in 2D space the standard basis \(s_1 = \begin{bmatrix} 1 \\ 0 \end{bmatrix}\), \(s_2 = \begin{bmatrix} 0 \\ 1 \end{bmatrix}\) and two other vectors \(v_S = \begin{bmatrix} 3 \\ -1 \end{bmatrix}\), \(w_S = \begin{bmatrix} 1 \\ 1 \end{bmatrix}\):
A group of vectors areindependentif none of them can be written as a weighted sum of the others. Vectors \(\begin{bmatrix} 3 \\ -1 \end{bmatrix}\) and \(\begin{bmatrix} 1 \\ 1 \end{bmatrix}\) are independent, but \(v = \begin{bmatrix} 1 \\ 1 \end{bmatrix}\) and \(u = \begin{bmatrix} 2 \\ 2 \end{bmatrix}\) are not because \(u = 2 \cdot v\).
In an N-dimensional space, abasisis any group of N vectors that are independent. The standard basis is not the only basis. Given a basis, every vector of the space can be written uniquely as a weighted sum of those basis vectors.
Therefore, the same vector can be defined with respect to different bases. In each case the value and meaning of each of its components may change, but the vector remains the same. In the example above we chose the standard basis and defined vectors \(v\) and \(w\) with respect to \(s_1\) and \(s_2\). Now let’s choose the basis as vectors \(v\) and \(w\), and instead write \(s_1\) and \(s_2\) respect to this new basis.
Say \(v_S\) is defined with respect to the standard basis and we want to redefine it as \(v_B\) with respect to another basis B (\(b_1, b_2, \dots, b_N\)).
First, define the N-by-N matrix \(B\) such that its jthcolumn is \(b_{jS}\).Then \(v_B = B^{-1} \cdot v_S\) and \(v_S = B \cdot v_B\).
Operators
An operator is an N-by-N matrix \(O_S\) describing how it maps a vector (\(v_S\)) to another vector (\(u_S\)): \(u_S=O_S \cdot v_S\).Think of vectors as “data”, and of operators as “transformation[3]” of the data.
In the 2D space we find some familiar classes of operators.
\(O_1 = \begin{bmatrix} k_x & 0 \\ 0 & k_y \end{bmatrix}\), for example \(O_1 = \begin{bmatrix} 1.5 & 0 \\ 0 & 2 \end{bmatrix}\).Below, the left image shows the original 2D space, the middle shows the space after being transformed by the operator \(O_1\), and the right image shows scaled gradients of how the points get moved.
\(O_2 = \begin{bmatrix} 1 & s_x \\ s_y & 1 \end{bmatrix}\), for example \(O_2 = \begin{bmatrix} 1 & 0.25 \\ 0.5 & 1 \end{bmatrix}\).
\(O_3 = \begin{bmatrix} cos \phi & -sin \phi \\ sin \phi & cos \phi \end{bmatrix}\) rotates the vectors counter-clockwise by \(\phi\).For example \(O_3 = \begin{bmatrix} 0.866 & -0.5 \\ 0.5 & 0.866 \end{bmatrix}\) rotates by \(30^{\circ} \).
If operator \(O\) is the composition of two operators \(O_1\) and \(O_2\), such that first we transform vectors with \(O_1\) and subsequently with \(O_2\), then \(O = O_2 \cdot O_1\).For example, to compose the operators above (rotate, then shear, then scale): \(O_4 = O_1 \cdot O_2 \cdot O_3 = \begin{bmatrix} 1.5 & 0 \\ 0 & 2\end{bmatrix} \cdot \begin{bmatrix} 1 & 0.25 \\ 0.5 & 1 \end{bmatrix} \cdot \begin{bmatrix} 0.866 & -0.5 \\ 0.5 & 0.866 \end{bmatrix} \), hence \(O_4 = \begin{bmatrix} 1.4865 & -0.42525 \\ 1.866 & 1.232 \end{bmatrix} \).
Perhaps surprisingly, translation is not an operator (not a linear-transform). It can be implemented as an operator by adding a temporary dimension to the space.
Example:in 2D, to translate vectors \(v = \begin{bmatrix} v_x \\ v_y \end{bmatrix} \) horizontally by \(t_x\) and vertically by \(t_y\) to \(u = \begin{bmatrix} v_x + t_x \\ v_y + t_y \end{bmatrix}\), first add a third dimension to it with a component of 1: \(v = \begin{bmatrix} v_x \\ v_y \\ 1 \end{bmatrix} \). Now we can use that extra component with an operator like \(O=\begin{bmatrix} 1 & 0 & t_x \\ 0 & 1 & t_y \\ 0 & 0 & 1 \\ \end{bmatrix}\). Then \(u = O \cdot v = \begin{bmatrix} v_x + t_x \\ v_y + t_y \\ 1 \end{bmatrix} \). Finally, drop the temporary dimension.
Note:affine transformation in homogeneous coordinates works similarly –here.
Note:SVG implements 2D translation this way –here.
If vector definitions change with respect to different bases, so do operators.Say \(O_S\) is defined with respect to the standard basis, and we want to redefine it as \(O_B\) with respect to another basis B (\(b_1, b_2, \dots, b_N\)).
Once again, define the N-by-N matrix \(B\) such that its jthcolumn is \(b_{jS}\).Then \(O_B = B^{-1} \cdot O_S \cdot B \) and \(O_S = B \cdot O_B \cdot B^{-1} \).
Eigenvalues and eigenvectors
Given operator \(O\), aneigenvector[2]is any non-zero vector which remains on the same axis (i.e., maintains or reverses direction) when transformed by \(O\). The eigenvector may change its length. Eigenvectors characterize the transformation (not the data).Thus, if there is a vector \(e \neq 0\) and a scalar \(\lambda \) such that \(O \cdot e = \lambda \cdot e\), then \(e\) is aneigenvectorand \(\lambda \) is itseigenvalue.
If \(e\) is an eigenvector, then so it every multiple of \(e\) (but they’re not independent). Therefore, we are generally interested in the axis of an eigenvector rather than the particular eigenvector on it.
The operator may have up to N independent eigenvectors. Any list of N independent eigenvectors is a basis (eigenbasis).
Repeatedly applying the operator to any vector \(v \neq 0 \) will eventually converge towards an eigenvector with the largest absolute eigenvalue (unless \(v\) is an eigenvector already). This is depicted intuitively in the gradient-images below, and will become more obvious once we discover the diagonal form of an operator (in application #1). Some operators converge slowly, but those with sparse matrices converge quickly.
\(O=\begin{bmatrix} 1 & 2 \\ 2 & 1 \end{bmatrix}\) has two eigenvector axes \(e_1=\begin{bmatrix} t \\ t \end{bmatrix} \), \(e_2=\begin{bmatrix} t \\ -t \end{bmatrix}, \forall t \neq 0 \) with \(\lambda_1=3\), \(\lambda_2=-1\) respectively.The images below depict this transformation and the two eigenvector axes shown as red lines in the rightmost image.
\(O=\begin{bmatrix} 1 & 0.5 \\ 0 & 1 \end{bmatrix}\) has single eigenvector axis \(e=\begin{bmatrix} t \\ 0 \end{bmatrix}, \forall t \neq 0 \), \(\lambda=1\).
\(O=\begin{bmatrix} 0.866 & -0.5 \\ 0.5 & 0.866 \end{bmatrix}\) has no eigenvectors.
Note:in 2D rotations have no eigenvectors (in 3D they have one eigenvector axis).
\(O=\begin{bmatrix} 2 & 0 \\ 0 & 2 \end{bmatrix}\) has all non-zero vectors as eigenvectors with \(\lambda=2\).
Note:for identity or uniform scale operators (where \(k_x = k_y\)) all vectors are eigenvectors. Although all axes are eigenvector axes, you can only choose 2 (N in N-dimensions) such that they are independent.
Recall that an eigenvalue is the scalar \(\lambda\) in the equation \(O \cdot e = \lambda \cdot e\).So we want to find \(\lambda\) such that \((O-\lambda \cdot I) \cdot e=0, e \neq 0\).Thus find \(\lambda\) such that \(det(O-\lambda \cdot I)=0\).
In 2D, if \(O=\begin{bmatrix} a & b \\ c & d \end{bmatrix} \) then \(\lambda_{1,2}=\frac{a+d \pm \sqrt{(a+d)^2 – 4 (a d – b c)} }{2} \):
if the \(\sqrt{\cdot}\) term is undefined in ℝ, then the operator has no eigenvalues (and no eigenvectors).Note:it would always be defined if our space was over ℂ (complex numbers), in which case even rotations would have eigenvalues
if \(\lambda_1 \neq \lambda_2\), then the operator has exactly two eigenvector axes
if \(\lambda_1 = \lambda_2\), then the operator either has a single eigenvector axis or all axes are
First determine the eigenvalues. Then for each eigenvalue \(\lambda_k\), solve for \(e_k\) in the system of equations: \((O-\lambda_k \cdot I) \cdot e_k=0, e_k \neq 0\). Recall that \(det(O-\lambda \cdot I)=0\) thus these equations are not independent. So expect to find not unique solutions but classes of solutions where at least one variable remains free.
In 2D, if \(O=\begin{bmatrix} a=1 & b=2 \\ c=2 & d=1 \end{bmatrix} \) then \(\lambda_1=3\) and \(\lambda_2=-1\).From \((O-\lambda_1 \cdot I) \cdot e_1=0\) then \(\begin{bmatrix} 1-3 & 2 \\ 2 & 1-3 \end{bmatrix} \cdot e_1=0\).Then \(-2 \cdot e_{1x} + 2 \cdot e_{1y} = 0\) then \(e_{1x}=e_{1y}=t\) so \(e_1=\begin{bmatrix} t \\ t \end{bmatrix}, \forall t \neq 0 \).Similarly, from \((O-\lambda_2 \cdot I) \cdot e_2=0\) we get \(e_2=\begin{bmatrix} t \\ -t \end{bmatrix}, \forall t \neq 0 \).
A square matrix \(A\) and its transpose  \(A^T\) have the same eigenvalues[18].
A stochastic[4]matrix has only positive values and each row sums up to 1. A stochastic matrix always has \(\lambda=1\) as an eigenvalue, which is also its largest[17].
All independent eigenvectors of a symmetric matrix are orthogonal to each other[20]. In other words the projection of one onto another is \(0=\sum_{k}{e_{ik} \cdot e_{jk}}\).
Applications
It may seem that the eigenvectors are so narrowly specified that they can’t be very consequential. They are! Let’s look at some exciting applications.
Given matrix \(A\), what is \(A^k (k \in ℕ, k \gg 1)\)?
To solve this, consider \(A\) as the definition of an operator \(O_S\) (relative to the standard basis). Choose an eigenbasis E and redefine \(O_S\) as \(O_E\) (relative to the eigenbasis). Relative to E, \(O_E\) looks like a simple scaling operator. In other words,\(O_E\) is a diagonal matrix, whose diagonal is the eigenvalues.
So \(A=O_S=E \cdot O_E \cdot E^{-1} \) where \(E=\begin{bmatrix} \overrightarrow{e_1} & | & \overrightarrow{e_2} & | & \dots & | & \overrightarrow{e_N} \end{bmatrix}\) (eigenvectors as columns) and \(O_E=\begin{bmatrix} \lambda_1 & 0 & \dots & 0 \\ 0 & \lambda_2 & \dots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \dots & \lambda_N \end{bmatrix} \) (eigenvalues as diagonal).
Because \(A^k\) means applying the transformation k times, then \(A^k=E \cdot O_E^k \cdot E^{-1} \). Finally, because \(O_E\) is a diagonal matrix, its kthpower is trivial: \(O_E^k=\begin{bmatrix} \lambda_1^k & 0 & \dots & 0 \\ 0 & \lambda_2^k & \dots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \dots & \lambda_N^k \end{bmatrix} \).
Once we determine matrices \(O_E\) and  \(E\) and \(E^{-1}\), computing  \(A^k\) is  \(O(N^3)\) operations (down from  \(O(k \cdot N^3) \) of the naive approach). This makes it possible to compute large (sometimes infinite) powers of a matrix.
Problem:let \(A=\begin{bmatrix} -2 & 1 \\ -4 & 3 \end{bmatrix} \), what is \(A^{1000}\)?
First, determine the eigenvalues as \(\lambda_1=-1\) and \(\lambda_2=2\).Next, find the eigenbasis as \(e_1=\begin{bmatrix} 1 \\ 1 \end{bmatrix} \) and \(e_2=\begin{bmatrix} 1 \\ 4 \end{bmatrix} \).Thus \(E=\begin{bmatrix} 1 & 1 \\ 1 & 4 \end{bmatrix} \) and \(E^{-1}=\begin{bmatrix} 4 & -1 \\ -1 & 1 \end{bmatrix} \cdot \frac{1}{3} \) and \(O_E=\begin{bmatrix} -1 & 0 \\ 0 & 2 \end{bmatrix} \).Then \(A^n=E \cdot O_E^n \cdot E^{-1}=\begin{bmatrix} 1 & 1 \\ 1 & 4 \end{bmatrix} \cdot \begin{bmatrix} (-1)^n & 0 \\ 0 & 2^n \end{bmatrix} \cdot \begin{bmatrix} 4 & -1 \\ -1 & 1 \end{bmatrix}  \cdot \frac{1}{3} \).Then \(A^n=\begin{bmatrix} 4 \cdot (-1)^n-2^n & (-1)^{n+1}+2^{1000} \\ 4 \cdot (-1)^n-2^{n+2} & (-1)^{n+1}+2^{1002} \end{bmatrix}  \cdot \frac{1}{3} \).Finally \(A^{1000}=\begin{bmatrix} 4-2^{1000} & 2^{1000}-1 \\ 4-2^{1002} & 2^{1002}-1 \end{bmatrix}  \cdot \frac{1}{3} \).
Problem:what is the direct formula for the nthFibonacci term?
Because each \(f_k\) is the sum of the previous two, we need a system with two units of memory – a 2D space.
Let \(v_{kS}=\begin{bmatrix} f_{k-1} \\ f_k \end{bmatrix} \) and \(v_{1S}=\begin{bmatrix} f_0 \\ f_1 \end{bmatrix}=\begin{bmatrix} 0 \\ 1 \end{bmatrix} \). See the first few vectors:
Let operator \(O_S=\begin{bmatrix} 0 & 1 \\ 1 & 1 \end{bmatrix}\) so that \(v_{k+1} = O_S \cdot v_k = \begin{bmatrix} 0 & 1 \\ 1 & 1 \end{bmatrix}\ \cdot \begin{bmatrix} f_{k-1} \\ f_k \end{bmatrix} = \begin{bmatrix} f_k \\ f_{k+1} \end{bmatrix}\).
Therefore \(v_{nS}=O_S^{n-1} \cdot v_{1S}\).
Next, find that \(\lambda_1=\frac{1+\sqrt{5}}{2}\), \(\lambda_2=\frac{1-\sqrt{5}}{2}\), and \(e_1=\begin{bmatrix} 1 \\ \lambda_1 \end{bmatrix} \), \(e_2=\begin{bmatrix} 1 \\ \lambda_2 \end{bmatrix} \).Hence \(O_E=\begin{bmatrix} \lambda_1 & 0 \\ 0 & \lambda_2 \end{bmatrix}\), \(E=\begin{bmatrix} 1 & 1 \\ \lambda_1 & \lambda_2 \end{bmatrix}\), \(E^{-1}=\begin{bmatrix} -\lambda_2 & 1 \\ \lambda_1 & -1 \end{bmatrix} \cdot \frac{1}{\sqrt{5}} \).
So \(v_{nS}=O_S^{n-1} \cdot v_{1S} = E \cdot O_E^{n-1} \cdot E^{-1} \cdot v_{1S}\).\(v_{nS}=\begin{bmatrix} \lambda_1^{n-1}-\lambda_2^{n-1} \\ \lambda_1^n – \lambda_2^n \end{bmatrix} \cdot \frac{1}{\sqrt{5}} = \begin{bmatrix} f_{n-1} \\ f_n \end{bmatrix}\).
Finally, \(f_n=\frac{1}{\sqrt{5}}\cdot(\frac{1+\sqrt{5}}{2})^n – \frac{1}{\sqrt{5}}\cdot(\frac{1-\sqrt{5}}{2})^n\).
Problem:what is the formula for geometric series?
Let the geometric series be \(g_n=c + c \cdot t^1 + c \cdot t^2 + \dots  + c \cdot t^n \).Rewrite it in a recursive fashion: \(g_{n+1}=g_n + t \cdot a^n \), with \(a_n=c \cdot t^n\). Once again we need a system with two memory units.
Let \(v_{kS}=\begin{bmatrix} a_k \\ g_k \end{bmatrix} \) and \(v_{0S}=\begin{bmatrix} c \\ c \end{bmatrix} \).
Let operator \(O_S=\begin{bmatrix} t & 0 \\ t & 1 \end{bmatrix}\) so that \(v_{k+1} = O_S \cdot v_k = \begin{bmatrix} t & 0 \\ t & 1 \end{bmatrix}\ \cdot \begin{bmatrix} a_k \\ g_k \end{bmatrix} = \begin{bmatrix} t \cdot a_k \\ t \cdot a_k + g_k \end{bmatrix} = \begin{bmatrix} a_{k+1} \\ g_{k+1} \end{bmatrix}\).
Next, find that \(\lambda_1=1\), \(\lambda_2=t\), and \(e_1=\begin{bmatrix} 0 \\ 1 \end{bmatrix} \), \(e_2=\begin{bmatrix} \frac{t-1}{t} \\ 1 \end{bmatrix} \).Hence \(O_E=\begin{bmatrix} 1 & 0 \\ 0 & t \end{bmatrix}\), \(E=\begin{bmatrix} 0 & \frac{t-1}{t} \\ 1 & 1 \end{bmatrix}\), \(E^{-1}=\begin{bmatrix} \frac{t}{1-t} & 1 \\ -\frac{t}{1-t} & 0 \end{bmatrix} \).
So \(v_{nS}=O_S^n \cdot v_{0S} = E \cdot O_E^n \cdot E^{-1} \cdot v_{0S}\).\(v_{nS}= c \cdot \begin{bmatrix} t^n \\ \frac{1-t^{n+1}}{1-t} \end{bmatrix} = \begin{bmatrix} a_n \\ g_n \end{bmatrix}\).
Finally, \(g_n=c \cdot \frac{1-t^{n+1}}{1-t}, \forall t > 1\).
A Markov Chain is a weighted and directed graph such that for each node the sum of all outgoing edges is 1. Self-edges are allowed, and each node may hold a value.
One interpretationis that each node represents a certain state (with a certain initial probability), and at each iteration the next state is one of the adjacent nodes with a probability equal to the weight of the edge.Another interpretationis that each node begins with a certain amount of energy, and in each iteration it passes it to each adjacent node proportionally to the edge weights.
Either way, the info on the nodes make up a piece of data (a vector) and the edges make up a transformation (an operator). N nodes means an N dimensional space.
The operator \(O_S\) defining the transition with each iteration is a column-stochastic matrix – so it has values between 0 and 1, and each column sums up to 1. Specifically, its kthcolumn is the probability vector associated with the outgoing edges of node k.
Stochastic matrices always have \(\lambda_1=1\) as their largest eigenvalue. The corresponding eigenvector \(e_1\) (such that \(A \cdot e_1 = e_1 \) and \(sum(e_1)=1\)) represents the steady-state of the system: the probability that a random walker ends in each of the nodes after \(\infty\) steps (or the energy each node has after \(\infty\) iterations). The only exception is when the initial system state is already an eigenvector, in which case the system remains locked in that state.
Problem:find the steady-state of a simple Markov chain
For this Markov chain, the adjacency matrix is \(A=\begin{bmatrix} 0.4 & 0.3 \\ 0.6 & 0.7 \end{bmatrix} \).\(\lambda_1=1\) (stochastic matrix) and \(e_1=\begin{bmatrix} t \\ 2t \end{bmatrix}\). But enforcing \(sum(e_1)=1\) means \(e_1=\begin{bmatrix} \frac{1}{3} \\ \frac{2}{3} \end{bmatrix}\). Validate that \(A \cdot e_1 = e_1 \) as expected.
Finally, after \(\infty\) iterations, a random walker is in n1with probability ⅓ and in n2with ⅔. Or, the energy in n1is ⅓ and in n2is ⅔ of the global energy.
Problem:compute Google Page-Rank for all web pages
Part of the calculation of the rank (importance) of each web page, is determining how other pages link to it and their own importance.
Therefore, a giant Markov Chain is created such that each node is a web page, and each edge represents that the source node links to the target node. For each node, the weights on the outgoing edges are all equal: \(weight=\frac{1}{degree(source node)}\).
Note:additional tricks are employed to ensure no node becomes a deadend[5](no energy sinks).
Then the eigenvector \(e_1\) (the steady-state of the graph) is the page-rank of each node.
Note:for practical reasons, considering the enormous size of this system, \(e_1\) is not calculated directly. Instead, it is approximated by applying the transform operator on an initial vector a number of times. Given that the operator matrix is sparse, it will quickly converge to \(e_1\).
Given a connected undirected graph (or network), find K clusters (or communities) such that nodes in each cluster are more connected to each other than to nodes outside the cluster.
Note:the quality of the clusters is hard to measure, and the problem statement is more intuitive than rigorous. For this many measures have been proposed, with modularity[7](by Newman) defined as “the fraction of the edges that fall within the given groups minus the expected fraction if edges were distributed at random” being the most popular.
First, define the following matrices:
\(A\) is the adjacency matrix
\(D\) s a diagonal matrix, such that \(D_{ii}=degree(node_i)=\sum_j A_{ij}\)
\(L=D-A\) calledthe Laplacian matrix[14].
Intuition:L is akin to a differential operator, therefore eigenvectors with large eigenvalues correspond to cuts with “high vibrations” (maximal cuts). But a good clustering is similar to a minimal cut, so in this case we’re interested in eigenvectors with smallest eigenvalues[22].
Let’s convene that that \(\lambda_1\) through \(\lambda_N\) are in ascending order – in fact \(\lambda_1=0\) is the smallest[19].
Note:if the graph is unconnected, 0 will appear multiple times as an eigenvalue. In fact, if there are C connected components in the graph, 0 will appear as an eigenvalue with multiplicity C. However, in this section we’re assuming the graph is connected (since that’s the interesting part), so 0 has multiplicity 1.
Note that L is symmetric and each row (and column) sums up to 0: \(\sum_j L_{ij}=0, \forall i\). So L has \(\lambda_1=0\) and \(e_1=\begin{bmatrix} 1 \\ 1 \\ \vdots \\ 1 \end{bmatrix} \).Proof:\((L-0 \cdot I) \cdot e_1 = L \cdot e_1 = 0 = 0 \cdot e_1\).
Also, because L is symmetric, all eigenvectors of its eigenbasis are orthogonal to each other: \(\sum_k {e_{ik} \cdot e_{jk}} = 0\). Thus, if we choose \(j=1\) in the previous equation we find that each eigenvector (other than \(e_1\)) sums up to 0: \(\sum_k{e_{ik}}=0, \forall i \ge 2\).
Finally, if we want to find:
\(K=2\) clusters, simply look at \(e_2\) to group the nodes into:those corresponding to a positive component of \(e_2\), andthose corresponding to a negative component of \(e_2\)
those corresponding to a positive component of \(e_2\), and
those corresponding to a negative component of \(e_2\)
\(K \ge 3\) clusters, then for each node define \(v_i=\begin{bmatrix} e_{2i} \\ e_{2i} \\ \vdots \\ e_{Ki} \end{bmatrix} \) as the projection of \(s_i\) onto each of the eigenvectors \(e_2\) through \(e_K\). Finally, cluster the nodes using the K-means algorithm on the newly defined \(v_i\) vectors.
Problem:find the 2 communities in Zachary’s Karate Club network
Zachary’s Karate Club[8]network is a famous network that represents the 78 social connections (members interacting outside the club) between the 34 members of a karate club he studied from 1970 to 1972.
Later, due to a leadership disagreement, the 34 members split: half of the members formed a new club around the old instructor, and the others found a new instructor or gave up karate.
Based on collected data Zachary correctly assigned all but one member (#9) of the club to the groups they actually joined after the split.
In the Python code below, variable “a” is the 34-by-34 adjacency matrix. It runs an algorithm similar to the one above as a one-liner:
labels = sklearn.cluster.SpectralClustering(n_clusters=2).fit(a).labels_
labels = sklearn.cluster.SpectralClustering(n_clusters=2).fit(a).labels_
The resulting partition correctly matches the real world split described in Zachary’s original paper, except for a single node (the same #9) which is described in the original paper[9]as having low-affinity to either group.
Note:the network clustering problem is NP-complete, and while “spectral clustering” is known to perform very well it doesn’t guarantee absolute optimal results.
If the samples in a dataset are N-dimensional vectors, we want to reduce them to K-dimensional vectors while retaining most of the information. This is valuable for several reasons:
compress the data
visualize (in 2D or 3D) data that cannot be visualized otherwise
improve model efficiency – train faster and with less resources
mitigate overfitting – remove redundancy to reduce learning “the noise” in the data
There are many algorithms forDimensionality Reduction[13], including PCA, LDA, T-SNE, UMAP and Autoencoders. However, in this section we’ll focus on PCA only.PCA[12]stands for Principal Component Analysis, and we’ll soon see that eigenvectors are the principal components[15].
First, normalize the dataset such that it is centered in 0 with a standard deviation of 1 by:
subtracting from each vector the average vector across the dataset
then for each dimension divide by its standard-deviation across the dataset
Next, compute the N-by-N covariance matrix[11]\(V\) for the N dimensions of the dataset and find itsEigenvectors. If the eigenvalues are sorted indescendingorder, then we choose \(\lambda_1\) through \(\lambda_K\) and \(e_1\) through \(e_K\) (such that they’re unit-length).Interpretation:\(e_1\) through \(e_K\) are the K principal components, or the axes along which the dataset has the highest variance. The variance along the axis of \(e_i\) is \(\lambda_i\). Intuitively, matrix \(V\) defines the operator that is the inverse of a whitening operator, such that \(V^{-1}\) would transform our dataset into white noise[10](uncorrelated data with variance 1 whose covariance is the identity matrix).
Now define matrix K-by-N matrix \(E\) such that its ithrow is \(e_i\). Hence, it’s transpose is \(E^T=\begin{bmatrix} \overrightarrow{e_1} & | & \overrightarrow{e_2} & | & \dots & | & \overrightarrow{e_N} \end{bmatrix}\).
Finally, to reduce any sample \(d_N\) from N to K dimensions, simply compute \(d_K=E \cdot d_N\).
Problem:compress a dataset of photos of human faces
Eigenfacesis a way to convert an image of N pixels of a human face to K numbers such that \(K \ll N\). It computes the first K principal components of the dataset (the Keigenfaces), then it expresses the original image in terms of these K eigenvectors. This method is generally used in face recognition problems. In this example, we use it to compress a dataset of human faces.
For this experiment I used Python (codehere) and the “Biggest Gender/Face Recognition” dataset[21](licensed CC0) which includes over 10,000 images of 250 x 250 = 62500 color pixels. I transform them into grayscale (so each pixel is one number, not three), because computing eigenvectors for such a large matrix can be slow.
Each picture becomes a PyTorch tensor of N=62500 dimensions. Normalize (subtract the average and divide by standard deviation) and find the largest K=1500 eigenvectors usingSciPy eighfunction (useeighnoteigsince a covariance matrix is symmetric). Now we have matrix \(E\).
To demo it, I take three images and convert each to N-dimensional \(v_N\) same as above. Then the compressed face is \(v_K=E \cdot v_N\), and now \(v_N \approx v_K \cdot E\). Finally, un-normalize it to visualize. The next collage shows 3 original photos (top) and their reconstruction (bottom):
The next image shows the top 25 eigenfaces:
Finally, the next image shows the average face (left), standard deviation face (middle), and their ratio (ration):
Problem:visualize high-dimensional data in 2D
We can’t visualize 4+dimensional space, but we can visualize 2D and 3D spaces. This is useful when we want to understand the data better, to debug a classifier that doesn’t quite work, or to understand if the data naturally forms clear groups.
To demo this, I use the IRIS dataset[23](license CC BY 4.0) and project each sample (originally 4D) against the top two eigenvectors.
Then plot the results in a 2D plane colored by iris species. The Python code ishere.
The results are promising, as the three iris species segregate quite well along the dominant two eigenvectors. These two eigenvectors would be effective in an iris species classifier.
Thank you
Thank you for reading this far!I hope this article made eigenvectors intuitive and offered an exciting overview of their wide applicability.
References
The main inspiration for this article is Sheldon Axler’s bookLinear AlgebraDone Right[1].
Sheldon Axler,Linear Algebra Done Right (2024), Springer
Eigenvalues and eigenvectors, Wikipedia
Transformation matrix, Wikipedia
Stochastic matrix, Wikipedia
PageRank Algorithm – The Mathematics of Google Search (2009), Cornell
Perron–Frobenius theorem, Wikipedia
Modularity (networks), Wikipedia
Zachary’s karate club, Wikipedia
Wayne W. Zachary,An Information Flow Model for Conflict and Fission in Small Groups (1977), Journal of Anthropological Research: Vol 33, No 4
Whitening transformation, Wikipedia
Covariance matrix, Wikipedia
Principal component analysis, Wikipedia
Stephen Oladele,Top 12 Dimensionality Reduction Techniques for Machine Learning (2024)
MIT OpenCourseWare,Finding Clusters in Graphs (2019), YouTube
Victor Lavrenko,PCA 4: principal components = eigenvectors (2014), YouTube
3Blue1Brown,Eigenvectors and eigenvalues | Chapter 14, Essence of linear algebra (2016), YouTube
Proof that the largest eigenvalue of a stochastic matrix is 1 (2011), Mathematics Stack Exchange
A matrix and its transpose have the same set of eigenvalues/other version (2012), Mathematics Stack Exchange
Laplacian matrix, Wikipedia
Orthogonality of eigenvectors of laplacian (2013), Mathematics Stack Exchange
Biggest gender/face recognition dataset (2021), Kaggle
Shriphani Palakodety,The Smallest Eigenvalues of a Graph Laplacian (2015)
R. A. Fisher,Iris dataset (2021), UCI Machine Learning Repository
Written By
Topics:
Share this article:
Share on Facebook
Share on LinkedIn
Share on X
Related Articles
Optimizing Marketing Campaigns with Budgeted Multi-Armed BanditsData ScienceWith demos, our new solution, and a videoVadim ArzamasovAugust 16, 202410 min read
Optimizing Marketing Campaigns with Budgeted Multi-Armed Bandits
With demos, our new solution, and a video
Methods for Modelling Customer Lifetime Value: The Good Stuff and the GotchasAnalyticsPart three of a comprehensive, practical guide to CLV techniques and real-world use-casesKatherine MunroNovember 17, 202312 min read
Methods for Modelling Customer Lifetime Value: The Good Stuff and the Gotchas
Part three of a comprehensive, practical guide to CLV techniques and real-world use-cases
Done is Better Than PerfectData ScienceHow to be more pragmatic as a Data Scientist, and why it matters for your…Torsten WalbaumJuly 30, 202411 min read
Done is Better Than Perfect
How to be more pragmatic as a Data Scientist, and why it matters for your…
Latest picks: Data Quality for Everyday AnalysisData ScienceYour daily dose of data scienceTDS EditorsNovember 17, 20201 min read
Latest picks: Data Quality for Everyday Analysis
Your daily dose of data science
Latest picks: No Free Lunch with Feature BiasData ScienceYour daily dose of data scienceTDS EditorsJanuary 25, 20211 min read
Latest picks: No Free Lunch with Feature Bias
Your daily dose of data science
Latest picks: Why & How to use the Naive Bayes algorithms in a regulated industry with sklearnData ScienceYour daily dose of data scienceTDS EditorsDecember 2, 20201 min read
Latest picks: Why & How to use the Naive Bayes algorithms in a regulated industry with sklearn
Your daily dose of data science
Latest picks: Deep Few-shot Anomaly DetectionArtificial IntelligenceYour daily dose of data scienceTDS EditorsNovember 10, 20201 min read
Latest picks: Deep Few-shot Anomaly Detection
Your daily dose of data science

The world’s leading publication for data science, AI, and ML professionals.
LinkedIn
X
Data Science
Practical EigenvectorsData ScienceLearn 80% about what they are and their applications with 20% effortConstantin DumitrascuMay 1, 202521 min read
Practical Eigenvectors
Learn 80% about what they are and their applications with 20% effort
Reducing Time to Value for Data Science Projects: Part 1Data ScienceCreating centralized and re-usable components for ease of experimentationKristopher McGlincheyApril 30, 202510 min read
Reducing Time to Value for Data Science Projects: Part 1
Creating centralized and re-usable components for ease of experimentation
Data Analyst or Data Engineer or Analytics Engineer or BI Engineer ?Data ScienceWhat’s the real difference?Isha GargApril 29, 20256 min read
Data Analyst or Data Engineer or Analytics Engineer or BI Engineer ?
What’s the real difference?
Struggling to Land a Data Role in 2025? These 5 Tips Will Change ThatData ScienceYour dream data job isn’t ghosting you—you just need to search smart.Chiedozie OnyearugbulemApril 28, 20257 min read
Struggling to Land a Data Role in 2025? These 5 Tips Will Change That
Your dream data job isn’t ghosting you—you just need to search smart.
AWS: Deploying a FastAPI App on EC2 in MinutesData EngineeringFrom zero to EC2: easy steps to launching an AWS InstanceVyacheslav EfimovApril 24, 20255 min read
AWS: Deploying a FastAPI App on EC2 in Minutes
From zero to EC2: easy steps to launching an AWS Instance
Government Funding Graph RAGData ScienceGraph visualisation for UK Research and Innovation (UKRI) funding, including NetworkX, PyVis and LlamaIndex graph…Lewis JamesApril 24, 202519 min read
Government Funding Graph RAG
Graph visualisation for UK Research and Innovation (UKRI) funding, including NetworkX, PyVis and LlamaIndex graph…
Predicting the NBA Champion with Machine LearningData ScienceBuilding a machine learning model to predict the NBA Champion and analyze the most impactful…Gabriel PastorelloApril 24, 202510 min read
Predicting the NBA Champion with Machine Learning
Building a machine learning model to predict the NBA Champion and analyze the most impactful…
Exporting MLflow Experiments from Restricted HPC SystemsData EngineeringA workaround method that bypasses direct communicationNagharjun Mathi MariappanApril 23, 20254 min read
Exporting MLflow Experiments from Restricted HPC Systems
A workaround method that bypasses direct communication
MapReduce: How It Powers Scalable Data ProcessingData EngineeringAn overview of the MapReduce programming model and how it can be used to optimize…Jimin KangApril 22, 202515 min read
MapReduce: How It Powers Scalable Data Processing
An overview of the MapReduce programming model and how it can be used to optimize…
Building a Personal API for Your Data Projects with FastAPIData ScienceSupercharge your data projects by turning your code into a lightweight, reusable APIPol MarinApril 22, 20258 min read
Building a Personal API for Your Data Projects with FastAPI
Supercharge your data projects by turning your code into a lightweight, reusable API