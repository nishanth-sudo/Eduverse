

GitHub
GitHub
User Guide#
1. Supervised learning1.1. Linear Models1.1.1. Ordinary Least Squares1.1.2. Ridge regression and classification1.1.3. Lasso1.1.4. Multi-task Lasso1.1.5. Elastic-Net1.1.6. Multi-task Elastic-Net1.1.7. Least Angle Regression1.1.8. LARS Lasso1.1.9. Orthogonal Matching Pursuit (OMP)1.1.10. Bayesian Regression1.1.11. Logistic regression1.1.12. Generalized Linear Models1.1.13. Stochastic Gradient Descent - SGD1.1.14. Perceptron1.1.15. Passive Aggressive Algorithms1.1.16. Robustness regression: outliers and modeling errors1.1.17. Quantile Regression1.1.18. Polynomial regression: extending linear models with basis functions1.2. Linear and Quadratic Discriminant Analysis1.2.1. Dimensionality reduction using Linear Discriminant Analysis1.2.2. Mathematical formulation of the LDA and QDA classifiers1.2.3. Mathematical formulation of LDA dimensionality reduction1.2.4. Shrinkage and Covariance Estimator1.2.5. Estimation algorithms1.3. Kernel ridge regression1.4. Support Vector Machines1.4.1. Classification1.4.2. Regression1.4.3. Density estimation, novelty detection1.4.4. Complexity1.4.5. Tips on Practical Use1.4.6. Kernel functions1.4.7. Mathematical formulation1.4.8. Implementation details1.5. Stochastic Gradient Descent1.5.1. Classification1.5.2. Regression1.5.3. Online One-Class SVM1.5.4. Stochastic Gradient Descent for sparse data1.5.5. Complexity1.5.6. Stopping criterion1.5.7. Tips on Practical Use1.5.8. Mathematical formulation1.5.9. Implementation details1.6. Nearest Neighbors1.6.1. Unsupervised Nearest Neighbors1.6.2. Nearest Neighbors Classification1.6.3. Nearest Neighbors Regression1.6.4. Nearest Neighbor Algorithms1.6.5. Nearest Centroid Classifier1.6.6. Nearest Neighbors Transformer1.6.7. Neighborhood Components Analysis1.7. Gaussian Processes1.7.1. Gaussian Process Regression (GPR)1.7.2. Gaussian Process Classification (GPC)1.7.3. GPC examples1.7.4. Kernels for Gaussian Processes1.8. Cross decomposition1.8.1. PLSCanonical1.8.2. PLSSVD1.8.3. PLSRegression1.8.4. Canonical Correlation Analysis1.9. Naive Bayes1.9.1. Gaussian Naive Bayes1.9.2. Multinomial Naive Bayes1.9.3. Complement Naive Bayes1.9.4. Bernoulli Naive Bayes1.9.5. Categorical Naive Bayes1.9.6. Out-of-core naive Bayes model fitting1.10. Decision Trees1.10.1. Classification1.10.2. Regression1.10.3. Multi-output problems1.10.4. Complexity1.10.5. Tips on practical use1.10.6. Tree algorithms: ID3, C4.5, C5.0 and CART1.10.7. Mathematical formulation1.10.8. Missing Values Support1.10.9. Minimal Cost-Complexity Pruning1.11. Ensembles: Gradient boosting, random forests, bagging, voting, stacking1.11.1. Gradient-boosted trees1.11.2. Random forests and other randomized tree ensembles1.11.3. Bagging meta-estimator1.11.4. Voting Classifier1.11.5. Voting Regressor1.11.6. Stacked generalization1.11.7. AdaBoost1.12. Multiclass and multioutput algorithms1.12.1. Multiclass classification1.12.2. Multilabel classification1.12.3. Multiclass-multioutput classification1.12.4. Multioutput regression1.13. Feature selection1.13.1. Removing features with low variance1.13.2. Univariate feature selection1.13.3. Recursive feature elimination1.13.4. Feature selection using SelectFromModel1.13.5. Sequential Feature Selection1.13.6. Feature selection as part of a pipeline1.14. Semi-supervised learning1.14.1. Self Training1.14.2. Label Propagation1.15. Isotonic regression1.16. Probability calibration1.16.1. Calibration curves1.16.2. Calibrating a classifier1.16.3. Usage1.17. Neural network models (supervised)1.17.1. Multi-layer Perceptron1.17.2. Classification1.17.3. Regression1.17.4. Regularization1.17.5. Algorithms1.17.6. Complexity1.17.7. Tips on Practical Use1.17.8. More control with warm_start
1.1. Linear Models1.1.1. Ordinary Least Squares1.1.2. Ridge regression and classification1.1.3. Lasso1.1.4. Multi-task Lasso1.1.5. Elastic-Net1.1.6. Multi-task Elastic-Net1.1.7. Least Angle Regression1.1.8. LARS Lasso1.1.9. Orthogonal Matching Pursuit (OMP)1.1.10. Bayesian Regression1.1.11. Logistic regression1.1.12. Generalized Linear Models1.1.13. Stochastic Gradient Descent - SGD1.1.14. Perceptron1.1.15. Passive Aggressive Algorithms1.1.16. Robustness regression: outliers and modeling errors1.1.17. Quantile Regression1.1.18. Polynomial regression: extending linear models with basis functions
1.1.1. Ordinary Least Squares
1.1.2. Ridge regression and classification
1.1.3. Lasso
1.1.4. Multi-task Lasso
1.1.5. Elastic-Net
1.1.6. Multi-task Elastic-Net
1.1.7. Least Angle Regression
1.1.8. LARS Lasso
1.1.9. Orthogonal Matching Pursuit (OMP)
1.1.10. Bayesian Regression
1.1.11. Logistic regression
1.1.12. Generalized Linear Models
1.1.13. Stochastic Gradient Descent - SGD
1.1.14. Perceptron
1.1.15. Passive Aggressive Algorithms
1.1.16. Robustness regression: outliers and modeling errors
1.1.17. Quantile Regression
1.1.18. Polynomial regression: extending linear models with basis functions
1.2. Linear and Quadratic Discriminant Analysis1.2.1. Dimensionality reduction using Linear Discriminant Analysis1.2.2. Mathematical formulation of the LDA and QDA classifiers1.2.3. Mathematical formulation of LDA dimensionality reduction1.2.4. Shrinkage and Covariance Estimator1.2.5. Estimation algorithms
1.2.1. Dimensionality reduction using Linear Discriminant Analysis
1.2.2. Mathematical formulation of the LDA and QDA classifiers
1.2.3. Mathematical formulation of LDA dimensionality reduction
1.2.4. Shrinkage and Covariance Estimator
1.2.5. Estimation algorithms
1.3. Kernel ridge regression
1.4. Support Vector Machines1.4.1. Classification1.4.2. Regression1.4.3. Density estimation, novelty detection1.4.4. Complexity1.4.5. Tips on Practical Use1.4.6. Kernel functions1.4.7. Mathematical formulation1.4.8. Implementation details
1.4.1. Classification
1.4.2. Regression
1.4.3. Density estimation, novelty detection
1.4.4. Complexity
1.4.5. Tips on Practical Use
1.4.6. Kernel functions
1.4.7. Mathematical formulation
1.4.8. Implementation details
1.5. Stochastic Gradient Descent1.5.1. Classification1.5.2. Regression1.5.3. Online One-Class SVM1.5.4. Stochastic Gradient Descent for sparse data1.5.5. Complexity1.5.6. Stopping criterion1.5.7. Tips on Practical Use1.5.8. Mathematical formulation1.5.9. Implementation details
1.5.1. Classification
1.5.2. Regression
1.5.3. Online One-Class SVM
1.5.4. Stochastic Gradient Descent for sparse data
1.5.5. Complexity
1.5.6. Stopping criterion
1.5.7. Tips on Practical Use
1.5.8. Mathematical formulation
1.5.9. Implementation details
1.6. Nearest Neighbors1.6.1. Unsupervised Nearest Neighbors1.6.2. Nearest Neighbors Classification1.6.3. Nearest Neighbors Regression1.6.4. Nearest Neighbor Algorithms1.6.5. Nearest Centroid Classifier1.6.6. Nearest Neighbors Transformer1.6.7. Neighborhood Components Analysis
1.6.1. Unsupervised Nearest Neighbors
1.6.2. Nearest Neighbors Classification
1.6.3. Nearest Neighbors Regression
1.6.4. Nearest Neighbor Algorithms
1.6.5. Nearest Centroid Classifier
1.6.6. Nearest Neighbors Transformer
1.6.7. Neighborhood Components Analysis
1.7. Gaussian Processes1.7.1. Gaussian Process Regression (GPR)1.7.2. Gaussian Process Classification (GPC)1.7.3. GPC examples1.7.4. Kernels for Gaussian Processes
1.7.1. Gaussian Process Regression (GPR)
1.7.2. Gaussian Process Classification (GPC)
1.7.3. GPC examples
1.7.4. Kernels for Gaussian Processes
1.8. Cross decomposition1.8.1. PLSCanonical1.8.2. PLSSVD1.8.3. PLSRegression1.8.4. Canonical Correlation Analysis
1.8.1. PLSCanonical
1.8.2. PLSSVD
1.8.3. PLSRegression
1.8.4. Canonical Correlation Analysis
1.9. Naive Bayes1.9.1. Gaussian Naive Bayes1.9.2. Multinomial Naive Bayes1.9.3. Complement Naive Bayes1.9.4. Bernoulli Naive Bayes1.9.5. Categorical Naive Bayes1.9.6. Out-of-core naive Bayes model fitting
1.9.1. Gaussian Naive Bayes
1.9.2. Multinomial Naive Bayes
1.9.3. Complement Naive Bayes
1.9.4. Bernoulli Naive Bayes
1.9.5. Categorical Naive Bayes
1.9.6. Out-of-core naive Bayes model fitting
1.10. Decision Trees1.10.1. Classification1.10.2. Regression1.10.3. Multi-output problems1.10.4. Complexity1.10.5. Tips on practical use1.10.6. Tree algorithms: ID3, C4.5, C5.0 and CART1.10.7. Mathematical formulation1.10.8. Missing Values Support1.10.9. Minimal Cost-Complexity Pruning
1.10.1. Classification
1.10.2. Regression
1.10.3. Multi-output problems
1.10.4. Complexity
1.10.5. Tips on practical use
1.10.6. Tree algorithms: ID3, C4.5, C5.0 and CART
1.10.7. Mathematical formulation
1.10.8. Missing Values Support
1.10.9. Minimal Cost-Complexity Pruning
1.11. Ensembles: Gradient boosting, random forests, bagging, voting, stacking1.11.1. Gradient-boosted trees1.11.2. Random forests and other randomized tree ensembles1.11.3. Bagging meta-estimator1.11.4. Voting Classifier1.11.5. Voting Regressor1.11.6. Stacked generalization1.11.7. AdaBoost
1.11.1. Gradient-boosted trees
1.11.2. Random forests and other randomized tree ensembles
1.11.3. Bagging meta-estimator
1.11.4. Voting Classifier
1.11.5. Voting Regressor
1.11.6. Stacked generalization
1.11.7. AdaBoost
1.12. Multiclass and multioutput algorithms1.12.1. Multiclass classification1.12.2. Multilabel classification1.12.3. Multiclass-multioutput classification1.12.4. Multioutput regression
1.12.1. Multiclass classification
1.12.2. Multilabel classification
1.12.3. Multiclass-multioutput classification
1.12.4. Multioutput regression
1.13. Feature selection1.13.1. Removing features with low variance1.13.2. Univariate feature selection1.13.3. Recursive feature elimination1.13.4. Feature selection using SelectFromModel1.13.5. Sequential Feature Selection1.13.6. Feature selection as part of a pipeline
1.13.1. Removing features with low variance
1.13.2. Univariate feature selection
1.13.3. Recursive feature elimination
1.13.4. Feature selection using SelectFromModel
1.13.5. Sequential Feature Selection
1.13.6. Feature selection as part of a pipeline
1.14. Semi-supervised learning1.14.1. Self Training1.14.2. Label Propagation
1.14.1. Self Training
1.14.2. Label Propagation
1.15. Isotonic regression
1.16. Probability calibration1.16.1. Calibration curves1.16.2. Calibrating a classifier1.16.3. Usage
1.16.1. Calibration curves
1.16.2. Calibrating a classifier
1.16.3. Usage
1.17. Neural network models (supervised)1.17.1. Multi-layer Perceptron1.17.2. Classification1.17.3. Regression1.17.4. Regularization1.17.5. Algorithms1.17.6. Complexity1.17.7. Tips on Practical Use1.17.8. More control with warm_start
1.17.1. Multi-layer Perceptron
1.17.2. Classification
1.17.3. Regression
1.17.4. Regularization
1.17.5. Algorithms
1.17.6. Complexity
1.17.7. Tips on Practical Use
1.17.8. More control with warm_start
2. Unsupervised learning2.1. Gaussian mixture models2.1.1. Gaussian Mixture2.1.2. Variational Bayesian Gaussian Mixture2.2. Manifold learning2.2.1. Introduction2.2.2. Isomap2.2.3. Locally Linear Embedding2.2.4. Modified Locally Linear Embedding2.2.5. Hessian Eigenmapping2.2.6. Spectral Embedding2.2.7. Local Tangent Space Alignment2.2.8. Multi-dimensional Scaling (MDS)2.2.9. t-distributed Stochastic Neighbor Embedding (t-SNE)2.2.10. Tips on practical use2.3. Clustering2.3.1. Overview of clustering methods2.3.2. K-means2.3.3. Affinity Propagation2.3.4. Mean Shift2.3.5. Spectral clustering2.3.6. Hierarchical clustering2.3.7. DBSCAN2.3.8. HDBSCAN2.3.9. OPTICS2.3.10. BIRCH2.3.11. Clustering performance evaluation2.4. Biclustering2.4.1. Spectral Co-Clustering2.4.2. Spectral Biclustering2.4.3. Biclustering evaluation2.5. Decomposing signals in components (matrix factorization problems)2.5.1. Principal component analysis (PCA)2.5.2. Kernel Principal Component Analysis (kPCA)2.5.3. Truncated singular value decomposition and latent semantic analysis2.5.4. Dictionary Learning2.5.5. Factor Analysis2.5.6. Independent component analysis (ICA)2.5.7. Non-negative matrix factorization (NMF or NNMF)2.5.8. Latent Dirichlet Allocation (LDA)2.6. Covariance estimation2.6.1. Empirical covariance2.6.2. Shrunk Covariance2.6.3. Sparse inverse covariance2.6.4. Robust Covariance Estimation2.7. Novelty and Outlier Detection2.7.1. Overview of outlier detection methods2.7.2. Novelty Detection2.7.3. Outlier Detection2.7.4. Novelty detection with Local Outlier Factor2.8. Density Estimation2.8.1. Density Estimation: Histograms2.8.2. Kernel Density Estimation2.9. Neural network models (unsupervised)2.9.1. Restricted Boltzmann machines
2.1. Gaussian mixture models2.1.1. Gaussian Mixture2.1.2. Variational Bayesian Gaussian Mixture
2.1.1. Gaussian Mixture
2.1.2. Variational Bayesian Gaussian Mixture
2.2. Manifold learning2.2.1. Introduction2.2.2. Isomap2.2.3. Locally Linear Embedding2.2.4. Modified Locally Linear Embedding2.2.5. Hessian Eigenmapping2.2.6. Spectral Embedding2.2.7. Local Tangent Space Alignment2.2.8. Multi-dimensional Scaling (MDS)2.2.9. t-distributed Stochastic Neighbor Embedding (t-SNE)2.2.10. Tips on practical use
2.2.1. Introduction
2.2.2. Isomap
2.2.3. Locally Linear Embedding
2.2.4. Modified Locally Linear Embedding
2.2.5. Hessian Eigenmapping
2.2.6. Spectral Embedding
2.2.7. Local Tangent Space Alignment
2.2.8. Multi-dimensional Scaling (MDS)
2.2.9. t-distributed Stochastic Neighbor Embedding (t-SNE)
2.2.10. Tips on practical use
2.3. Clustering2.3.1. Overview of clustering methods2.3.2. K-means2.3.3. Affinity Propagation2.3.4. Mean Shift2.3.5. Spectral clustering2.3.6. Hierarchical clustering2.3.7. DBSCAN2.3.8. HDBSCAN2.3.9. OPTICS2.3.10. BIRCH2.3.11. Clustering performance evaluation
2.3.1. Overview of clustering methods
2.3.2. K-means
2.3.3. Affinity Propagation
2.3.4. Mean Shift
2.3.5. Spectral clustering
2.3.6. Hierarchical clustering
2.3.7. DBSCAN
2.3.8. HDBSCAN
2.3.9. OPTICS
2.3.10. BIRCH
2.3.11. Clustering performance evaluation
2.4. Biclustering2.4.1. Spectral Co-Clustering2.4.2. Spectral Biclustering2.4.3. Biclustering evaluation
2.4.1. Spectral Co-Clustering
2.4.2. Spectral Biclustering
2.4.3. Biclustering evaluation
2.5. Decomposing signals in components (matrix factorization problems)2.5.1. Principal component analysis (PCA)2.5.2. Kernel Principal Component Analysis (kPCA)2.5.3. Truncated singular value decomposition and latent semantic analysis2.5.4. Dictionary Learning2.5.5. Factor Analysis2.5.6. Independent component analysis (ICA)2.5.7. Non-negative matrix factorization (NMF or NNMF)2.5.8. Latent Dirichlet Allocation (LDA)
2.5.1. Principal component analysis (PCA)
2.5.2. Kernel Principal Component Analysis (kPCA)
2.5.3. Truncated singular value decomposition and latent semantic analysis
2.5.4. Dictionary Learning
2.5.5. Factor Analysis
2.5.6. Independent component analysis (ICA)
2.5.7. Non-negative matrix factorization (NMF or NNMF)
2.5.8. Latent Dirichlet Allocation (LDA)
2.6. Covariance estimation2.6.1. Empirical covariance2.6.2. Shrunk Covariance2.6.3. Sparse inverse covariance2.6.4. Robust Covariance Estimation
2.6.1. Empirical covariance
2.6.2. Shrunk Covariance
2.6.3. Sparse inverse covariance
2.6.4. Robust Covariance Estimation
2.7. Novelty and Outlier Detection2.7.1. Overview of outlier detection methods2.7.2. Novelty Detection2.7.3. Outlier Detection2.7.4. Novelty detection with Local Outlier Factor
2.7.1. Overview of outlier detection methods
2.7.2. Novelty Detection
2.7.3. Outlier Detection
2.7.4. Novelty detection with Local Outlier Factor
2.8. Density Estimation2.8.1. Density Estimation: Histograms2.8.2. Kernel Density Estimation
2.8.1. Density Estimation: Histograms
2.8.2. Kernel Density Estimation
2.9. Neural network models (unsupervised)2.9.1. Restricted Boltzmann machines
2.9.1. Restricted Boltzmann machines
3. Model selection and evaluation3.1. Cross-validation: evaluating estimator performance3.1.1. Computing cross-validated metrics3.1.2. Cross validation iterators3.1.3. A note on shuffling3.1.4. Cross validation and model selection3.1.5. Permutation test score3.2. Tuning the hyper-parameters of an estimator3.2.1. Exhaustive Grid Search3.2.2. Randomized Parameter Optimization3.2.3. Searching for optimal parameters with successive halving3.2.4. Tips for parameter search3.2.5. Alternatives to brute force parameter search3.3. Tuning the decision threshold for class prediction3.3.1. Post-tuning the decision threshold3.4. Metrics and scoring: quantifying the quality of predictions3.4.1. Which scoring function should I use?3.4.2. Scoring API overview3.4.3. Thescoringparameter: defining model evaluation rules3.4.4. Classification metrics3.4.5. Multilabel ranking metrics3.4.6. Regression metrics3.4.7. Clustering metrics3.4.8. Dummy estimators3.5. Validation curves: plotting scores to evaluate models3.5.1. Validation curve3.5.2. Learning curve
3.1. Cross-validation: evaluating estimator performance3.1.1. Computing cross-validated metrics3.1.2. Cross validation iterators3.1.3. A note on shuffling3.1.4. Cross validation and model selection3.1.5. Permutation test score
3.1.1. Computing cross-validated metrics
3.1.2. Cross validation iterators
3.1.3. A note on shuffling
3.1.4. Cross validation and model selection
3.1.5. Permutation test score
3.2. Tuning the hyper-parameters of an estimator3.2.1. Exhaustive Grid Search3.2.2. Randomized Parameter Optimization3.2.3. Searching for optimal parameters with successive halving3.2.4. Tips for parameter search3.2.5. Alternatives to brute force parameter search
3.2.1. Exhaustive Grid Search
3.2.2. Randomized Parameter Optimization
3.2.3. Searching for optimal parameters with successive halving
3.2.4. Tips for parameter search
3.2.5. Alternatives to brute force parameter search
3.3. Tuning the decision threshold for class prediction3.3.1. Post-tuning the decision threshold
3.3.1. Post-tuning the decision threshold
3.4. Metrics and scoring: quantifying the quality of predictions3.4.1. Which scoring function should I use?3.4.2. Scoring API overview3.4.3. Thescoringparameter: defining model evaluation rules3.4.4. Classification metrics3.4.5. Multilabel ranking metrics3.4.6. Regression metrics3.4.7. Clustering metrics3.4.8. Dummy estimators
3.4.1. Which scoring function should I use?
3.4.2. Scoring API overview
3.4.3. Thescoringparameter: defining model evaluation rules
scoring
3.4.4. Classification metrics
3.4.5. Multilabel ranking metrics
3.4.6. Regression metrics
3.4.7. Clustering metrics
3.4.8. Dummy estimators
3.5. Validation curves: plotting scores to evaluate models3.5.1. Validation curve3.5.2. Learning curve
3.5.1. Validation curve
3.5.2. Learning curve
4. Inspection4.1. Partial Dependence and Individual Conditional Expectation plots4.1.1. Partial dependence plots4.1.2. Individual conditional expectation (ICE) plot4.1.3. Mathematical Definition4.1.4. Computation methods4.2. Permutation feature importance4.2.1. Outline of the permutation importance algorithm4.2.2. Relation to impurity-based importance in trees4.2.3. Misleading values on strongly correlated features
4.1. Partial Dependence and Individual Conditional Expectation plots4.1.1. Partial dependence plots4.1.2. Individual conditional expectation (ICE) plot4.1.3. Mathematical Definition4.1.4. Computation methods
4.1.1. Partial dependence plots
4.1.2. Individual conditional expectation (ICE) plot
4.1.3. Mathematical Definition
4.1.4. Computation methods
4.2. Permutation feature importance4.2.1. Outline of the permutation importance algorithm4.2.2. Relation to impurity-based importance in trees4.2.3. Misleading values on strongly correlated features
4.2.1. Outline of the permutation importance algorithm
4.2.2. Relation to impurity-based importance in trees
4.2.3. Misleading values on strongly correlated features
5. Visualizations5.1. Available Plotting Utilities5.1.1. Display Objects
5.1. Available Plotting Utilities5.1.1. Display Objects
5.1.1. Display Objects
6. Dataset transformations6.1. Pipelines and composite estimators6.1.1. Pipeline: chaining estimators6.1.2. Transforming target in regression6.1.3. FeatureUnion: composite feature spaces6.1.4. ColumnTransformer for heterogeneous data6.1.5. Visualizing Composite Estimators6.2. Feature extraction6.2.1. Loading features from dicts6.2.2. Feature hashing6.2.3. Text feature extraction6.2.4. Image feature extraction6.3. Preprocessing data6.3.1. Standardization, or mean removal and variance scaling6.3.2. Non-linear transformation6.3.3. Normalization6.3.4. Encoding categorical features6.3.5. Discretization6.3.6. Imputation of missing values6.3.7. Generating polynomial features6.3.8. Custom transformers6.4. Imputation of missing values6.4.1. Univariate vs. Multivariate Imputation6.4.2. Univariate feature imputation6.4.3. Multivariate feature imputation6.4.4. Nearest neighbors imputation6.4.5. Keeping the number of features constant6.4.6. Marking imputed values6.4.7. Estimators that handle NaN values6.5. Unsupervised dimensionality reduction6.5.1. PCA: principal component analysis6.5.2. Random projections6.5.3. Feature agglomeration6.6. Random Projection6.6.1. The Johnson-Lindenstrauss lemma6.6.2. Gaussian random projection6.6.3. Sparse random projection6.6.4. Inverse Transform6.7. Kernel Approximation6.7.1. Nystroem Method for Kernel Approximation6.7.2. Radial Basis Function Kernel6.7.3. Additive Chi Squared Kernel6.7.4. Skewed Chi Squared Kernel6.7.5. Polynomial Kernel Approximation via Tensor Sketch6.7.6. Mathematical Details6.8. Pairwise metrics, Affinities and Kernels6.8.1. Cosine similarity6.8.2. Linear kernel6.8.3. Polynomial kernel6.8.4. Sigmoid kernel6.8.5. RBF kernel6.8.6. Laplacian kernel6.8.7. Chi-squared kernel6.9. Transforming the prediction target (y)6.9.1. Label binarization6.9.2. Label encoding
6.1. Pipelines and composite estimators6.1.1. Pipeline: chaining estimators6.1.2. Transforming target in regression6.1.3. FeatureUnion: composite feature spaces6.1.4. ColumnTransformer for heterogeneous data6.1.5. Visualizing Composite Estimators
6.1.1. Pipeline: chaining estimators
6.1.2. Transforming target in regression
6.1.3. FeatureUnion: composite feature spaces
6.1.4. ColumnTransformer for heterogeneous data
6.1.5. Visualizing Composite Estimators
6.2. Feature extraction6.2.1. Loading features from dicts6.2.2. Feature hashing6.2.3. Text feature extraction6.2.4. Image feature extraction
6.2.1. Loading features from dicts
6.2.2. Feature hashing
6.2.3. Text feature extraction
6.2.4. Image feature extraction
6.3. Preprocessing data6.3.1. Standardization, or mean removal and variance scaling6.3.2. Non-linear transformation6.3.3. Normalization6.3.4. Encoding categorical features6.3.5. Discretization6.3.6. Imputation of missing values6.3.7. Generating polynomial features6.3.8. Custom transformers
6.3.1. Standardization, or mean removal and variance scaling
6.3.2. Non-linear transformation
6.3.3. Normalization
6.3.4. Encoding categorical features
6.3.5. Discretization
6.3.6. Imputation of missing values
6.3.7. Generating polynomial features
6.3.8. Custom transformers
6.4. Imputation of missing values6.4.1. Univariate vs. Multivariate Imputation6.4.2. Univariate feature imputation6.4.3. Multivariate feature imputation6.4.4. Nearest neighbors imputation6.4.5. Keeping the number of features constant6.4.6. Marking imputed values6.4.7. Estimators that handle NaN values
6.4.1. Univariate vs. Multivariate Imputation
6.4.2. Univariate feature imputation
6.4.3. Multivariate feature imputation
6.4.4. Nearest neighbors imputation
6.4.5. Keeping the number of features constant
6.4.6. Marking imputed values
6.4.7. Estimators that handle NaN values
6.5. Unsupervised dimensionality reduction6.5.1. PCA: principal component analysis6.5.2. Random projections6.5.3. Feature agglomeration
6.5.1. PCA: principal component analysis
6.5.2. Random projections
6.5.3. Feature agglomeration
6.6. Random Projection6.6.1. The Johnson-Lindenstrauss lemma6.6.2. Gaussian random projection6.6.3. Sparse random projection6.6.4. Inverse Transform
6.6.1. The Johnson-Lindenstrauss lemma
6.6.2. Gaussian random projection
6.6.3. Sparse random projection
6.6.4. Inverse Transform
6.7. Kernel Approximation6.7.1. Nystroem Method for Kernel Approximation6.7.2. Radial Basis Function Kernel6.7.3. Additive Chi Squared Kernel6.7.4. Skewed Chi Squared Kernel6.7.5. Polynomial Kernel Approximation via Tensor Sketch6.7.6. Mathematical Details
6.7.1. Nystroem Method for Kernel Approximation
6.7.2. Radial Basis Function Kernel
6.7.3. Additive Chi Squared Kernel
6.7.4. Skewed Chi Squared Kernel
6.7.5. Polynomial Kernel Approximation via Tensor Sketch
6.7.6. Mathematical Details
6.8. Pairwise metrics, Affinities and Kernels6.8.1. Cosine similarity6.8.2. Linear kernel6.8.3. Polynomial kernel6.8.4. Sigmoid kernel6.8.5. RBF kernel6.8.6. Laplacian kernel6.8.7. Chi-squared kernel
6.8.1. Cosine similarity
6.8.2. Linear kernel
6.8.3. Polynomial kernel
6.8.4. Sigmoid kernel
6.8.5. RBF kernel
6.8.6. Laplacian kernel
6.8.7. Chi-squared kernel
6.9. Transforming the prediction target (y)6.9.1. Label binarization6.9.2. Label encoding
y
6.9.1. Label binarization
6.9.2. Label encoding
7. Dataset loading utilities7.1. Toy datasets7.1.1. Iris plants dataset7.1.2. Diabetes dataset7.1.3. Optical recognition of handwritten digits dataset7.1.4. Linnerrud dataset7.1.5. Wine recognition dataset7.1.6. Breast cancer wisconsin (diagnostic) dataset7.2. Real world datasets7.2.1. The Olivetti faces dataset7.2.2. The 20 newsgroups text dataset7.2.3. The Labeled Faces in the Wild face recognition dataset7.2.4. Forest covertypes7.2.5. RCV1 dataset7.2.6. Kddcup 99 dataset7.2.7. California Housing dataset7.2.8. Species distribution dataset7.3. Generated datasets7.3.1. Generators for classification and clustering7.3.2. Generators for regression7.3.3. Generators for manifold learning7.3.4. Generators for decomposition7.4. Loading other datasets7.4.1. Sample images7.4.2. Datasets in svmlight / libsvm format7.4.3. Downloading datasets from the openml.org repository7.4.4. Loading from external datasets
7.1. Toy datasets7.1.1. Iris plants dataset7.1.2. Diabetes dataset7.1.3. Optical recognition of handwritten digits dataset7.1.4. Linnerrud dataset7.1.5. Wine recognition dataset7.1.6. Breast cancer wisconsin (diagnostic) dataset
7.1.1. Iris plants dataset
7.1.2. Diabetes dataset
7.1.3. Optical recognition of handwritten digits dataset
7.1.4. Linnerrud dataset
7.1.5. Wine recognition dataset
7.1.6. Breast cancer wisconsin (diagnostic) dataset
7.2. Real world datasets7.2.1. The Olivetti faces dataset7.2.2. The 20 newsgroups text dataset7.2.3. The Labeled Faces in the Wild face recognition dataset7.2.4. Forest covertypes7.2.5. RCV1 dataset7.2.6. Kddcup 99 dataset7.2.7. California Housing dataset7.2.8. Species distribution dataset
7.2.1. The Olivetti faces dataset
7.2.2. The 20 newsgroups text dataset
7.2.3. The Labeled Faces in the Wild face recognition dataset
7.2.4. Forest covertypes
7.2.5. RCV1 dataset
7.2.6. Kddcup 99 dataset
7.2.7. California Housing dataset
7.2.8. Species distribution dataset
7.3. Generated datasets7.3.1. Generators for classification and clustering7.3.2. Generators for regression7.3.3. Generators for manifold learning7.3.4. Generators for decomposition
7.3.1. Generators for classification and clustering
7.3.2. Generators for regression
7.3.3. Generators for manifold learning
7.3.4. Generators for decomposition
7.4. Loading other datasets7.4.1. Sample images7.4.2. Datasets in svmlight / libsvm format7.4.3. Downloading datasets from the openml.org repository7.4.4. Loading from external datasets
7.4.1. Sample images
7.4.2. Datasets in svmlight / libsvm format
7.4.3. Downloading datasets from the openml.org repository
7.4.4. Loading from external datasets
8. Computing with scikit-learn8.1. Strategies to scale computationally: bigger data8.1.1. Scaling with instances using out-of-core learning8.2. Computational Performance8.2.1. Prediction Latency8.2.2. Prediction Throughput8.2.3. Tips and Tricks8.3. Parallelism, resource management, and configuration8.3.1. Parallelism8.3.2. Configuration switches
8.1. Strategies to scale computationally: bigger data8.1.1. Scaling with instances using out-of-core learning
8.1.1. Scaling with instances using out-of-core learning
8.2. Computational Performance8.2.1. Prediction Latency8.2.2. Prediction Throughput8.2.3. Tips and Tricks
8.2.1. Prediction Latency
8.2.2. Prediction Throughput
8.2.3. Tips and Tricks
8.3. Parallelism, resource management, and configuration8.3.1. Parallelism8.3.2. Configuration switches
8.3.1. Parallelism
8.3.2. Configuration switches
9. Model persistence9.1. Workflow Overview9.1.1. Train and Persist the Model9.2. ONNX9.3.skops.io9.4.pickle,joblib, andcloudpickle9.5. Security & Maintainability Limitations9.5.1. Replicating the training environment in production9.5.2. Serving the model artifact9.6. Summarizing the key points
9.1. Workflow Overview9.1.1. Train and Persist the Model
9.1.1. Train and Persist the Model
9.2. ONNX
9.3.skops.io
skops.io
9.4.pickle,joblib, andcloudpickle
pickle
joblib
cloudpickle
9.5. Security & Maintainability Limitations9.5.1. Replicating the training environment in production9.5.2. Serving the model artifact
9.5.1. Replicating the training environment in production
9.5.2. Serving the model artifact
9.6. Summarizing the key points
10. Common pitfalls and recommended practices10.1. Inconsistent preprocessing10.2. Data leakage10.2.1. How to avoid data leakage10.2.2. Data leakage during pre-processing10.3. Controlling randomness10.3.1. UsingNoneorRandomStateinstances, and repeated calls tofitandsplit10.3.2. Common pitfalls and subtleties10.3.3. General recommendations
10.1. Inconsistent preprocessing
10.2. Data leakage10.2.1. How to avoid data leakage10.2.2. Data leakage during pre-processing
10.2.1. How to avoid data leakage
10.2.2. Data leakage during pre-processing
10.3. Controlling randomness10.3.1. UsingNoneorRandomStateinstances, and repeated calls tofitandsplit10.3.2. Common pitfalls and subtleties10.3.3. General recommendations
10.3.1. UsingNoneorRandomStateinstances, and repeated calls tofitandsplit
None
RandomState
fit
split
10.3.2. Common pitfalls and subtleties
10.3.3. General recommendations
11. Dispatching11.1. Array API support (experimental)11.1.1. Example usage11.1.2. Support forArrayAPI-compatible inputs11.1.3. Common estimator checks
11.1. Array API support (experimental)11.1.1. Example usage11.1.2. Support forArrayAPI-compatible inputs11.1.3. Common estimator checks
11.1.1. Example usage
11.1.2. Support forArrayAPI-compatible inputs
ArrayAPI
11.1.3. Common estimator checks
12. Choosing the right estimator
13. External Resources, Videos and Talks13.1. New to Scientific Python?13.2. External Tutorials13.3. Videos
13.1. New to Scientific Python?
13.2. External Tutorials
13.3. Videos
Under Development#
1. Metadata Routing
This Page
Show Source

GitHub
GitHub
User Guide#
1. Supervised learning1.1. Linear Models1.1.1. Ordinary Least Squares1.1.2. Ridge regression and classification1.1.3. Lasso1.1.4. Multi-task Lasso1.1.5. Elastic-Net1.1.6. Multi-task Elastic-Net1.1.7. Least Angle Regression1.1.8. LARS Lasso1.1.9. Orthogonal Matching Pursuit (OMP)1.1.10. Bayesian Regression1.1.11. Logistic regression1.1.12. Generalized Linear Models1.1.13. Stochastic Gradient Descent - SGD1.1.14. Perceptron1.1.15. Passive Aggressive Algorithms1.1.16. Robustness regression: outliers and modeling errors1.1.17. Quantile Regression1.1.18. Polynomial regression: extending linear models with basis functions1.2. Linear and Quadratic Discriminant Analysis1.2.1. Dimensionality reduction using Linear Discriminant Analysis1.2.2. Mathematical formulation of the LDA and QDA classifiers1.2.3. Mathematical formulation of LDA dimensionality reduction1.2.4. Shrinkage and Covariance Estimator1.2.5. Estimation algorithms1.3. Kernel ridge regression1.4. Support Vector Machines1.4.1. Classification1.4.2. Regression1.4.3. Density estimation, novelty detection1.4.4. Complexity1.4.5. Tips on Practical Use1.4.6. Kernel functions1.4.7. Mathematical formulation1.4.8. Implementation details1.5. Stochastic Gradient Descent1.5.1. Classification1.5.2. Regression1.5.3. Online One-Class SVM1.5.4. Stochastic Gradient Descent for sparse data1.5.5. Complexity1.5.6. Stopping criterion1.5.7. Tips on Practical Use1.5.8. Mathematical formulation1.5.9. Implementation details1.6. Nearest Neighbors1.6.1. Unsupervised Nearest Neighbors1.6.2. Nearest Neighbors Classification1.6.3. Nearest Neighbors Regression1.6.4. Nearest Neighbor Algorithms1.6.5. Nearest Centroid Classifier1.6.6. Nearest Neighbors Transformer1.6.7. Neighborhood Components Analysis1.7. Gaussian Processes1.7.1. Gaussian Process Regression (GPR)1.7.2. Gaussian Process Classification (GPC)1.7.3. GPC examples1.7.4. Kernels for Gaussian Processes1.8. Cross decomposition1.8.1. PLSCanonical1.8.2. PLSSVD1.8.3. PLSRegression1.8.4. Canonical Correlation Analysis1.9. Naive Bayes1.9.1. Gaussian Naive Bayes1.9.2. Multinomial Naive Bayes1.9.3. Complement Naive Bayes1.9.4. Bernoulli Naive Bayes1.9.5. Categorical Naive Bayes1.9.6. Out-of-core naive Bayes model fitting1.10. Decision Trees1.10.1. Classification1.10.2. Regression1.10.3. Multi-output problems1.10.4. Complexity1.10.5. Tips on practical use1.10.6. Tree algorithms: ID3, C4.5, C5.0 and CART1.10.7. Mathematical formulation1.10.8. Missing Values Support1.10.9. Minimal Cost-Complexity Pruning1.11. Ensembles: Gradient boosting, random forests, bagging, voting, stacking1.11.1. Gradient-boosted trees1.11.2. Random forests and other randomized tree ensembles1.11.3. Bagging meta-estimator1.11.4. Voting Classifier1.11.5. Voting Regressor1.11.6. Stacked generalization1.11.7. AdaBoost1.12. Multiclass and multioutput algorithms1.12.1. Multiclass classification1.12.2. Multilabel classification1.12.3. Multiclass-multioutput classification1.12.4. Multioutput regression1.13. Feature selection1.13.1. Removing features with low variance1.13.2. Univariate feature selection1.13.3. Recursive feature elimination1.13.4. Feature selection using SelectFromModel1.13.5. Sequential Feature Selection1.13.6. Feature selection as part of a pipeline1.14. Semi-supervised learning1.14.1. Self Training1.14.2. Label Propagation1.15. Isotonic regression1.16. Probability calibration1.16.1. Calibration curves1.16.2. Calibrating a classifier1.16.3. Usage1.17. Neural network models (supervised)1.17.1. Multi-layer Perceptron1.17.2. Classification1.17.3. Regression1.17.4. Regularization1.17.5. Algorithms1.17.6. Complexity1.17.7. Tips on Practical Use1.17.8. More control with warm_start
1.1. Linear Models1.1.1. Ordinary Least Squares1.1.2. Ridge regression and classification1.1.3. Lasso1.1.4. Multi-task Lasso1.1.5. Elastic-Net1.1.6. Multi-task Elastic-Net1.1.7. Least Angle Regression1.1.8. LARS Lasso1.1.9. Orthogonal Matching Pursuit (OMP)1.1.10. Bayesian Regression1.1.11. Logistic regression1.1.12. Generalized Linear Models1.1.13. Stochastic Gradient Descent - SGD1.1.14. Perceptron1.1.15. Passive Aggressive Algorithms1.1.16. Robustness regression: outliers and modeling errors1.1.17. Quantile Regression1.1.18. Polynomial regression: extending linear models with basis functions
1.1.1. Ordinary Least Squares
1.1.2. Ridge regression and classification
1.1.3. Lasso
1.1.4. Multi-task Lasso
1.1.5. Elastic-Net
1.1.6. Multi-task Elastic-Net
1.1.7. Least Angle Regression
1.1.8. LARS Lasso
1.1.9. Orthogonal Matching Pursuit (OMP)
1.1.10. Bayesian Regression
1.1.11. Logistic regression
1.1.12. Generalized Linear Models
1.1.13. Stochastic Gradient Descent - SGD
1.1.14. Perceptron
1.1.15. Passive Aggressive Algorithms
1.1.16. Robustness regression: outliers and modeling errors
1.1.17. Quantile Regression
1.1.18. Polynomial regression: extending linear models with basis functions
1.2. Linear and Quadratic Discriminant Analysis1.2.1. Dimensionality reduction using Linear Discriminant Analysis1.2.2. Mathematical formulation of the LDA and QDA classifiers1.2.3. Mathematical formulation of LDA dimensionality reduction1.2.4. Shrinkage and Covariance Estimator1.2.5. Estimation algorithms
1.2.1. Dimensionality reduction using Linear Discriminant Analysis
1.2.2. Mathematical formulation of the LDA and QDA classifiers
1.2.3. Mathematical formulation of LDA dimensionality reduction
1.2.4. Shrinkage and Covariance Estimator
1.2.5. Estimation algorithms
1.3. Kernel ridge regression
1.4. Support Vector Machines1.4.1. Classification1.4.2. Regression1.4.3. Density estimation, novelty detection1.4.4. Complexity1.4.5. Tips on Practical Use1.4.6. Kernel functions1.4.7. Mathematical formulation1.4.8. Implementation details
1.4.1. Classification
1.4.2. Regression
1.4.3. Density estimation, novelty detection
1.4.4. Complexity
1.4.5. Tips on Practical Use
1.4.6. Kernel functions
1.4.7. Mathematical formulation
1.4.8. Implementation details
1.5. Stochastic Gradient Descent1.5.1. Classification1.5.2. Regression1.5.3. Online One-Class SVM1.5.4. Stochastic Gradient Descent for sparse data1.5.5. Complexity1.5.6. Stopping criterion1.5.7. Tips on Practical Use1.5.8. Mathematical formulation1.5.9. Implementation details
1.5.1. Classification
1.5.2. Regression
1.5.3. Online One-Class SVM
1.5.4. Stochastic Gradient Descent for sparse data
1.5.5. Complexity
1.5.6. Stopping criterion
1.5.7. Tips on Practical Use
1.5.8. Mathematical formulation
1.5.9. Implementation details
1.6. Nearest Neighbors1.6.1. Unsupervised Nearest Neighbors1.6.2. Nearest Neighbors Classification1.6.3. Nearest Neighbors Regression1.6.4. Nearest Neighbor Algorithms1.6.5. Nearest Centroid Classifier1.6.6. Nearest Neighbors Transformer1.6.7. Neighborhood Components Analysis
1.6.1. Unsupervised Nearest Neighbors
1.6.2. Nearest Neighbors Classification
1.6.3. Nearest Neighbors Regression
1.6.4. Nearest Neighbor Algorithms
1.6.5. Nearest Centroid Classifier
1.6.6. Nearest Neighbors Transformer
1.6.7. Neighborhood Components Analysis
1.7. Gaussian Processes1.7.1. Gaussian Process Regression (GPR)1.7.2. Gaussian Process Classification (GPC)1.7.3. GPC examples1.7.4. Kernels for Gaussian Processes
1.7.1. Gaussian Process Regression (GPR)
1.7.2. Gaussian Process Classification (GPC)
1.7.3. GPC examples
1.7.4. Kernels for Gaussian Processes
1.8. Cross decomposition1.8.1. PLSCanonical1.8.2. PLSSVD1.8.3. PLSRegression1.8.4. Canonical Correlation Analysis
1.8.1. PLSCanonical
1.8.2. PLSSVD
1.8.3. PLSRegression
1.8.4. Canonical Correlation Analysis
1.9. Naive Bayes1.9.1. Gaussian Naive Bayes1.9.2. Multinomial Naive Bayes1.9.3. Complement Naive Bayes1.9.4. Bernoulli Naive Bayes1.9.5. Categorical Naive Bayes1.9.6. Out-of-core naive Bayes model fitting
1.9.1. Gaussian Naive Bayes
1.9.2. Multinomial Naive Bayes
1.9.3. Complement Naive Bayes
1.9.4. Bernoulli Naive Bayes
1.9.5. Categorical Naive Bayes
1.9.6. Out-of-core naive Bayes model fitting
1.10. Decision Trees1.10.1. Classification1.10.2. Regression1.10.3. Multi-output problems1.10.4. Complexity1.10.5. Tips on practical use1.10.6. Tree algorithms: ID3, C4.5, C5.0 and CART1.10.7. Mathematical formulation1.10.8. Missing Values Support1.10.9. Minimal Cost-Complexity Pruning
1.10.1. Classification
1.10.2. Regression
1.10.3. Multi-output problems
1.10.4. Complexity
1.10.5. Tips on practical use
1.10.6. Tree algorithms: ID3, C4.5, C5.0 and CART
1.10.7. Mathematical formulation
1.10.8. Missing Values Support
1.10.9. Minimal Cost-Complexity Pruning
1.11. Ensembles: Gradient boosting, random forests, bagging, voting, stacking1.11.1. Gradient-boosted trees1.11.2. Random forests and other randomized tree ensembles1.11.3. Bagging meta-estimator1.11.4. Voting Classifier1.11.5. Voting Regressor1.11.6. Stacked generalization1.11.7. AdaBoost
1.11.1. Gradient-boosted trees
1.11.2. Random forests and other randomized tree ensembles
1.11.3. Bagging meta-estimator
1.11.4. Voting Classifier
1.11.5. Voting Regressor
1.11.6. Stacked generalization
1.11.7. AdaBoost
1.12. Multiclass and multioutput algorithms1.12.1. Multiclass classification1.12.2. Multilabel classification1.12.3. Multiclass-multioutput classification1.12.4. Multioutput regression
1.12.1. Multiclass classification
1.12.2. Multilabel classification
1.12.3. Multiclass-multioutput classification
1.12.4. Multioutput regression
1.13. Feature selection1.13.1. Removing features with low variance1.13.2. Univariate feature selection1.13.3. Recursive feature elimination1.13.4. Feature selection using SelectFromModel1.13.5. Sequential Feature Selection1.13.6. Feature selection as part of a pipeline
1.13.1. Removing features with low variance
1.13.2. Univariate feature selection
1.13.3. Recursive feature elimination
1.13.4. Feature selection using SelectFromModel
1.13.5. Sequential Feature Selection
1.13.6. Feature selection as part of a pipeline
1.14. Semi-supervised learning1.14.1. Self Training1.14.2. Label Propagation
1.14.1. Self Training
1.14.2. Label Propagation
1.15. Isotonic regression
1.16. Probability calibration1.16.1. Calibration curves1.16.2. Calibrating a classifier1.16.3. Usage
1.16.1. Calibration curves
1.16.2. Calibrating a classifier
1.16.3. Usage
1.17. Neural network models (supervised)1.17.1. Multi-layer Perceptron1.17.2. Classification1.17.3. Regression1.17.4. Regularization1.17.5. Algorithms1.17.6. Complexity1.17.7. Tips on Practical Use1.17.8. More control with warm_start
1.17.1. Multi-layer Perceptron
1.17.2. Classification
1.17.3. Regression
1.17.4. Regularization
1.17.5. Algorithms
1.17.6. Complexity
1.17.7. Tips on Practical Use
1.17.8. More control with warm_start
2. Unsupervised learning2.1. Gaussian mixture models2.1.1. Gaussian Mixture2.1.2. Variational Bayesian Gaussian Mixture2.2. Manifold learning2.2.1. Introduction2.2.2. Isomap2.2.3. Locally Linear Embedding2.2.4. Modified Locally Linear Embedding2.2.5. Hessian Eigenmapping2.2.6. Spectral Embedding2.2.7. Local Tangent Space Alignment2.2.8. Multi-dimensional Scaling (MDS)2.2.9. t-distributed Stochastic Neighbor Embedding (t-SNE)2.2.10. Tips on practical use2.3. Clustering2.3.1. Overview of clustering methods2.3.2. K-means2.3.3. Affinity Propagation2.3.4. Mean Shift2.3.5. Spectral clustering2.3.6. Hierarchical clustering2.3.7. DBSCAN2.3.8. HDBSCAN2.3.9. OPTICS2.3.10. BIRCH2.3.11. Clustering performance evaluation2.4. Biclustering2.4.1. Spectral Co-Clustering2.4.2. Spectral Biclustering2.4.3. Biclustering evaluation2.5. Decomposing signals in components (matrix factorization problems)2.5.1. Principal component analysis (PCA)2.5.2. Kernel Principal Component Analysis (kPCA)2.5.3. Truncated singular value decomposition and latent semantic analysis2.5.4. Dictionary Learning2.5.5. Factor Analysis2.5.6. Independent component analysis (ICA)2.5.7. Non-negative matrix factorization (NMF or NNMF)2.5.8. Latent Dirichlet Allocation (LDA)2.6. Covariance estimation2.6.1. Empirical covariance2.6.2. Shrunk Covariance2.6.3. Sparse inverse covariance2.6.4. Robust Covariance Estimation2.7. Novelty and Outlier Detection2.7.1. Overview of outlier detection methods2.7.2. Novelty Detection2.7.3. Outlier Detection2.7.4. Novelty detection with Local Outlier Factor2.8. Density Estimation2.8.1. Density Estimation: Histograms2.8.2. Kernel Density Estimation2.9. Neural network models (unsupervised)2.9.1. Restricted Boltzmann machines
2.1. Gaussian mixture models2.1.1. Gaussian Mixture2.1.2. Variational Bayesian Gaussian Mixture
2.1.1. Gaussian Mixture
2.1.2. Variational Bayesian Gaussian Mixture
2.2. Manifold learning2.2.1. Introduction2.2.2. Isomap2.2.3. Locally Linear Embedding2.2.4. Modified Locally Linear Embedding2.2.5. Hessian Eigenmapping2.2.6. Spectral Embedding2.2.7. Local Tangent Space Alignment2.2.8. Multi-dimensional Scaling (MDS)2.2.9. t-distributed Stochastic Neighbor Embedding (t-SNE)2.2.10. Tips on practical use
2.2.1. Introduction
2.2.2. Isomap
2.2.3. Locally Linear Embedding
2.2.4. Modified Locally Linear Embedding
2.2.5. Hessian Eigenmapping
2.2.6. Spectral Embedding
2.2.7. Local Tangent Space Alignment
2.2.8. Multi-dimensional Scaling (MDS)
2.2.9. t-distributed Stochastic Neighbor Embedding (t-SNE)
2.2.10. Tips on practical use
2.3. Clustering2.3.1. Overview of clustering methods2.3.2. K-means2.3.3. Affinity Propagation2.3.4. Mean Shift2.3.5. Spectral clustering2.3.6. Hierarchical clustering2.3.7. DBSCAN2.3.8. HDBSCAN2.3.9. OPTICS2.3.10. BIRCH2.3.11. Clustering performance evaluation
2.3.1. Overview of clustering methods
2.3.2. K-means
2.3.3. Affinity Propagation
2.3.4. Mean Shift
2.3.5. Spectral clustering
2.3.6. Hierarchical clustering
2.3.7. DBSCAN
2.3.8. HDBSCAN
2.3.9. OPTICS
2.3.10. BIRCH
2.3.11. Clustering performance evaluation
2.4. Biclustering2.4.1. Spectral Co-Clustering2.4.2. Spectral Biclustering2.4.3. Biclustering evaluation
2.4.1. Spectral Co-Clustering
2.4.2. Spectral Biclustering
2.4.3. Biclustering evaluation
2.5. Decomposing signals in components (matrix factorization problems)2.5.1. Principal component analysis (PCA)2.5.2. Kernel Principal Component Analysis (kPCA)2.5.3. Truncated singular value decomposition and latent semantic analysis2.5.4. Dictionary Learning2.5.5. Factor Analysis2.5.6. Independent component analysis (ICA)2.5.7. Non-negative matrix factorization (NMF or NNMF)2.5.8. Latent Dirichlet Allocation (LDA)
2.5.1. Principal component analysis (PCA)
2.5.2. Kernel Principal Component Analysis (kPCA)
2.5.3. Truncated singular value decomposition and latent semantic analysis
2.5.4. Dictionary Learning
2.5.5. Factor Analysis
2.5.6. Independent component analysis (ICA)
2.5.7. Non-negative matrix factorization (NMF or NNMF)
2.5.8. Latent Dirichlet Allocation (LDA)
2.6. Covariance estimation2.6.1. Empirical covariance2.6.2. Shrunk Covariance2.6.3. Sparse inverse covariance2.6.4. Robust Covariance Estimation
2.6.1. Empirical covariance
2.6.2. Shrunk Covariance
2.6.3. Sparse inverse covariance
2.6.4. Robust Covariance Estimation
2.7. Novelty and Outlier Detection2.7.1. Overview of outlier detection methods2.7.2. Novelty Detection2.7.3. Outlier Detection2.7.4. Novelty detection with Local Outlier Factor
2.7.1. Overview of outlier detection methods
2.7.2. Novelty Detection
2.7.3. Outlier Detection
2.7.4. Novelty detection with Local Outlier Factor
2.8. Density Estimation2.8.1. Density Estimation: Histograms2.8.2. Kernel Density Estimation
2.8.1. Density Estimation: Histograms
2.8.2. Kernel Density Estimation
2.9. Neural network models (unsupervised)2.9.1. Restricted Boltzmann machines
2.9.1. Restricted Boltzmann machines
3. Model selection and evaluation3.1. Cross-validation: evaluating estimator performance3.1.1. Computing cross-validated metrics3.1.2. Cross validation iterators3.1.3. A note on shuffling3.1.4. Cross validation and model selection3.1.5. Permutation test score3.2. Tuning the hyper-parameters of an estimator3.2.1. Exhaustive Grid Search3.2.2. Randomized Parameter Optimization3.2.3. Searching for optimal parameters with successive halving3.2.4. Tips for parameter search3.2.5. Alternatives to brute force parameter search3.3. Tuning the decision threshold for class prediction3.3.1. Post-tuning the decision threshold3.4. Metrics and scoring: quantifying the quality of predictions3.4.1. Which scoring function should I use?3.4.2. Scoring API overview3.4.3. Thescoringparameter: defining model evaluation rules3.4.4. Classification metrics3.4.5. Multilabel ranking metrics3.4.6. Regression metrics3.4.7. Clustering metrics3.4.8. Dummy estimators3.5. Validation curves: plotting scores to evaluate models3.5.1. Validation curve3.5.2. Learning curve
3.1. Cross-validation: evaluating estimator performance3.1.1. Computing cross-validated metrics3.1.2. Cross validation iterators3.1.3. A note on shuffling3.1.4. Cross validation and model selection3.1.5. Permutation test score
3.1.1. Computing cross-validated metrics
3.1.2. Cross validation iterators
3.1.3. A note on shuffling
3.1.4. Cross validation and model selection
3.1.5. Permutation test score
3.2. Tuning the hyper-parameters of an estimator3.2.1. Exhaustive Grid Search3.2.2. Randomized Parameter Optimization3.2.3. Searching for optimal parameters with successive halving3.2.4. Tips for parameter search3.2.5. Alternatives to brute force parameter search
3.2.1. Exhaustive Grid Search
3.2.2. Randomized Parameter Optimization
3.2.3. Searching for optimal parameters with successive halving
3.2.4. Tips for parameter search
3.2.5. Alternatives to brute force parameter search
3.3. Tuning the decision threshold for class prediction3.3.1. Post-tuning the decision threshold
3.3.1. Post-tuning the decision threshold
3.4. Metrics and scoring: quantifying the quality of predictions3.4.1. Which scoring function should I use?3.4.2. Scoring API overview3.4.3. Thescoringparameter: defining model evaluation rules3.4.4. Classification metrics3.4.5. Multilabel ranking metrics3.4.6. Regression metrics3.4.7. Clustering metrics3.4.8. Dummy estimators
3.4.1. Which scoring function should I use?
3.4.2. Scoring API overview
3.4.3. Thescoringparameter: defining model evaluation rules
scoring
3.4.4. Classification metrics
3.4.5. Multilabel ranking metrics
3.4.6. Regression metrics
3.4.7. Clustering metrics
3.4.8. Dummy estimators
3.5. Validation curves: plotting scores to evaluate models3.5.1. Validation curve3.5.2. Learning curve
3.5.1. Validation curve
3.5.2. Learning curve
4. Inspection4.1. Partial Dependence and Individual Conditional Expectation plots4.1.1. Partial dependence plots4.1.2. Individual conditional expectation (ICE) plot4.1.3. Mathematical Definition4.1.4. Computation methods4.2. Permutation feature importance4.2.1. Outline of the permutation importance algorithm4.2.2. Relation to impurity-based importance in trees4.2.3. Misleading values on strongly correlated features
4.1. Partial Dependence and Individual Conditional Expectation plots4.1.1. Partial dependence plots4.1.2. Individual conditional expectation (ICE) plot4.1.3. Mathematical Definition4.1.4. Computation methods
4.1.1. Partial dependence plots
4.1.2. Individual conditional expectation (ICE) plot
4.1.3. Mathematical Definition
4.1.4. Computation methods
4.2. Permutation feature importance4.2.1. Outline of the permutation importance algorithm4.2.2. Relation to impurity-based importance in trees4.2.3. Misleading values on strongly correlated features
4.2.1. Outline of the permutation importance algorithm
4.2.2. Relation to impurity-based importance in trees
4.2.3. Misleading values on strongly correlated features
5. Visualizations5.1. Available Plotting Utilities5.1.1. Display Objects
5.1. Available Plotting Utilities5.1.1. Display Objects
5.1.1. Display Objects
6. Dataset transformations6.1. Pipelines and composite estimators6.1.1. Pipeline: chaining estimators6.1.2. Transforming target in regression6.1.3. FeatureUnion: composite feature spaces6.1.4. ColumnTransformer for heterogeneous data6.1.5. Visualizing Composite Estimators6.2. Feature extraction6.2.1. Loading features from dicts6.2.2. Feature hashing6.2.3. Text feature extraction6.2.4. Image feature extraction6.3. Preprocessing data6.3.1. Standardization, or mean removal and variance scaling6.3.2. Non-linear transformation6.3.3. Normalization6.3.4. Encoding categorical features6.3.5. Discretization6.3.6. Imputation of missing values6.3.7. Generating polynomial features6.3.8. Custom transformers6.4. Imputation of missing values6.4.1. Univariate vs. Multivariate Imputation6.4.2. Univariate feature imputation6.4.3. Multivariate feature imputation6.4.4. Nearest neighbors imputation6.4.5. Keeping the number of features constant6.4.6. Marking imputed values6.4.7. Estimators that handle NaN values6.5. Unsupervised dimensionality reduction6.5.1. PCA: principal component analysis6.5.2. Random projections6.5.3. Feature agglomeration6.6. Random Projection6.6.1. The Johnson-Lindenstrauss lemma6.6.2. Gaussian random projection6.6.3. Sparse random projection6.6.4. Inverse Transform6.7. Kernel Approximation6.7.1. Nystroem Method for Kernel Approximation6.7.2. Radial Basis Function Kernel6.7.3. Additive Chi Squared Kernel6.7.4. Skewed Chi Squared Kernel6.7.5. Polynomial Kernel Approximation via Tensor Sketch6.7.6. Mathematical Details6.8. Pairwise metrics, Affinities and Kernels6.8.1. Cosine similarity6.8.2. Linear kernel6.8.3. Polynomial kernel6.8.4. Sigmoid kernel6.8.5. RBF kernel6.8.6. Laplacian kernel6.8.7. Chi-squared kernel6.9. Transforming the prediction target (y)6.9.1. Label binarization6.9.2. Label encoding
6.1. Pipelines and composite estimators6.1.1. Pipeline: chaining estimators6.1.2. Transforming target in regression6.1.3. FeatureUnion: composite feature spaces6.1.4. ColumnTransformer for heterogeneous data6.1.5. Visualizing Composite Estimators
6.1.1. Pipeline: chaining estimators
6.1.2. Transforming target in regression
6.1.3. FeatureUnion: composite feature spaces
6.1.4. ColumnTransformer for heterogeneous data
6.1.5. Visualizing Composite Estimators
6.2. Feature extraction6.2.1. Loading features from dicts6.2.2. Feature hashing6.2.3. Text feature extraction6.2.4. Image feature extraction
6.2.1. Loading features from dicts
6.2.2. Feature hashing
6.2.3. Text feature extraction
6.2.4. Image feature extraction
6.3. Preprocessing data6.3.1. Standardization, or mean removal and variance scaling6.3.2. Non-linear transformation6.3.3. Normalization6.3.4. Encoding categorical features6.3.5. Discretization6.3.6. Imputation of missing values6.3.7. Generating polynomial features6.3.8. Custom transformers
6.3.1. Standardization, or mean removal and variance scaling
6.3.2. Non-linear transformation
6.3.3. Normalization
6.3.4. Encoding categorical features
6.3.5. Discretization
6.3.6. Imputation of missing values
6.3.7. Generating polynomial features
6.3.8. Custom transformers
6.4. Imputation of missing values6.4.1. Univariate vs. Multivariate Imputation6.4.2. Univariate feature imputation6.4.3. Multivariate feature imputation6.4.4. Nearest neighbors imputation6.4.5. Keeping the number of features constant6.4.6. Marking imputed values6.4.7. Estimators that handle NaN values
6.4.1. Univariate vs. Multivariate Imputation
6.4.2. Univariate feature imputation
6.4.3. Multivariate feature imputation
6.4.4. Nearest neighbors imputation
6.4.5. Keeping the number of features constant
6.4.6. Marking imputed values
6.4.7. Estimators that handle NaN values
6.5. Unsupervised dimensionality reduction6.5.1. PCA: principal component analysis6.5.2. Random projections6.5.3. Feature agglomeration
6.5.1. PCA: principal component analysis
6.5.2. Random projections
6.5.3. Feature agglomeration
6.6. Random Projection6.6.1. The Johnson-Lindenstrauss lemma6.6.2. Gaussian random projection6.6.3. Sparse random projection6.6.4. Inverse Transform
6.6.1. The Johnson-Lindenstrauss lemma
6.6.2. Gaussian random projection
6.6.3. Sparse random projection
6.6.4. Inverse Transform
6.7. Kernel Approximation6.7.1. Nystroem Method for Kernel Approximation6.7.2. Radial Basis Function Kernel6.7.3. Additive Chi Squared Kernel6.7.4. Skewed Chi Squared Kernel6.7.5. Polynomial Kernel Approximation via Tensor Sketch6.7.6. Mathematical Details
6.7.1. Nystroem Method for Kernel Approximation
6.7.2. Radial Basis Function Kernel
6.7.3. Additive Chi Squared Kernel
6.7.4. Skewed Chi Squared Kernel
6.7.5. Polynomial Kernel Approximation via Tensor Sketch
6.7.6. Mathematical Details
6.8. Pairwise metrics, Affinities and Kernels6.8.1. Cosine similarity6.8.2. Linear kernel6.8.3. Polynomial kernel6.8.4. Sigmoid kernel6.8.5. RBF kernel6.8.6. Laplacian kernel6.8.7. Chi-squared kernel
6.8.1. Cosine similarity
6.8.2. Linear kernel
6.8.3. Polynomial kernel
6.8.4. Sigmoid kernel
6.8.5. RBF kernel
6.8.6. Laplacian kernel
6.8.7. Chi-squared kernel
6.9. Transforming the prediction target (y)6.9.1. Label binarization6.9.2. Label encoding
y
6.9.1. Label binarization
6.9.2. Label encoding
7. Dataset loading utilities7.1. Toy datasets7.1.1. Iris plants dataset7.1.2. Diabetes dataset7.1.3. Optical recognition of handwritten digits dataset7.1.4. Linnerrud dataset7.1.5. Wine recognition dataset7.1.6. Breast cancer wisconsin (diagnostic) dataset7.2. Real world datasets7.2.1. The Olivetti faces dataset7.2.2. The 20 newsgroups text dataset7.2.3. The Labeled Faces in the Wild face recognition dataset7.2.4. Forest covertypes7.2.5. RCV1 dataset7.2.6. Kddcup 99 dataset7.2.7. California Housing dataset7.2.8. Species distribution dataset7.3. Generated datasets7.3.1. Generators for classification and clustering7.3.2. Generators for regression7.3.3. Generators for manifold learning7.3.4. Generators for decomposition7.4. Loading other datasets7.4.1. Sample images7.4.2. Datasets in svmlight / libsvm format7.4.3. Downloading datasets from the openml.org repository7.4.4. Loading from external datasets
7.1. Toy datasets7.1.1. Iris plants dataset7.1.2. Diabetes dataset7.1.3. Optical recognition of handwritten digits dataset7.1.4. Linnerrud dataset7.1.5. Wine recognition dataset7.1.6. Breast cancer wisconsin (diagnostic) dataset
7.1.1. Iris plants dataset
7.1.2. Diabetes dataset
7.1.3. Optical recognition of handwritten digits dataset
7.1.4. Linnerrud dataset
7.1.5. Wine recognition dataset
7.1.6. Breast cancer wisconsin (diagnostic) dataset
7.2. Real world datasets7.2.1. The Olivetti faces dataset7.2.2. The 20 newsgroups text dataset7.2.3. The Labeled Faces in the Wild face recognition dataset7.2.4. Forest covertypes7.2.5. RCV1 dataset7.2.6. Kddcup 99 dataset7.2.7. California Housing dataset7.2.8. Species distribution dataset
7.2.1. The Olivetti faces dataset
7.2.2. The 20 newsgroups text dataset
7.2.3. The Labeled Faces in the Wild face recognition dataset
7.2.4. Forest covertypes
7.2.5. RCV1 dataset
7.2.6. Kddcup 99 dataset
7.2.7. California Housing dataset
7.2.8. Species distribution dataset
7.3. Generated datasets7.3.1. Generators for classification and clustering7.3.2. Generators for regression7.3.3. Generators for manifold learning7.3.4. Generators for decomposition
7.3.1. Generators for classification and clustering
7.3.2. Generators for regression
7.3.3. Generators for manifold learning
7.3.4. Generators for decomposition
7.4. Loading other datasets7.4.1. Sample images7.4.2. Datasets in svmlight / libsvm format7.4.3. Downloading datasets from the openml.org repository7.4.4. Loading from external datasets
7.4.1. Sample images
7.4.2. Datasets in svmlight / libsvm format
7.4.3. Downloading datasets from the openml.org repository
7.4.4. Loading from external datasets
8. Computing with scikit-learn8.1. Strategies to scale computationally: bigger data8.1.1. Scaling with instances using out-of-core learning8.2. Computational Performance8.2.1. Prediction Latency8.2.2. Prediction Throughput8.2.3. Tips and Tricks8.3. Parallelism, resource management, and configuration8.3.1. Parallelism8.3.2. Configuration switches
8.1. Strategies to scale computationally: bigger data8.1.1. Scaling with instances using out-of-core learning
8.1.1. Scaling with instances using out-of-core learning
8.2. Computational Performance8.2.1. Prediction Latency8.2.2. Prediction Throughput8.2.3. Tips and Tricks
8.2.1. Prediction Latency
8.2.2. Prediction Throughput
8.2.3. Tips and Tricks
8.3. Parallelism, resource management, and configuration8.3.1. Parallelism8.3.2. Configuration switches
8.3.1. Parallelism
8.3.2. Configuration switches
9. Model persistence9.1. Workflow Overview9.1.1. Train and Persist the Model9.2. ONNX9.3.skops.io9.4.pickle,joblib, andcloudpickle9.5. Security & Maintainability Limitations9.5.1. Replicating the training environment in production9.5.2. Serving the model artifact9.6. Summarizing the key points
9.1. Workflow Overview9.1.1. Train and Persist the Model
9.1.1. Train and Persist the Model
9.2. ONNX
9.3.skops.io
skops.io
9.4.pickle,joblib, andcloudpickle
pickle
joblib
cloudpickle
9.5. Security & Maintainability Limitations9.5.1. Replicating the training environment in production9.5.2. Serving the model artifact
9.5.1. Replicating the training environment in production
9.5.2. Serving the model artifact
9.6. Summarizing the key points
10. Common pitfalls and recommended practices10.1. Inconsistent preprocessing10.2. Data leakage10.2.1. How to avoid data leakage10.2.2. Data leakage during pre-processing10.3. Controlling randomness10.3.1. UsingNoneorRandomStateinstances, and repeated calls tofitandsplit10.3.2. Common pitfalls and subtleties10.3.3. General recommendations
10.1. Inconsistent preprocessing
10.2. Data leakage10.2.1. How to avoid data leakage10.2.2. Data leakage during pre-processing
10.2.1. How to avoid data leakage
10.2.2. Data leakage during pre-processing
10.3. Controlling randomness10.3.1. UsingNoneorRandomStateinstances, and repeated calls tofitandsplit10.3.2. Common pitfalls and subtleties10.3.3. General recommendations
10.3.1. UsingNoneorRandomStateinstances, and repeated calls tofitandsplit
None
RandomState
fit
split
10.3.2. Common pitfalls and subtleties
10.3.3. General recommendations
11. Dispatching11.1. Array API support (experimental)11.1.1. Example usage11.1.2. Support forArrayAPI-compatible inputs11.1.3. Common estimator checks
11.1. Array API support (experimental)11.1.1. Example usage11.1.2. Support forArrayAPI-compatible inputs11.1.3. Common estimator checks
11.1.1. Example usage
11.1.2. Support forArrayAPI-compatible inputs
ArrayAPI
11.1.3. Common estimator checks
12. Choosing the right estimator
13. External Resources, Videos and Talks13.1. New to Scientific Python?13.2. External Tutorials13.3. Videos
13.1. New to Scientific Python?
13.2. External Tutorials
13.3. Videos
Under Development#
1. Metadata Routing
This Page
Show Source

GitHub
scikit-learn
Simple and efficient tools for predictive data analysis
Accessible to everybody, and reusable in various contexts
Built on NumPy, SciPy, and matplotlib
Open source, commercially usable - BSD license
GitHub
Identifying which category an object belongs to.
Applications:Spam detection, image recognition.Algorithms:Gradient boosting,nearest neighbors,random forest,logistic regression,
            andmore...
Predicting a continuous-valued attribute associated with an object.
Applications:Drug response, stock prices.Algorithms:Gradient boosting,nearest neighbors,random forest,ridge,
            andmore...
Automatic grouping of similar objects into sets.
Applications:Customer segmentation, grouping experiment outcomes.Algorithms:k-Means,HDBSCAN,hierarchical clustering,
            andmore...
Reducing the number of random variables to consider.
Applications:Visualization, increased efficiency.Algorithms:PCA,feature selection,non-negative matrix factorization,
            andmore...
Comparing, validating and choosing parameters and models.
Applications:Improved accuracy via parameter tuning.Algorithms:Grid search,cross validation,metrics,
            andmore...
Feature extraction and normalization.
Applications:Transforming input data such as text for use with machine learning algorithms.Algorithms:Preprocessing,feature extraction,
            andmore...
On-going development:scikit-learn 1.7 (Changelog).
January 2025.scikit-learn 1.6.1 is available for download (Changelog).
December 2024.scikit-learn 1.6.0 is available for download (Changelog).
September 2024.scikit-learn 1.5.2 is available for download (Changelog).
July 2024.scikit-learn 1.5.1 is available for download (Changelog).
May 2024.scikit-learn 1.5.0 is available for download (Changelog).
April 2024.scikit-learn 1.4.2 is available for download (Changelog).
February 2024.scikit-learn 1.4.1.post1 is available for download (Changelog).
January 2024.scikit-learn 1.4.0 is available for download (Changelog).
All releases:What's new(Changelog).
About us:Seepeopleandcontributing
More Machine Learning:Findrelated projects
Questions?SeeFAQ,support, andstackoverflow
Subscribe to themailing list
Blog:blog.scikit-learn.org
Logos & Branding:logos and branding
Calendar:calendar
LinkedIn:linkedin/scikit-learn
Bluesky:bluesky/scikit-learn.org
Mastodon:@sklearn
YouTube:youtube.com/scikit-learn
Facebook:@scikitlearnofficial
Instagram:@scikitlearnofficial
TikTok:@scikit.learn
Discord:@scikit-learn
Communication on all channels should respectPSF's code of conduct.
Help us,donate!Cite us!
More testimonials...
scikit-learn development and maintenance are financially supported by

GitHub
GitHub
Installing scikit-learn#
There are different ways to install scikit-learn:
Install the latest official release. This
is the best approach for most users. It will provide a stable version
and pre-built packages are available for most platforms.
Install the latest official release. This
is the best approach for most users. It will provide a stable version
and pre-built packages are available for most platforms.
Install the version of scikit-learn provided by youroperating system or Python distribution.
This is a quick option for those who have operating systems or Python
distributions that distribute scikit-learn.
It might not provide the latest release version.
Install the version of scikit-learn provided by youroperating system or Python distribution.
This is a quick option for those who have operating systems or Python
distributions that distribute scikit-learn.
It might not provide the latest release version.
Building the package from source. This is best for users who want the
latest-and-greatest features and aren’t afraid of running
brand-new code. This is also needed for users who wish to contribute to the
project.
Building the package from source. This is best for users who want the
latest-and-greatest features and aren’t afraid of running
brand-new code. This is also needed for users who wish to contribute to the
project.
Installing the latest release#
Install the 64-bit version of Python 3, for instance from theofficial website.
Now create avirtual environment (venv)and install scikit-learn.
Note that the virtual environment is optional but strongly recommended, in
order to avoid potential conflicts with other packages.
python-mvenvsklearn-envsklearn-env\Scripts\activate# activatepipinstall-Uscikit-learn
In order to check your installation, you can use:
python-mpipshowscikit-learn# show scikit-learn version and locationpython-mpipfreeze# show all installed packages in the environmentpython-c"import sklearn; sklearn.show_versions()"
Install conda using theconda-forge installers(no
administrator permission required). Then run:
condacreate-nsklearn-env-cconda-forgescikit-learncondaactivatesklearn-env
In order to check your installation, you can use:
condalistscikit-learn# show scikit-learn version and locationcondalist# show all installed packages in the environmentpython-c"import sklearn; sklearn.show_versions()"
Install Python 3 usinghomebrew(brewinstallpython)
or by manually installing the package from theofficial website.
brewinstallpython
Now create avirtual environment (venv)and install scikit-learn.
Note that the virtual environment is optional but strongly recommended, in
order to avoid potential conflicts with other packges.
python-mvenvsklearn-envsourcesklearn-env/bin/activate# activatepipinstall-Uscikit-learn
In order to check your installation, you can use:
python-mpipshowscikit-learn# show scikit-learn version and locationpython-mpipfreeze# show all installed packages in the environmentpython-c"import sklearn; sklearn.show_versions()"
Install conda using theconda-forge installers(no
administrator permission required). Then run:
condacreate-nsklearn-env-cconda-forgescikit-learncondaactivatesklearn-env
In order to check your installation, you can use:
condalistscikit-learn# show scikit-learn version and locationcondalist# show all installed packages in the environmentpython-c"import sklearn; sklearn.show_versions()"
Python 3 is usually installed by default on most Linux distributions. To
check if you have it installed, try:
python3--versionpip3--version
If you don’t have Python 3 installed, please installpython3andpython3-pipfrom your distribution’s package manager.
python3
python3-pip
Now create avirtual environment (venv)and install scikit-learn.
Note that the virtual environment is optional but strongly recommended, in
order to avoid potential conflicts with other packages.
python3-mvenvsklearn-envsourcesklearn-env/bin/activate# activatepip3install-Uscikit-learn
In order to check your installation, you can use:
python3-mpipshowscikit-learn# show scikit-learn version and locationpython3-mpipfreeze# show all installed packages in the environmentpython3-c"import sklearn; sklearn.show_versions()"
Install conda using theconda-forge installers(no
administrator permission required). Then run:
condacreate-nsklearn-env-cconda-forgescikit-learncondaactivatesklearn-env
In order to check your installation, you can use:
condalistscikit-learn# show scikit-learn version and locationcondalist# show all installed packages in the environmentpython-c"import sklearn; sklearn.show_versions()"
Using an isolated environment such as pip venv or conda makes it possible to
install a specific version of scikit-learn with pip or conda and its dependencies
independently of any previously installed Python packages. In particular under Linux
it is discouraged to install pip packages alongside the packages managed by the
package manager of the distribution (apt, dnf, pacman…).
Note that you should always remember to activate the environment of your choice
prior to running any Python command whenever you start a new terminal session.
If you have not installed NumPy or SciPy yet, you can also install these using
conda or pip. When using pip, please ensure thatbinary wheelsare used,
and NumPy and SciPy are not recompiled from source, which can happen when using
particular configurations of operating system and hardware (such as Linux on
a Raspberry Pi).
Scikit-learn plotting capabilities (i.e., functions starting withplot_and classes ending withDisplay) require Matplotlib. The examples require
Matplotlib and some examples require scikit-image, pandas, or seaborn. The
minimum version of scikit-learn dependencies are listed below along with its
purpose.
plot_
Display
Dependency
Minimum Version
Purpose
numpy
1.19.5
build, install
scipy
1.6.0
build, install
joblib
1.2.0
install
threadpoolctl
3.1.0
install
cython
3.0.10
build
meson-python
0.16.0
build
matplotlib
3.3.4
benchmark, docs, examples, tests
scikit-image
0.17.2
docs, examples, tests
pandas
1.1.5
benchmark, docs, examples, tests
seaborn
0.9.0
docs, examples
memory_profiler
0.57.0
benchmark, docs
pytest
7.1.2
tests
pytest-cov
2.9.0
tests
ruff
0.5.1
tests
black
24.3.0
tests
mypy
1.9
tests
pyamg
4.0.0
tests
polars
0.20.30
docs, tests
pyarrow
12.0.0
tests
sphinx
7.3.7
docs
sphinx-copybutton
0.5.2
docs
sphinx-gallery
0.17.1
docs
numpydoc
1.2.0
docs, tests
Pillow
7.1.2
docs
pooch
1.6.0
docs, examples, tests
sphinx-prompt
1.4.0
docs
sphinxext-opengraph
0.9.1
docs
plotly
5.14.0
docs, examples
sphinxcontrib-sass
0.3.4
docs
sphinx-remove-toctrees
1.0.0.post1
docs
sphinx-design
0.6.0
docs
pydata-sphinx-theme
0.15.3
docs
towncrier
24.8.0
docs
conda-lock
2.5.6
maintenance
Warning
Scikit-learn 0.20 was the last version to support Python 2.7 and Python 3.4.
Scikit-learn 0.21 supported Python 3.5-3.7.
Scikit-learn 0.22 supported Python 3.5-3.8.
Scikit-learn 0.23-0.24 required Python 3.6 or newer.
Scikit-learn 1.0 supported Python 3.7-3.10.
Scikit-learn 1.1, 1.2 and 1.3 support Python 3.8-3.12
Scikit-learn 1.4 requires Python 3.9 or newer.
Third party distributions of scikit-learn#
Some third-party distributions provide versions of
scikit-learn integrated with their package-management systems.
These can make installation and upgrading much easier for users since
the integration includes the ability to automatically install
dependencies (numpy, scipy) that scikit-learn requires.
The following is an incomplete list of OS and python distributions
that provide their own version of scikit-learn.
Alpine Linux#
Alpine Linux’s package is provided through theofficial repositoriesaspy3-scikit-learnfor Python.
It can be installed by typing the following command:
py3-scikit-learn
sudoapkaddpy3-scikit-learn
Arch Linux#
Arch Linux’s package is provided through theofficial repositoriesaspython-scikit-learnfor Python.
It can be installed by typing the following command:
python-scikit-learn
sudopacman-Spython-scikit-learn
Debian/Ubuntu#
The Debian/Ubuntu package is split in three different packages calledpython3-sklearn(python modules),python3-sklearn-lib(low-level
implementations and bindings),python-sklearn-doc(documentation).
Note that scikit-learn requires Python 3, hence the need to use thepython3-suffixed package names.
Packages can be installed usingapt-get:
python3-sklearn
python3-sklearn-lib
python-sklearn-doc
python3-
apt-get
sudoapt-getinstallpython3-sklearnpython3-sklearn-libpython-sklearn-doc
Fedora#
The Fedora package is calledpython3-scikit-learnfor the python 3 version,
the only one available in Fedora.
It can be installed usingdnf:
python3-scikit-learn
dnf
sudodnfinstallpython3-scikit-learn
NetBSD#
scikit-learn is available viapkgsrc-wip:https://pkgsrc.se/math/py-scikit-learn
MacPorts for Mac OSX#
The MacPorts package is namedpy<XY>-scikits-learn,
whereXYdenotes the Python version.
It can be installed by typing the following
command:
py<XY>-scikits-learn
XY
sudoportinstallpy39-scikit-learn
Anaconda and Enthought Deployment Manager for all supported platforms#
AnacondaandEnthought Deployment Managerboth ship with scikit-learn in addition to a large set of scientific
python library for Windows, Mac OSX and Linux.
Anaconda offers scikit-learn as part of its free distribution.
Intel Extension for Scikit-learn#
Intel maintains an optimized x86_64 package, available in PyPI (viapip),
and in themain,conda-forgeandintelconda channels:
pip
main
conda-forge
intel
condainstallscikit-learn-intelex
This package has an Intel optimized version of many estimators. Whenever
an alternative implementation doesn’t exist, scikit-learn implementation
is used as a fallback. Those optimized solvers come from the oneDAL
C++ library and are optimized for the x86_64 architecture, and are
optimized for multi-core Intel CPUs.
Note that those solvers are not enabled by default, please refer to thescikit-learn-intelexdocumentation for more details on usage scenarios. Direct export example:
fromsklearnex.neighborsimportNearestNeighbors
Compatibility with the standard scikit-learn solvers is checked by running the
full scikit-learn test suite via automated continuous integration as reported
onintel/scikit-learn-intelex. If you observe any issue
withscikit-learn-intelex, please report the issue on theirissue tracker.
scikit-learn-intelex
WinPython for Windows#
TheWinPythonproject distributes
scikit-learn as an additional plugin.
Troubleshooting#
If you encounter unexpected failures when installing scikit-learn, you may submit
an issue to theissue tracker.
Before that, please also make sure to check the following common issues.
Error caused by file path length limit on Windows#
It can happen that pip fails to install packages when reaching the default path
size limit of Windows if Python is installed in a nested location such as theAppDatafolder structure under the user home directory, for instance:
AppData
C:\Users\username>C:\Users\username\AppData\Local\Microsoft\WindowsApps\python.exe-mpipinstallscikit-learnCollectingscikit-learn...Installingcollectedpackages:scikit-learnERROR:CouldnotinstallpackagesduetoanOSError:[Errno2]Nosuchfileordirectory:'C:\\Users\\username\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\sklearn\\datasets\\tests\\data\\openml\\292\\api-v1-json-data-list-data_name-australian-limit-2-data_version-1-status-deactivated.json.gz'
In this case it is possible to lift that limit in the Windows registry by
using theregedittool:
regedit
Type “regedit” in the Windows start menu to launchregedit.
Type “regedit” in the Windows start menu to launchregedit.
regedit
Go to theComputer\HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\Control\FileSystemkey.
Go to theComputer\HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\Control\FileSystemkey.
Computer\HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\Control\FileSystem
Edit the value of theLongPathsEnabledproperty of that key and set
it to 1.
Edit the value of theLongPathsEnabledproperty of that key and set
it to 1.
LongPathsEnabled
Reinstall scikit-learn (ignoring the previous broken installation):pipinstall--exists-action=iscikit-learn
Reinstall scikit-learn (ignoring the previous broken installation):
pipinstall--exists-action=iscikit-learn
This Page
Show Source

GitHub
GitHub
API Reference#
This is the class and function reference of scikit-learn. Please refer to thefull user guidefor further details, as the raw specifications of
classes and functions may not be enough to give full guidelines on their uses. For
reference on concepts repeated across the API, seeGlossary of Common Terms and API Elements.
Object
Description
config_context
config_context
Context manager for global scikit-learn configuration.
sklearn
sklearn
get_config
get_config
Retrieve current values for configuration set byset_config.
set_config
sklearn
sklearn
set_config
set_config
Set global scikit-learn configuration.
sklearn
sklearn
show_versions
show_versions
Print useful debugging information”
sklearn
sklearn
BaseEstimator
BaseEstimator
Base class for all estimators in scikit-learn.
sklearn.base
sklearn.base
BiclusterMixin
BiclusterMixin
Mixin class for all bicluster estimators in scikit-learn.
sklearn.base
sklearn.base
ClassNamePrefixFeaturesOutMixin
ClassNamePrefixFeaturesOutMixin
Mixin class for transformers that generate their own names by prefixing.
sklearn.base
sklearn.base
ClassifierMixin
ClassifierMixin
Mixin class for all classifiers in scikit-learn.
sklearn.base
sklearn.base
ClusterMixin
ClusterMixin
Mixin class for all cluster estimators in scikit-learn.
sklearn.base
sklearn.base
DensityMixin
DensityMixin
Mixin class for all density estimators in scikit-learn.
sklearn.base
sklearn.base
MetaEstimatorMixin
MetaEstimatorMixin
Mixin class for all meta estimators in scikit-learn.
sklearn.base
sklearn.base
OneToOneFeatureMixin
OneToOneFeatureMixin
Providesget_feature_names_outfor simple transformers.
get_feature_names_out
sklearn.base
sklearn.base
OutlierMixin
OutlierMixin
Mixin class for all outlier detection estimators in scikit-learn.
sklearn.base
sklearn.base
RegressorMixin
RegressorMixin
Mixin class for all regression estimators in scikit-learn.
sklearn.base
sklearn.base
TransformerMixin
TransformerMixin
Mixin class for all transformers in scikit-learn.
sklearn.base
sklearn.base
clone
clone
Construct a new unfitted estimator with the same parameters.
sklearn.base
sklearn.base
is_classifier
is_classifier
Return True if the given estimator is (probably) a classifier.
sklearn.base
sklearn.base
is_clusterer
is_clusterer
Return True if the given estimator is (probably) a clusterer.
sklearn.base
sklearn.base
is_regressor
is_regressor
Return True if the given estimator is (probably) a regressor.
sklearn.base
sklearn.base
is_outlier_detector
is_outlier_detector
Return True if the given estimator is (probably) an outlier detector.
sklearn.base
sklearn.base
CalibratedClassifierCV
CalibratedClassifierCV
Probability calibration with isotonic regression or logistic regression.
sklearn.calibration
sklearn.calibration
calibration_curve
calibration_curve
Compute true and predicted probabilities for a calibration curve.
sklearn.calibration
sklearn.calibration
CalibrationDisplay
CalibrationDisplay
Calibration curve (also known as reliability diagram) visualization.
sklearn.calibration
sklearn.calibration
AffinityPropagation
AffinityPropagation
Perform Affinity Propagation Clustering of data.
sklearn.cluster
sklearn.cluster
AgglomerativeClustering
AgglomerativeClustering
Agglomerative Clustering.
sklearn.cluster
sklearn.cluster
Birch
Birch
Implements the BIRCH clustering algorithm.
sklearn.cluster
sklearn.cluster
BisectingKMeans
BisectingKMeans
Bisecting K-Means clustering.
sklearn.cluster
sklearn.cluster
DBSCAN
DBSCAN
Perform DBSCAN clustering from vector array or distance matrix.
sklearn.cluster
sklearn.cluster
FeatureAgglomeration
FeatureAgglomeration
Agglomerate features.
sklearn.cluster
sklearn.cluster
HDBSCAN
HDBSCAN
Cluster data using hierarchical density-based clustering.
sklearn.cluster
sklearn.cluster
KMeans
KMeans
K-Means clustering.
sklearn.cluster
sklearn.cluster
MeanShift
MeanShift
Mean shift clustering using a flat kernel.
sklearn.cluster
sklearn.cluster
MiniBatchKMeans
MiniBatchKMeans
Mini-Batch K-Means clustering.
sklearn.cluster
sklearn.cluster
OPTICS
OPTICS
Estimate clustering structure from vector array.
sklearn.cluster
sklearn.cluster
SpectralBiclustering
SpectralBiclustering
Spectral biclustering (Kluger, 2003).
sklearn.cluster
sklearn.cluster
SpectralClustering
SpectralClustering
Apply clustering to a projection of the normalized Laplacian.
sklearn.cluster
sklearn.cluster
SpectralCoclustering
SpectralCoclustering
Spectral Co-Clustering algorithm (Dhillon, 2001).
sklearn.cluster
sklearn.cluster
affinity_propagation
affinity_propagation
Perform Affinity Propagation Clustering of data.
sklearn.cluster
sklearn.cluster
cluster_optics_dbscan
cluster_optics_dbscan
Perform DBSCAN extraction for an arbitrary epsilon.
sklearn.cluster
sklearn.cluster
cluster_optics_xi
cluster_optics_xi
Automatically extract clusters according to the Xi-steep method.
sklearn.cluster
sklearn.cluster
compute_optics_graph
compute_optics_graph
Compute the OPTICS reachability graph.
sklearn.cluster
sklearn.cluster
dbscan
dbscan
Perform DBSCAN clustering from vector array or distance matrix.
sklearn.cluster
sklearn.cluster
estimate_bandwidth
estimate_bandwidth
Estimate the bandwidth to use with the mean-shift algorithm.
sklearn.cluster
sklearn.cluster
k_means
k_means
Perform K-means clustering algorithm.
sklearn.cluster
sklearn.cluster
kmeans_plusplus
kmeans_plusplus
Init n_clusters seeds according to k-means++.
sklearn.cluster
sklearn.cluster
mean_shift
mean_shift
Perform mean shift clustering of data using a flat kernel.
sklearn.cluster
sklearn.cluster
spectral_clustering
spectral_clustering
Apply clustering to a projection of the normalized Laplacian.
sklearn.cluster
sklearn.cluster
ward_tree
ward_tree
Ward clustering based on a Feature matrix.
sklearn.cluster
sklearn.cluster
ColumnTransformer
ColumnTransformer
Applies transformers to columns of an array or pandas DataFrame.
sklearn.compose
sklearn.compose
TransformedTargetRegressor
TransformedTargetRegressor
Meta-estimator to regress on a transformed target.
sklearn.compose
sklearn.compose
make_column_selector
make_column_selector
Create a callable to select columns to be used with
sklearn.compose
sklearn.compose
make_column_transformer
make_column_transformer
Construct a ColumnTransformer from the given transformers.
sklearn.compose
sklearn.compose
EllipticEnvelope
EllipticEnvelope
An object for detecting outliers in a Gaussian distributed dataset.
sklearn.covariance
sklearn.covariance
EmpiricalCovariance
EmpiricalCovariance
Maximum likelihood covariance estimator.
sklearn.covariance
sklearn.covariance
GraphicalLasso
GraphicalLasso
Sparse inverse covariance estimation with an l1-penalized estimator.
sklearn.covariance
sklearn.covariance
GraphicalLassoCV
GraphicalLassoCV
Sparse inverse covariance w/ cross-validated choice of the l1 penalty.
sklearn.covariance
sklearn.covariance
LedoitWolf
LedoitWolf
LedoitWolf Estimator.
sklearn.covariance
sklearn.covariance
MinCovDet
MinCovDet
Minimum Covariance Determinant (MCD): robust estimator of covariance.
sklearn.covariance
sklearn.covariance
OAS
OAS
Oracle Approximating Shrinkage Estimator.
sklearn.covariance
sklearn.covariance
ShrunkCovariance
ShrunkCovariance
Covariance estimator with shrinkage.
sklearn.covariance
sklearn.covariance
empirical_covariance
empirical_covariance
Compute the Maximum likelihood covariance estimator.
sklearn.covariance
sklearn.covariance
graphical_lasso
graphical_lasso
L1-penalized covariance estimator.
sklearn.covariance
sklearn.covariance
ledoit_wolf
ledoit_wolf
Estimate the shrunk Ledoit-Wolf covariance matrix.
sklearn.covariance
sklearn.covariance
ledoit_wolf_shrinkage
ledoit_wolf_shrinkage
Estimate the shrunk Ledoit-Wolf covariance matrix.
sklearn.covariance
sklearn.covariance
oas
oas
Estimate covariance with the Oracle Approximating Shrinkage.
sklearn.covariance
sklearn.covariance
shrunk_covariance
shrunk_covariance
Calculate covariance matrices shrunk on the diagonal.
sklearn.covariance
sklearn.covariance
CCA
CCA
Canonical Correlation Analysis, also known as “Mode B” PLS.
sklearn.cross_decomposition
sklearn.cross_decomposition
PLSCanonical
PLSCanonical
Partial Least Squares transformer and regressor.
sklearn.cross_decomposition
sklearn.cross_decomposition
PLSRegression
PLSRegression
PLS regression.
sklearn.cross_decomposition
sklearn.cross_decomposition
PLSSVD
PLSSVD
Partial Least Square SVD.
sklearn.cross_decomposition
sklearn.cross_decomposition
clear_data_home
clear_data_home
Delete all the content of the data home cache.
sklearn.datasets
sklearn.datasets
dump_svmlight_file
dump_svmlight_file
Dump the dataset in svmlight / libsvm file format.
sklearn.datasets
sklearn.datasets
fetch_20newsgroups
fetch_20newsgroups
Load the filenames and data from the 20 newsgroups dataset (classification).
sklearn.datasets
sklearn.datasets
fetch_20newsgroups_vectorized
fetch_20newsgroups_vectorized
Load and vectorize the 20 newsgroups dataset (classification).
sklearn.datasets
sklearn.datasets
fetch_california_housing
fetch_california_housing
Load the California housing dataset (regression).
sklearn.datasets
sklearn.datasets
fetch_covtype
fetch_covtype
Load the covertype dataset (classification).
sklearn.datasets
sklearn.datasets
fetch_file
fetch_file
Fetch a file from the web if not already present in the local folder.
sklearn.datasets
sklearn.datasets
fetch_kddcup99
fetch_kddcup99
Load the kddcup99 dataset (classification).
sklearn.datasets
sklearn.datasets
fetch_lfw_pairs
fetch_lfw_pairs
Load the Labeled Faces in the Wild (LFW) pairs dataset (classification).
sklearn.datasets
sklearn.datasets
fetch_lfw_people
fetch_lfw_people
Load the Labeled Faces in the Wild (LFW) people dataset (classification).
sklearn.datasets
sklearn.datasets
fetch_olivetti_faces
fetch_olivetti_faces
Load the Olivetti faces data-set from AT&T (classification).
sklearn.datasets
sklearn.datasets
fetch_openml
fetch_openml
Fetch dataset from openml by name or dataset id.
sklearn.datasets
sklearn.datasets
fetch_rcv1
fetch_rcv1
Load the RCV1 multilabel dataset (classification).
sklearn.datasets
sklearn.datasets
fetch_species_distributions
fetch_species_distributions
Loader for species distribution dataset from Phillips et. al. (2006).
sklearn.datasets
sklearn.datasets
get_data_home
get_data_home
Return the path of the scikit-learn data directory.
sklearn.datasets
sklearn.datasets
load_breast_cancer
load_breast_cancer
Load and return the breast cancer wisconsin dataset (classification).
sklearn.datasets
sklearn.datasets
load_diabetes
load_diabetes
Load and return the diabetes dataset (regression).
sklearn.datasets
sklearn.datasets
load_digits
load_digits
Load and return the digits dataset (classification).
sklearn.datasets
sklearn.datasets
load_files
load_files
Load text files with categories as subfolder names.
sklearn.datasets
sklearn.datasets
load_iris
load_iris
Load and return the iris dataset (classification).
sklearn.datasets
sklearn.datasets
load_linnerud
load_linnerud
Load and return the physical exercise Linnerud dataset.
sklearn.datasets
sklearn.datasets
load_sample_image
load_sample_image
Load the numpy array of a single sample image.
sklearn.datasets
sklearn.datasets
load_sample_images
load_sample_images
Load sample images for image manipulation.
sklearn.datasets
sklearn.datasets
load_svmlight_file
load_svmlight_file
Load datasets in the svmlight / libsvm format into sparse CSR matrix.
sklearn.datasets
sklearn.datasets
load_svmlight_files
load_svmlight_files
Load dataset from multiple files in SVMlight format.
sklearn.datasets
sklearn.datasets
load_wine
load_wine
Load and return the wine dataset (classification).
sklearn.datasets
sklearn.datasets
make_biclusters
make_biclusters
Generate a constant block diagonal structure array for biclustering.
sklearn.datasets
sklearn.datasets
make_blobs
make_blobs
Generate isotropic Gaussian blobs for clustering.
sklearn.datasets
sklearn.datasets
make_checkerboard
make_checkerboard
Generate an array with block checkerboard structure for biclustering.
sklearn.datasets
sklearn.datasets
make_circles
make_circles
Make a large circle containing a smaller circle in 2d.
sklearn.datasets
sklearn.datasets
make_classification
make_classification
Generate a random n-class classification problem.
sklearn.datasets
sklearn.datasets
make_friedman1
make_friedman1
Generate the “Friedman #1” regression problem.
sklearn.datasets
sklearn.datasets
make_friedman2
make_friedman2
Generate the “Friedman #2” regression problem.
sklearn.datasets
sklearn.datasets
make_friedman3
make_friedman3
Generate the “Friedman #3” regression problem.
sklearn.datasets
sklearn.datasets
make_gaussian_quantiles
make_gaussian_quantiles
Generate isotropic Gaussian and label samples by quantile.
sklearn.datasets
sklearn.datasets
make_hastie_10_2
make_hastie_10_2
Generate data for binary classification used in Hastie et al. 2009, Example 10.2.
sklearn.datasets
sklearn.datasets
make_low_rank_matrix
make_low_rank_matrix
Generate a mostly low rank matrix with bell-shaped singular values.
sklearn.datasets
sklearn.datasets
make_moons
make_moons
Make two interleaving half circles.
sklearn.datasets
sklearn.datasets
make_multilabel_classification
make_multilabel_classification
Generate a random multilabel classification problem.
sklearn.datasets
sklearn.datasets
make_regression
make_regression
Generate a random regression problem.
sklearn.datasets
sklearn.datasets
make_s_curve
make_s_curve
Generate an S curve dataset.
sklearn.datasets
sklearn.datasets
make_sparse_coded_signal
make_sparse_coded_signal
Generate a signal as a sparse combination of dictionary elements.
sklearn.datasets
sklearn.datasets
make_sparse_spd_matrix
make_sparse_spd_matrix
Generate a sparse symmetric definite positive matrix.
sklearn.datasets
sklearn.datasets
make_sparse_uncorrelated
make_sparse_uncorrelated
Generate a random regression problem with sparse uncorrelated design.
sklearn.datasets
sklearn.datasets
make_spd_matrix
make_spd_matrix
Generate a random symmetric, positive-definite matrix.
sklearn.datasets
sklearn.datasets
make_swiss_roll
make_swiss_roll
Generate a swiss roll dataset.
sklearn.datasets
sklearn.datasets
DictionaryLearning
DictionaryLearning
Dictionary learning.
sklearn.decomposition
sklearn.decomposition
FactorAnalysis
FactorAnalysis
Factor Analysis (FA).
sklearn.decomposition
sklearn.decomposition
FastICA
FastICA
FastICA: a fast algorithm for Independent Component Analysis.
sklearn.decomposition
sklearn.decomposition
IncrementalPCA
IncrementalPCA
Incremental principal components analysis (IPCA).
sklearn.decomposition
sklearn.decomposition
KernelPCA
KernelPCA
Kernel Principal component analysis (KPCA).
sklearn.decomposition
sklearn.decomposition
LatentDirichletAllocation
LatentDirichletAllocation
Latent Dirichlet Allocation with online variational Bayes algorithm.
sklearn.decomposition
sklearn.decomposition
MiniBatchDictionaryLearning
MiniBatchDictionaryLearning
Mini-batch dictionary learning.
sklearn.decomposition
sklearn.decomposition
MiniBatchNMF
MiniBatchNMF
Mini-Batch Non-Negative Matrix Factorization (NMF).
sklearn.decomposition
sklearn.decomposition
MiniBatchSparsePCA
MiniBatchSparsePCA
Mini-batch Sparse Principal Components Analysis.
sklearn.decomposition
sklearn.decomposition
NMF
NMF
Non-Negative Matrix Factorization (NMF).
sklearn.decomposition
sklearn.decomposition
PCA
PCA
Principal component analysis (PCA).
sklearn.decomposition
sklearn.decomposition
SparseCoder
SparseCoder
Sparse coding.
sklearn.decomposition
sklearn.decomposition
SparsePCA
SparsePCA
Sparse Principal Components Analysis (SparsePCA).
sklearn.decomposition
sklearn.decomposition
TruncatedSVD
TruncatedSVD
Dimensionality reduction using truncated SVD (aka LSA).
sklearn.decomposition
sklearn.decomposition
dict_learning
dict_learning
Solve a dictionary learning matrix factorization problem.
sklearn.decomposition
sklearn.decomposition
dict_learning_online
dict_learning_online
Solve a dictionary learning matrix factorization problem online.
sklearn.decomposition
sklearn.decomposition
fastica
fastica
Perform Fast Independent Component Analysis.
sklearn.decomposition
sklearn.decomposition
non_negative_factorization
non_negative_factorization
Compute Non-negative Matrix Factorization (NMF).
sklearn.decomposition
sklearn.decomposition
sparse_encode
sparse_encode
Sparse coding.
sklearn.decomposition
sklearn.decomposition
LinearDiscriminantAnalysis
LinearDiscriminantAnalysis
Linear Discriminant Analysis.
sklearn.discriminant_analysis
sklearn.discriminant_analysis
QuadraticDiscriminantAnalysis
QuadraticDiscriminantAnalysis
Quadratic Discriminant Analysis.
sklearn.discriminant_analysis
sklearn.discriminant_analysis
DummyClassifier
DummyClassifier
DummyClassifier makes predictions that ignore the input features.
sklearn.dummy
sklearn.dummy
DummyRegressor
DummyRegressor
Regressor that makes predictions using simple rules.
sklearn.dummy
sklearn.dummy
AdaBoostClassifier
AdaBoostClassifier
An AdaBoost classifier.
sklearn.ensemble
sklearn.ensemble
AdaBoostRegressor
AdaBoostRegressor
An AdaBoost regressor.
sklearn.ensemble
sklearn.ensemble
BaggingClassifier
BaggingClassifier
A Bagging classifier.
sklearn.ensemble
sklearn.ensemble
BaggingRegressor
BaggingRegressor
A Bagging regressor.
sklearn.ensemble
sklearn.ensemble
ExtraTreesClassifier
ExtraTreesClassifier
An extra-trees classifier.
sklearn.ensemble
sklearn.ensemble
ExtraTreesRegressor
ExtraTreesRegressor
An extra-trees regressor.
sklearn.ensemble
sklearn.ensemble
GradientBoostingClassifier
GradientBoostingClassifier
Gradient Boosting for classification.
sklearn.ensemble
sklearn.ensemble
GradientBoostingRegressor
GradientBoostingRegressor
Gradient Boosting for regression.
sklearn.ensemble
sklearn.ensemble
HistGradientBoostingClassifier
HistGradientBoostingClassifier
Histogram-based Gradient Boosting Classification Tree.
sklearn.ensemble
sklearn.ensemble
HistGradientBoostingRegressor
HistGradientBoostingRegressor
Histogram-based Gradient Boosting Regression Tree.
sklearn.ensemble
sklearn.ensemble
IsolationForest
IsolationForest
Isolation Forest Algorithm.
sklearn.ensemble
sklearn.ensemble
RandomForestClassifier
RandomForestClassifier
A random forest classifier.
sklearn.ensemble
sklearn.ensemble
RandomForestRegressor
RandomForestRegressor
A random forest regressor.
sklearn.ensemble
sklearn.ensemble
RandomTreesEmbedding
RandomTreesEmbedding
An ensemble of totally random trees.
sklearn.ensemble
sklearn.ensemble
StackingClassifier
StackingClassifier
Stack of estimators with a final classifier.
sklearn.ensemble
sklearn.ensemble
StackingRegressor
StackingRegressor
Stack of estimators with a final regressor.
sklearn.ensemble
sklearn.ensemble
VotingClassifier
VotingClassifier
Soft Voting/Majority Rule classifier for unfitted estimators.
sklearn.ensemble
sklearn.ensemble
VotingRegressor
VotingRegressor
Prediction voting regressor for unfitted estimators.
sklearn.ensemble
sklearn.ensemble
ConvergenceWarning
ConvergenceWarning
Custom warning to capture convergence problems
sklearn.exceptions
sklearn.exceptions
DataConversionWarning
DataConversionWarning
Warning used to notify implicit data conversions happening in the code.
sklearn.exceptions
sklearn.exceptions
DataDimensionalityWarning
DataDimensionalityWarning
Custom warning to notify potential issues with data dimensionality.
sklearn.exceptions
sklearn.exceptions
EfficiencyWarning
EfficiencyWarning
Warning used to notify the user of inefficient computation.
sklearn.exceptions
sklearn.exceptions
FitFailedWarning
FitFailedWarning
Warning class used if there is an error while fitting the estimator.
sklearn.exceptions
sklearn.exceptions
InconsistentVersionWarning
InconsistentVersionWarning
Warning raised when an estimator is unpickled with a inconsistent version.
sklearn.exceptions
sklearn.exceptions
NotFittedError
NotFittedError
Exception class to raise if estimator is used before fitting.
sklearn.exceptions
sklearn.exceptions
UndefinedMetricWarning
UndefinedMetricWarning
Warning used when the metric is invalid
sklearn.exceptions
sklearn.exceptions
EstimatorCheckFailedWarning
EstimatorCheckFailedWarning
Warning raised when an estimator check from the common tests fails.
sklearn.exceptions
sklearn.exceptions
enable_halving_search_cv
enable_halving_search_cv
Enables Successive Halving search-estimators
sklearn.experimental
sklearn.experimental
enable_iterative_imputer
enable_iterative_imputer
Enables IterativeImputer
sklearn.experimental
sklearn.experimental
DictVectorizer
DictVectorizer
Transforms lists of feature-value mappings to vectors.
sklearn.feature_extraction
sklearn.feature_extraction
FeatureHasher
FeatureHasher
Implements feature hashing, aka the hashing trick.
sklearn.feature_extraction
sklearn.feature_extraction
PatchExtractor
PatchExtractor
Extracts patches from a collection of images.
sklearn.feature_extraction.image
sklearn.feature_extraction.image
extract_patches_2d
extract_patches_2d
Reshape a 2D image into a collection of patches.
sklearn.feature_extraction.image
sklearn.feature_extraction.image
grid_to_graph
grid_to_graph
Graph of the pixel-to-pixel connections.
sklearn.feature_extraction.image
sklearn.feature_extraction.image
img_to_graph
img_to_graph
Graph of the pixel-to-pixel gradient connections.
sklearn.feature_extraction.image
sklearn.feature_extraction.image
reconstruct_from_patches_2d
reconstruct_from_patches_2d
Reconstruct the image from all of its patches.
sklearn.feature_extraction.image
sklearn.feature_extraction.image
CountVectorizer
CountVectorizer
Convert a collection of text documents to a matrix of token counts.
sklearn.feature_extraction.text
sklearn.feature_extraction.text
HashingVectorizer
HashingVectorizer
Convert a collection of text documents to a matrix of token occurrences.
sklearn.feature_extraction.text
sklearn.feature_extraction.text
TfidfTransformer
TfidfTransformer
Transform a count matrix to a normalized tf or tf-idf representation.
sklearn.feature_extraction.text
sklearn.feature_extraction.text
TfidfVectorizer
TfidfVectorizer
Convert a collection of raw documents to a matrix of TF-IDF features.
sklearn.feature_extraction.text
sklearn.feature_extraction.text
GenericUnivariateSelect
GenericUnivariateSelect
Univariate feature selector with configurable strategy.
sklearn.feature_selection
sklearn.feature_selection
RFE
RFE
Feature ranking with recursive feature elimination.
sklearn.feature_selection
sklearn.feature_selection
RFECV
RFECV
Recursive feature elimination with cross-validation to select features.
sklearn.feature_selection
sklearn.feature_selection
SelectFdr
SelectFdr
Filter: Select the p-values for an estimated false discovery rate.
sklearn.feature_selection
sklearn.feature_selection
SelectFpr
SelectFpr
Filter: Select the pvalues below alpha based on a FPR test.
sklearn.feature_selection
sklearn.feature_selection
SelectFromModel
SelectFromModel
Meta-transformer for selecting features based on importance weights.
sklearn.feature_selection
sklearn.feature_selection
SelectFwe
SelectFwe
Filter: Select the p-values corresponding to Family-wise error rate.
sklearn.feature_selection
sklearn.feature_selection
SelectKBest
SelectKBest
Select features according to the k highest scores.
sklearn.feature_selection
sklearn.feature_selection
SelectPercentile
SelectPercentile
Select features according to a percentile of the highest scores.
sklearn.feature_selection
sklearn.feature_selection
SelectorMixin
SelectorMixin
Transformer mixin that performs feature selection given a support mask
sklearn.feature_selection
sklearn.feature_selection
SequentialFeatureSelector
SequentialFeatureSelector
Transformer that performs Sequential Feature Selection.
sklearn.feature_selection
sklearn.feature_selection
VarianceThreshold
VarianceThreshold
Feature selector that removes all low-variance features.
sklearn.feature_selection
sklearn.feature_selection
chi2
chi2
Compute chi-squared stats between each non-negative feature and class.
sklearn.feature_selection
sklearn.feature_selection
f_classif
f_classif
Compute the ANOVA F-value for the provided sample.
sklearn.feature_selection
sklearn.feature_selection
f_regression
f_regression
Univariate linear regression tests returning F-statistic and p-values.
sklearn.feature_selection
sklearn.feature_selection
mutual_info_classif
mutual_info_classif
Estimate mutual information for a discrete target variable.
sklearn.feature_selection
sklearn.feature_selection
mutual_info_regression
mutual_info_regression
Estimate mutual information for a continuous target variable.
sklearn.feature_selection
sklearn.feature_selection
r_regression
r_regression
Compute Pearson’s r for each features and the target.
sklearn.feature_selection
sklearn.feature_selection
FrozenEstimator
FrozenEstimator
Estimator that wraps a fitted estimator to prevent re-fitting.
sklearn.frozen
sklearn.frozen
GaussianProcessClassifier
GaussianProcessClassifier
Gaussian process classification (GPC) based on Laplace approximation.
sklearn.gaussian_process
sklearn.gaussian_process
GaussianProcessRegressor
GaussianProcessRegressor
Gaussian process regression (GPR).
sklearn.gaussian_process
sklearn.gaussian_process
CompoundKernel
CompoundKernel
Kernel which is composed of a set of other kernels.
sklearn.gaussian_process.kernels
sklearn.gaussian_process.kernels
ConstantKernel
ConstantKernel
Constant kernel.
sklearn.gaussian_process.kernels
sklearn.gaussian_process.kernels
DotProduct
DotProduct
Dot-Product kernel.
sklearn.gaussian_process.kernels
sklearn.gaussian_process.kernels
ExpSineSquared
ExpSineSquared
Exp-Sine-Squared kernel (aka periodic kernel).
sklearn.gaussian_process.kernels
sklearn.gaussian_process.kernels
Exponentiation
Exponentiation
The Exponentiation kernel takes one base kernel and a scalar parameter
sklearn.gaussian_process.kernels
sklearn.gaussian_process.kernels
Hyperparameter
Hyperparameter
A kernel hyperparameter’s specification in form of a namedtuple.
sklearn.gaussian_process.kernels
sklearn.gaussian_process.kernels
Kernel
Kernel
Base class for all kernels.
sklearn.gaussian_process.kernels
sklearn.gaussian_process.kernels
Matern
Matern
Matern kernel.
sklearn.gaussian_process.kernels
sklearn.gaussian_process.kernels
PairwiseKernel
PairwiseKernel
Wrapper for kernels in sklearn.metrics.pairwise.
sklearn.gaussian_process.kernels
sklearn.gaussian_process.kernels
Product
Product
TheProductkernel takes two kernels\(k_1\)and\(k_2\)
Product
sklearn.gaussian_process.kernels
sklearn.gaussian_process.kernels
RBF
RBF
Radial basis function kernel (aka squared-exponential kernel).
sklearn.gaussian_process.kernels
sklearn.gaussian_process.kernels
RationalQuadratic
RationalQuadratic
Rational Quadratic kernel.
sklearn.gaussian_process.kernels
sklearn.gaussian_process.kernels
Sum
Sum
TheSumkernel takes two kernels\(k_1\)and\(k_2\)
Sum
sklearn.gaussian_process.kernels
sklearn.gaussian_process.kernels
WhiteKernel
WhiteKernel
White kernel.
sklearn.gaussian_process.kernels
sklearn.gaussian_process.kernels
IterativeImputer
IterativeImputer
Multivariate imputer that estimates each feature from all the others.
sklearn.impute
sklearn.impute
KNNImputer
KNNImputer
Imputation for completing missing values using k-Nearest Neighbors.
sklearn.impute
sklearn.impute
MissingIndicator
MissingIndicator
Binary indicators for missing values.
sklearn.impute
sklearn.impute
SimpleImputer
SimpleImputer
Univariate imputer for completing missing values with simple strategies.
sklearn.impute
sklearn.impute
partial_dependence
partial_dependence
Partial dependence offeatures.
features
sklearn.inspection
sklearn.inspection
permutation_importance
permutation_importance
Permutation importance for feature evaluation[Rd9e56ef97513-BRE].
sklearn.inspection
sklearn.inspection
DecisionBoundaryDisplay
DecisionBoundaryDisplay
Decisions boundary visualization.
sklearn.inspection
sklearn.inspection
PartialDependenceDisplay
PartialDependenceDisplay
Partial Dependence Plot (PDP).
sklearn.inspection
sklearn.inspection
IsotonicRegression
IsotonicRegression
Isotonic regression model.
sklearn.isotonic
sklearn.isotonic
check_increasing
check_increasing
Determine whether y is monotonically correlated with x.
sklearn.isotonic
sklearn.isotonic
isotonic_regression
isotonic_regression
Solve the isotonic regression model.
sklearn.isotonic
sklearn.isotonic
AdditiveChi2Sampler
AdditiveChi2Sampler
Approximate feature map for additive chi2 kernel.
sklearn.kernel_approximation
sklearn.kernel_approximation
Nystroem
Nystroem
Approximate a kernel map using a subset of the training data.
sklearn.kernel_approximation
sklearn.kernel_approximation
PolynomialCountSketch
PolynomialCountSketch
Polynomial kernel approximation via Tensor Sketch.
sklearn.kernel_approximation
sklearn.kernel_approximation
RBFSampler
RBFSampler
Approximate a RBF kernel feature map using random Fourier features.
sklearn.kernel_approximation
sklearn.kernel_approximation
SkewedChi2Sampler
SkewedChi2Sampler
Approximate feature map for “skewed chi-squared” kernel.
sklearn.kernel_approximation
sklearn.kernel_approximation
KernelRidge
KernelRidge
Kernel ridge regression.
sklearn.kernel_ridge
sklearn.kernel_ridge
LogisticRegression
LogisticRegression
Logistic Regression (aka logit, MaxEnt) classifier.
sklearn.linear_model
sklearn.linear_model
LogisticRegressionCV
LogisticRegressionCV
Logistic Regression CV (aka logit, MaxEnt) classifier.
sklearn.linear_model
sklearn.linear_model
PassiveAggressiveClassifier
PassiveAggressiveClassifier
Passive Aggressive Classifier.
sklearn.linear_model
sklearn.linear_model
Perceptron
Perceptron
Linear perceptron classifier.
sklearn.linear_model
sklearn.linear_model
RidgeClassifier
RidgeClassifier
Classifier using Ridge regression.
sklearn.linear_model
sklearn.linear_model
RidgeClassifierCV
RidgeClassifierCV
Ridge classifier with built-in cross-validation.
sklearn.linear_model
sklearn.linear_model
SGDClassifier
SGDClassifier
Linear classifiers (SVM, logistic regression, etc.) with SGD training.
sklearn.linear_model
sklearn.linear_model
SGDOneClassSVM
SGDOneClassSVM
Solves linear One-Class SVM using Stochastic Gradient Descent.
sklearn.linear_model
sklearn.linear_model
LinearRegression
LinearRegression
Ordinary least squares Linear Regression.
sklearn.linear_model
sklearn.linear_model
Ridge
Ridge
Linear least squares with l2 regularization.
sklearn.linear_model
sklearn.linear_model
RidgeCV
RidgeCV
Ridge regression with built-in cross-validation.
sklearn.linear_model
sklearn.linear_model
SGDRegressor
SGDRegressor
Linear model fitted by minimizing a regularized empirical loss with SGD.
sklearn.linear_model
sklearn.linear_model
ElasticNet
ElasticNet
Linear regression with combined L1 and L2 priors as regularizer.
sklearn.linear_model
sklearn.linear_model
ElasticNetCV
ElasticNetCV
Elastic Net model with iterative fitting along a regularization path.
sklearn.linear_model
sklearn.linear_model
Lars
Lars
Least Angle Regression model a.k.a. LAR.
sklearn.linear_model
sklearn.linear_model
LarsCV
LarsCV
Cross-validated Least Angle Regression model.
sklearn.linear_model
sklearn.linear_model
Lasso
Lasso
Linear Model trained with L1 prior as regularizer (aka the Lasso).
sklearn.linear_model
sklearn.linear_model
LassoCV
LassoCV
Lasso linear model with iterative fitting along a regularization path.
sklearn.linear_model
sklearn.linear_model
LassoLars
LassoLars
Lasso model fit with Least Angle Regression a.k.a. Lars.
sklearn.linear_model
sklearn.linear_model
LassoLarsCV
LassoLarsCV
Cross-validated Lasso, using the LARS algorithm.
sklearn.linear_model
sklearn.linear_model
LassoLarsIC
LassoLarsIC
Lasso model fit with Lars using BIC or AIC for model selection.
sklearn.linear_model
sklearn.linear_model
OrthogonalMatchingPursuit
OrthogonalMatchingPursuit
Orthogonal Matching Pursuit model (OMP).
sklearn.linear_model
sklearn.linear_model
OrthogonalMatchingPursuitCV
OrthogonalMatchingPursuitCV
Cross-validated Orthogonal Matching Pursuit model (OMP).
sklearn.linear_model
sklearn.linear_model
ARDRegression
ARDRegression
Bayesian ARD regression.
sklearn.linear_model
sklearn.linear_model
BayesianRidge
BayesianRidge
Bayesian ridge regression.
sklearn.linear_model
sklearn.linear_model
MultiTaskElasticNet
MultiTaskElasticNet
Multi-task ElasticNet model trained with L1/L2 mixed-norm as regularizer.
sklearn.linear_model
sklearn.linear_model
MultiTaskElasticNetCV
MultiTaskElasticNetCV
Multi-task L1/L2 ElasticNet with built-in cross-validation.
sklearn.linear_model
sklearn.linear_model
MultiTaskLasso
MultiTaskLasso
Multi-task Lasso model trained with L1/L2 mixed-norm as regularizer.
sklearn.linear_model
sklearn.linear_model
MultiTaskLassoCV
MultiTaskLassoCV
Multi-task Lasso model trained with L1/L2 mixed-norm as regularizer.
sklearn.linear_model
sklearn.linear_model
HuberRegressor
HuberRegressor
L2-regularized linear regression model that is robust to outliers.
sklearn.linear_model
sklearn.linear_model
QuantileRegressor
QuantileRegressor
Linear regression model that predicts conditional quantiles.
sklearn.linear_model
sklearn.linear_model
RANSACRegressor
RANSACRegressor
RANSAC (RANdom SAmple Consensus) algorithm.
sklearn.linear_model
sklearn.linear_model
TheilSenRegressor
TheilSenRegressor
Theil-Sen Estimator: robust multivariate regression model.
sklearn.linear_model
sklearn.linear_model
GammaRegressor
GammaRegressor
Generalized Linear Model with a Gamma distribution.
sklearn.linear_model
sklearn.linear_model
PoissonRegressor
PoissonRegressor
Generalized Linear Model with a Poisson distribution.
sklearn.linear_model
sklearn.linear_model
TweedieRegressor
TweedieRegressor
Generalized Linear Model with a Tweedie distribution.
sklearn.linear_model
sklearn.linear_model
PassiveAggressiveRegressor
PassiveAggressiveRegressor
Passive Aggressive Regressor.
sklearn.linear_model
sklearn.linear_model
enet_path
enet_path
Compute elastic net path with coordinate descent.
sklearn.linear_model
sklearn.linear_model
lars_path
lars_path
Compute Least Angle Regression or Lasso path using the LARS algorithm.
sklearn.linear_model
sklearn.linear_model
lars_path_gram
lars_path_gram
The lars_path in the sufficient stats mode.
sklearn.linear_model
sklearn.linear_model
lasso_path
lasso_path
Compute Lasso path with coordinate descent.
sklearn.linear_model
sklearn.linear_model
orthogonal_mp
orthogonal_mp
Orthogonal Matching Pursuit (OMP).
sklearn.linear_model
sklearn.linear_model
orthogonal_mp_gram
orthogonal_mp_gram
Gram Orthogonal Matching Pursuit (OMP).
sklearn.linear_model
sklearn.linear_model
ridge_regression
ridge_regression
Solve the ridge equation by the method of normal equations.
sklearn.linear_model
sklearn.linear_model
Isomap
Isomap
Isomap Embedding.
sklearn.manifold
sklearn.manifold
LocallyLinearEmbedding
LocallyLinearEmbedding
Locally Linear Embedding.
sklearn.manifold
sklearn.manifold
MDS
MDS
Multidimensional scaling.
sklearn.manifold
sklearn.manifold
SpectralEmbedding
SpectralEmbedding
Spectral embedding for non-linear dimensionality reduction.
sklearn.manifold
sklearn.manifold
TSNE
TSNE
T-distributed Stochastic Neighbor Embedding.
sklearn.manifold
sklearn.manifold
locally_linear_embedding
locally_linear_embedding
Perform a Locally Linear Embedding analysis on the data.
sklearn.manifold
sklearn.manifold
smacof
smacof
Compute multidimensional scaling using the SMACOF algorithm.
sklearn.manifold
sklearn.manifold
spectral_embedding
spectral_embedding
Project the sample on the first eigenvectors of the graph Laplacian.
sklearn.manifold
sklearn.manifold
trustworthiness
trustworthiness
Indicate to what extent the local structure is retained.
sklearn.manifold
sklearn.manifold
check_scoring
check_scoring
Determine scorer from user options.
sklearn.metrics
sklearn.metrics
get_scorer
get_scorer
Get a scorer from string.
sklearn.metrics
sklearn.metrics
get_scorer_names
get_scorer_names
Get the names of all available scorers.
sklearn.metrics
sklearn.metrics
make_scorer
make_scorer
Make a scorer from a performance metric or loss function.
sklearn.metrics
sklearn.metrics
accuracy_score
accuracy_score
Accuracy classification score.
sklearn.metrics
sklearn.metrics
auc
auc
Compute Area Under the Curve (AUC) using the trapezoidal rule.
sklearn.metrics
sklearn.metrics
average_precision_score
average_precision_score
Compute average precision (AP) from prediction scores.
sklearn.metrics
sklearn.metrics
balanced_accuracy_score
balanced_accuracy_score
Compute the balanced accuracy.
sklearn.metrics
sklearn.metrics
brier_score_loss
brier_score_loss
Compute the Brier score loss.
sklearn.metrics
sklearn.metrics
class_likelihood_ratios
class_likelihood_ratios
Compute binary classification positive and negative likelihood ratios.
sklearn.metrics
sklearn.metrics
classification_report
classification_report
Build a text report showing the main classification metrics.
sklearn.metrics
sklearn.metrics
cohen_kappa_score
cohen_kappa_score
Compute Cohen’s kappa: a statistic that measures inter-annotator agreement.
sklearn.metrics
sklearn.metrics
confusion_matrix
confusion_matrix
Compute confusion matrix to evaluate the accuracy of a classification.
sklearn.metrics
sklearn.metrics
d2_log_loss_score
d2_log_loss_score
\(D^2\)score function, fraction of log loss explained.
sklearn.metrics
sklearn.metrics
dcg_score
dcg_score
Compute Discounted Cumulative Gain.
sklearn.metrics
sklearn.metrics
det_curve
det_curve
Compute error rates for different probability thresholds.
sklearn.metrics
sklearn.metrics
f1_score
f1_score
Compute the F1 score, also known as balanced F-score or F-measure.
sklearn.metrics
sklearn.metrics
fbeta_score
fbeta_score
Compute the F-beta score.
sklearn.metrics
sklearn.metrics
hamming_loss
hamming_loss
Compute the average Hamming loss.
sklearn.metrics
sklearn.metrics
hinge_loss
hinge_loss
Average hinge loss (non-regularized).
sklearn.metrics
sklearn.metrics
jaccard_score
jaccard_score
Jaccard similarity coefficient score.
sklearn.metrics
sklearn.metrics
log_loss
log_loss
Log loss, aka logistic loss or cross-entropy loss.
sklearn.metrics
sklearn.metrics
matthews_corrcoef
matthews_corrcoef
Compute the Matthews correlation coefficient (MCC).
sklearn.metrics
sklearn.metrics
multilabel_confusion_matrix
multilabel_confusion_matrix
Compute a confusion matrix for each class or sample.
sklearn.metrics
sklearn.metrics
ndcg_score
ndcg_score
Compute Normalized Discounted Cumulative Gain.
sklearn.metrics
sklearn.metrics
precision_recall_curve
precision_recall_curve
Compute precision-recall pairs for different probability thresholds.
sklearn.metrics
sklearn.metrics
precision_recall_fscore_support
precision_recall_fscore_support
Compute precision, recall, F-measure and support for each class.
sklearn.metrics
sklearn.metrics
precision_score
precision_score
Compute the precision.
sklearn.metrics
sklearn.metrics
recall_score
recall_score
Compute the recall.
sklearn.metrics
sklearn.metrics
roc_auc_score
roc_auc_score
Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC)     from prediction scores.
sklearn.metrics
sklearn.metrics
roc_curve
roc_curve
Compute Receiver operating characteristic (ROC).
sklearn.metrics
sklearn.metrics
top_k_accuracy_score
top_k_accuracy_score
Top-k Accuracy classification score.
sklearn.metrics
sklearn.metrics
zero_one_loss
zero_one_loss
Zero-one classification loss.
sklearn.metrics
sklearn.metrics
d2_absolute_error_score
d2_absolute_error_score
\(D^2\)regression score function, fraction of absolute error explained.
sklearn.metrics
sklearn.metrics
d2_pinball_score
d2_pinball_score
\(D^2\)regression score function, fraction of pinball loss explained.
sklearn.metrics
sklearn.metrics
d2_tweedie_score
d2_tweedie_score
\(D^2\)regression score function, fraction of Tweedie deviance explained.
sklearn.metrics
sklearn.metrics
explained_variance_score
explained_variance_score
Explained variance regression score function.
sklearn.metrics
sklearn.metrics
max_error
max_error
The max_error metric calculates the maximum residual error.
sklearn.metrics
sklearn.metrics
mean_absolute_error
mean_absolute_error
Mean absolute error regression loss.
sklearn.metrics
sklearn.metrics
mean_absolute_percentage_error
mean_absolute_percentage_error
Mean absolute percentage error (MAPE) regression loss.
sklearn.metrics
sklearn.metrics
mean_gamma_deviance
mean_gamma_deviance
Mean Gamma deviance regression loss.
sklearn.metrics
sklearn.metrics
mean_pinball_loss
mean_pinball_loss
Pinball loss for quantile regression.
sklearn.metrics
sklearn.metrics
mean_poisson_deviance
mean_poisson_deviance
Mean Poisson deviance regression loss.
sklearn.metrics
sklearn.metrics
mean_squared_error
mean_squared_error
Mean squared error regression loss.
sklearn.metrics
sklearn.metrics
mean_squared_log_error
mean_squared_log_error
Mean squared logarithmic error regression loss.
sklearn.metrics
sklearn.metrics
mean_tweedie_deviance
mean_tweedie_deviance
Mean Tweedie deviance regression loss.
sklearn.metrics
sklearn.metrics
median_absolute_error
median_absolute_error
Median absolute error regression loss.
sklearn.metrics
sklearn.metrics
r2_score
r2_score
\(R^2\)(coefficient of determination) regression score function.
sklearn.metrics
sklearn.metrics
root_mean_squared_error
root_mean_squared_error
Root mean squared error regression loss.
sklearn.metrics
sklearn.metrics
root_mean_squared_log_error
root_mean_squared_log_error
Root mean squared logarithmic error regression loss.
sklearn.metrics
sklearn.metrics
coverage_error
coverage_error
Coverage error measure.
sklearn.metrics
sklearn.metrics
label_ranking_average_precision_score
label_ranking_average_precision_score
Compute ranking-based average precision.
sklearn.metrics
sklearn.metrics
label_ranking_loss
label_ranking_loss
Compute Ranking loss measure.
sklearn.metrics
sklearn.metrics
adjusted_mutual_info_score
adjusted_mutual_info_score
Adjusted Mutual Information between two clusterings.
sklearn.metrics
sklearn.metrics
adjusted_rand_score
adjusted_rand_score
Rand index adjusted for chance.
sklearn.metrics
sklearn.metrics
calinski_harabasz_score
calinski_harabasz_score
Compute the Calinski and Harabasz score.
sklearn.metrics
sklearn.metrics
contingency_matrix
contingency_matrix
Build a contingency matrix describing the relationship between labels.
sklearn.metrics.cluster
sklearn.metrics.cluster
pair_confusion_matrix
pair_confusion_matrix
Pair confusion matrix arising from two clusterings.
sklearn.metrics.cluster
sklearn.metrics.cluster
completeness_score
completeness_score
Compute completeness metric of a cluster labeling given a ground truth.
sklearn.metrics
sklearn.metrics
davies_bouldin_score
davies_bouldin_score
Compute the Davies-Bouldin score.
sklearn.metrics
sklearn.metrics
fowlkes_mallows_score
fowlkes_mallows_score
Measure the similarity of two clusterings of a set of points.
sklearn.metrics
sklearn.metrics
homogeneity_completeness_v_measure
homogeneity_completeness_v_measure
Compute the homogeneity and completeness and V-Measure scores at once.
sklearn.metrics
sklearn.metrics
homogeneity_score
homogeneity_score
Homogeneity metric of a cluster labeling given a ground truth.
sklearn.metrics
sklearn.metrics
mutual_info_score
mutual_info_score
Mutual Information between two clusterings.
sklearn.metrics
sklearn.metrics
normalized_mutual_info_score
normalized_mutual_info_score
Normalized Mutual Information between two clusterings.
sklearn.metrics
sklearn.metrics
rand_score
rand_score
Rand index.
sklearn.metrics
sklearn.metrics
silhouette_samples
silhouette_samples
Compute the Silhouette Coefficient for each sample.
sklearn.metrics
sklearn.metrics
silhouette_score
silhouette_score
Compute the mean Silhouette Coefficient of all samples.
sklearn.metrics
sklearn.metrics
v_measure_score
v_measure_score
V-measure cluster labeling given a ground truth.
sklearn.metrics
sklearn.metrics
consensus_score
consensus_score
The similarity of two sets of biclusters.
sklearn.metrics
sklearn.metrics
DistanceMetric
DistanceMetric
Uniform interface for fast distance metric functions.
sklearn.metrics
sklearn.metrics
additive_chi2_kernel
additive_chi2_kernel
Compute the additive chi-squared kernel between observations in X and Y.
sklearn.metrics.pairwise
sklearn.metrics.pairwise
chi2_kernel
chi2_kernel
Compute the exponential chi-squared kernel between X and Y.
sklearn.metrics.pairwise
sklearn.metrics.pairwise
cosine_distances
cosine_distances
Compute cosine distance between samples in X and Y.
sklearn.metrics.pairwise
sklearn.metrics.pairwise
cosine_similarity
cosine_similarity
Compute cosine similarity between samples in X and Y.
sklearn.metrics.pairwise
sklearn.metrics.pairwise
distance_metrics
distance_metrics
Valid metrics for pairwise_distances.
sklearn.metrics.pairwise
sklearn.metrics.pairwise
euclidean_distances
euclidean_distances
Compute the distance matrix between each pair from a vector array X and Y.
sklearn.metrics.pairwise
sklearn.metrics.pairwise
haversine_distances
haversine_distances
Compute the Haversine distance between samples in X and Y.
sklearn.metrics.pairwise
sklearn.metrics.pairwise
kernel_metrics
kernel_metrics
Valid metrics for pairwise_kernels.
sklearn.metrics.pairwise
sklearn.metrics.pairwise
laplacian_kernel
laplacian_kernel
Compute the laplacian kernel between X and Y.
sklearn.metrics.pairwise
sklearn.metrics.pairwise
linear_kernel
linear_kernel
Compute the linear kernel between X and Y.
sklearn.metrics.pairwise
sklearn.metrics.pairwise
manhattan_distances
manhattan_distances
Compute the L1 distances between the vectors in X and Y.
sklearn.metrics.pairwise
sklearn.metrics.pairwise
nan_euclidean_distances
nan_euclidean_distances
Calculate the euclidean distances in the presence of missing values.
sklearn.metrics.pairwise
sklearn.metrics.pairwise
paired_cosine_distances
paired_cosine_distances
Compute the paired cosine distances between X and Y.
sklearn.metrics.pairwise
sklearn.metrics.pairwise
paired_distances
paired_distances
Compute the paired distances between X and Y.
sklearn.metrics.pairwise
sklearn.metrics.pairwise
paired_euclidean_distances
paired_euclidean_distances
Compute the paired euclidean distances between X and Y.
sklearn.metrics.pairwise
sklearn.metrics.pairwise
paired_manhattan_distances
paired_manhattan_distances
Compute the paired L1 distances between X and Y.
sklearn.metrics.pairwise
sklearn.metrics.pairwise
pairwise_kernels
pairwise_kernels
Compute the kernel between arrays X and optional array Y.
sklearn.metrics.pairwise
sklearn.metrics.pairwise
polynomial_kernel
polynomial_kernel
Compute the polynomial kernel between X and Y.
sklearn.metrics.pairwise
sklearn.metrics.pairwise
rbf_kernel
rbf_kernel
Compute the rbf (gaussian) kernel between X and Y.
sklearn.metrics.pairwise
sklearn.metrics.pairwise
sigmoid_kernel
sigmoid_kernel
Compute the sigmoid kernel between X and Y.
sklearn.metrics.pairwise
sklearn.metrics.pairwise
pairwise_distances
pairwise_distances
Compute the distance matrix from a vector array X and optional Y.
sklearn.metrics
sklearn.metrics
pairwise_distances_argmin
pairwise_distances_argmin
Compute minimum distances between one point and a set of points.
sklearn.metrics
sklearn.metrics
pairwise_distances_argmin_min
pairwise_distances_argmin_min
Compute minimum distances between one point and a set of points.
sklearn.metrics
sklearn.metrics
pairwise_distances_chunked
pairwise_distances_chunked
Generate a distance matrix chunk by chunk with optional reduction.
sklearn.metrics
sklearn.metrics
ConfusionMatrixDisplay
ConfusionMatrixDisplay
Confusion Matrix visualization.
sklearn.metrics
sklearn.metrics
DetCurveDisplay
DetCurveDisplay
DET curve visualization.
sklearn.metrics
sklearn.metrics
PrecisionRecallDisplay
PrecisionRecallDisplay
Precision Recall visualization.
sklearn.metrics
sklearn.metrics
PredictionErrorDisplay
PredictionErrorDisplay
Visualization of the prediction error of a regression model.
sklearn.metrics
sklearn.metrics
RocCurveDisplay
RocCurveDisplay
ROC Curve visualization.
sklearn.metrics
sklearn.metrics
BayesianGaussianMixture
BayesianGaussianMixture
Variational Bayesian estimation of a Gaussian mixture.
sklearn.mixture
sklearn.mixture
GaussianMixture
GaussianMixture
Gaussian Mixture.
sklearn.mixture
sklearn.mixture
GroupKFold
GroupKFold
K-fold iterator variant with non-overlapping groups.
sklearn.model_selection
sklearn.model_selection
GroupShuffleSplit
GroupShuffleSplit
Shuffle-Group(s)-Out cross-validation iterator.
sklearn.model_selection
sklearn.model_selection
KFold
KFold
K-Fold cross-validator.
sklearn.model_selection
sklearn.model_selection
LeaveOneGroupOut
LeaveOneGroupOut
Leave One Group Out cross-validator.
sklearn.model_selection
sklearn.model_selection
LeaveOneOut
LeaveOneOut
Leave-One-Out cross-validator.
sklearn.model_selection
sklearn.model_selection
LeavePGroupsOut
LeavePGroupsOut
Leave P Group(s) Out cross-validator.
sklearn.model_selection
sklearn.model_selection
LeavePOut
LeavePOut
Leave-P-Out cross-validator.
sklearn.model_selection
sklearn.model_selection
PredefinedSplit
PredefinedSplit
Predefined split cross-validator.
sklearn.model_selection
sklearn.model_selection
RepeatedKFold
RepeatedKFold
Repeated K-Fold cross validator.
sklearn.model_selection
sklearn.model_selection
RepeatedStratifiedKFold
RepeatedStratifiedKFold
Repeated Stratified K-Fold cross validator.
sklearn.model_selection
sklearn.model_selection
ShuffleSplit
ShuffleSplit
Random permutation cross-validator.
sklearn.model_selection
sklearn.model_selection
StratifiedGroupKFold
StratifiedGroupKFold
Stratified K-Fold iterator variant with non-overlapping groups.
sklearn.model_selection
sklearn.model_selection
StratifiedKFold
StratifiedKFold
Stratified K-Fold cross-validator.
sklearn.model_selection
sklearn.model_selection
StratifiedShuffleSplit
StratifiedShuffleSplit
Stratified ShuffleSplit cross-validator.
sklearn.model_selection
sklearn.model_selection
TimeSeriesSplit
TimeSeriesSplit
Time Series cross-validator.
sklearn.model_selection
sklearn.model_selection
check_cv
check_cv
Input checker utility for building a cross-validator.
sklearn.model_selection
sklearn.model_selection
train_test_split
train_test_split
Split arrays or matrices into random train and test subsets.
sklearn.model_selection
sklearn.model_selection
GridSearchCV
GridSearchCV
Exhaustive search over specified parameter values for an estimator.
sklearn.model_selection
sklearn.model_selection
HalvingGridSearchCV
HalvingGridSearchCV
Search over specified parameter values with successive halving.
sklearn.model_selection
sklearn.model_selection
HalvingRandomSearchCV
HalvingRandomSearchCV
Randomized search on hyper parameters.
sklearn.model_selection
sklearn.model_selection
ParameterGrid
ParameterGrid
Grid of parameters with a discrete number of values for each.
sklearn.model_selection
sklearn.model_selection
ParameterSampler
ParameterSampler
Generator on parameters sampled from given distributions.
sklearn.model_selection
sklearn.model_selection
RandomizedSearchCV
RandomizedSearchCV
Randomized search on hyper parameters.
sklearn.model_selection
sklearn.model_selection
FixedThresholdClassifier
FixedThresholdClassifier
Binary classifier that manually sets the decision threshold.
sklearn.model_selection
sklearn.model_selection
TunedThresholdClassifierCV
TunedThresholdClassifierCV
Classifier that post-tunes the decision threshold using cross-validation.
sklearn.model_selection
sklearn.model_selection
cross_val_predict
cross_val_predict
Generate cross-validated estimates for each input data point.
sklearn.model_selection
sklearn.model_selection
cross_val_score
cross_val_score
Evaluate a score by cross-validation.
sklearn.model_selection
sklearn.model_selection
cross_validate
cross_validate
Evaluate metric(s) by cross-validation and also record fit/score times.
sklearn.model_selection
sklearn.model_selection
learning_curve
learning_curve
Learning curve.
sklearn.model_selection
sklearn.model_selection
permutation_test_score
permutation_test_score
Evaluate the significance of a cross-validated score with permutations.
sklearn.model_selection
sklearn.model_selection
validation_curve
validation_curve
Validation curve.
sklearn.model_selection
sklearn.model_selection
LearningCurveDisplay
LearningCurveDisplay
Learning Curve visualization.
sklearn.model_selection
sklearn.model_selection
ValidationCurveDisplay
ValidationCurveDisplay
Validation Curve visualization.
sklearn.model_selection
sklearn.model_selection
OneVsOneClassifier
OneVsOneClassifier
One-vs-one multiclass strategy.
sklearn.multiclass
sklearn.multiclass
OneVsRestClassifier
OneVsRestClassifier
One-vs-the-rest (OvR) multiclass strategy.
sklearn.multiclass
sklearn.multiclass
OutputCodeClassifier
OutputCodeClassifier
(Error-Correcting) Output-Code multiclass strategy.
sklearn.multiclass
sklearn.multiclass
ClassifierChain
ClassifierChain
A multi-label model that arranges binary classifiers into a chain.
sklearn.multioutput
sklearn.multioutput
MultiOutputClassifier
MultiOutputClassifier
Multi target classification.
sklearn.multioutput
sklearn.multioutput
MultiOutputRegressor
MultiOutputRegressor
Multi target regression.
sklearn.multioutput
sklearn.multioutput
RegressorChain
RegressorChain
A multi-label model that arranges regressions into a chain.
sklearn.multioutput
sklearn.multioutput
BernoulliNB
BernoulliNB
Naive Bayes classifier for multivariate Bernoulli models.
sklearn.naive_bayes
sklearn.naive_bayes
CategoricalNB
CategoricalNB
Naive Bayes classifier for categorical features.
sklearn.naive_bayes
sklearn.naive_bayes
ComplementNB
ComplementNB
The Complement Naive Bayes classifier described in Rennie et al. (2003).
sklearn.naive_bayes
sklearn.naive_bayes
GaussianNB
GaussianNB
Gaussian Naive Bayes (GaussianNB).
sklearn.naive_bayes
sklearn.naive_bayes
MultinomialNB
MultinomialNB
Naive Bayes classifier for multinomial models.
sklearn.naive_bayes
sklearn.naive_bayes
BallTree
BallTree
BallTree for fast generalized N-point problems
sklearn.neighbors
sklearn.neighbors
KDTree
KDTree
KDTree for fast generalized N-point problems
sklearn.neighbors
sklearn.neighbors
KNeighborsClassifier
KNeighborsClassifier
Classifier implementing the k-nearest neighbors vote.
sklearn.neighbors
sklearn.neighbors
KNeighborsRegressor
KNeighborsRegressor
Regression based on k-nearest neighbors.
sklearn.neighbors
sklearn.neighbors
KNeighborsTransformer
KNeighborsTransformer
Transform X into a (weighted) graph of k nearest neighbors.
sklearn.neighbors
sklearn.neighbors
KernelDensity
KernelDensity
Kernel Density Estimation.
sklearn.neighbors
sklearn.neighbors
LocalOutlierFactor
LocalOutlierFactor
Unsupervised Outlier Detection using the Local Outlier Factor (LOF).
sklearn.neighbors
sklearn.neighbors
NearestCentroid
NearestCentroid
Nearest centroid classifier.
sklearn.neighbors
sklearn.neighbors
NearestNeighbors
NearestNeighbors
Unsupervised learner for implementing neighbor searches.
sklearn.neighbors
sklearn.neighbors
NeighborhoodComponentsAnalysis
NeighborhoodComponentsAnalysis
Neighborhood Components Analysis.
sklearn.neighbors
sklearn.neighbors
RadiusNeighborsClassifier
RadiusNeighborsClassifier
Classifier implementing a vote among neighbors within a given radius.
sklearn.neighbors
sklearn.neighbors
RadiusNeighborsRegressor
RadiusNeighborsRegressor
Regression based on neighbors within a fixed radius.
sklearn.neighbors
sklearn.neighbors
RadiusNeighborsTransformer
RadiusNeighborsTransformer
Transform X into a (weighted) graph of neighbors nearer than a radius.
sklearn.neighbors
sklearn.neighbors
kneighbors_graph
kneighbors_graph
Compute the (weighted) graph of k-Neighbors for points in X.
sklearn.neighbors
sklearn.neighbors
radius_neighbors_graph
radius_neighbors_graph
Compute the (weighted) graph of Neighbors for points in X.
sklearn.neighbors
sklearn.neighbors
sort_graph_by_row_values
sort_graph_by_row_values
Sort a sparse graph such that each row is stored with increasing values.
sklearn.neighbors
sklearn.neighbors
BernoulliRBM
BernoulliRBM
Bernoulli Restricted Boltzmann Machine (RBM).
sklearn.neural_network
sklearn.neural_network
MLPClassifier
MLPClassifier
Multi-layer Perceptron classifier.
sklearn.neural_network
sklearn.neural_network
MLPRegressor
MLPRegressor
Multi-layer Perceptron regressor.
sklearn.neural_network
sklearn.neural_network
FeatureUnion
FeatureUnion
Concatenates results of multiple transformer objects.
sklearn.pipeline
sklearn.pipeline
Pipeline
Pipeline
A sequence of data transformers with an optional final predictor.
sklearn.pipeline
sklearn.pipeline
make_pipeline
make_pipeline
Construct aPipelinefrom the given estimators.
Pipeline
sklearn.pipeline
sklearn.pipeline
make_union
make_union
Construct aFeatureUnionfrom the given transformers.
FeatureUnion
sklearn.pipeline
sklearn.pipeline
Binarizer
Binarizer
Binarize data (set feature values to 0 or 1) according to a threshold.
sklearn.preprocessing
sklearn.preprocessing
FunctionTransformer
FunctionTransformer
Constructs a transformer from an arbitrary callable.
sklearn.preprocessing
sklearn.preprocessing
KBinsDiscretizer
KBinsDiscretizer
Bin continuous data into intervals.
sklearn.preprocessing
sklearn.preprocessing
KernelCenterer
KernelCenterer
Center an arbitrary kernel matrix\(K\).
sklearn.preprocessing
sklearn.preprocessing
LabelBinarizer
LabelBinarizer
Binarize labels in a one-vs-all fashion.
sklearn.preprocessing
sklearn.preprocessing
LabelEncoder
LabelEncoder
Encode target labels with value between 0 and n_classes-1.
sklearn.preprocessing
sklearn.preprocessing
MaxAbsScaler
MaxAbsScaler
Scale each feature by its maximum absolute value.
sklearn.preprocessing
sklearn.preprocessing
MinMaxScaler
MinMaxScaler
Transform features by scaling each feature to a given range.
sklearn.preprocessing
sklearn.preprocessing
MultiLabelBinarizer
MultiLabelBinarizer
Transform between iterable of iterables and a multilabel format.
sklearn.preprocessing
sklearn.preprocessing
Normalizer
Normalizer
Normalize samples individually to unit norm.
sklearn.preprocessing
sklearn.preprocessing
OneHotEncoder
OneHotEncoder
Encode categorical features as a one-hot numeric array.
sklearn.preprocessing
sklearn.preprocessing
OrdinalEncoder
OrdinalEncoder
Encode categorical features as an integer array.
sklearn.preprocessing
sklearn.preprocessing
PolynomialFeatures
PolynomialFeatures
Generate polynomial and interaction features.
sklearn.preprocessing
sklearn.preprocessing
PowerTransformer
PowerTransformer
Apply a power transform featurewise to make data more Gaussian-like.
sklearn.preprocessing
sklearn.preprocessing
QuantileTransformer
QuantileTransformer
Transform features using quantiles information.
sklearn.preprocessing
sklearn.preprocessing
RobustScaler
RobustScaler
Scale features using statistics that are robust to outliers.
sklearn.preprocessing
sklearn.preprocessing
SplineTransformer
SplineTransformer
Generate univariate B-spline bases for features.
sklearn.preprocessing
sklearn.preprocessing
StandardScaler
StandardScaler
Standardize features by removing the mean and scaling to unit variance.
sklearn.preprocessing
sklearn.preprocessing
TargetEncoder
TargetEncoder
Target Encoder for regression and classification targets.
sklearn.preprocessing
sklearn.preprocessing
add_dummy_feature
add_dummy_feature
Augment dataset with an additional dummy feature.
sklearn.preprocessing
sklearn.preprocessing
binarize
binarize
Boolean thresholding of array-like or scipy.sparse matrix.
sklearn.preprocessing
sklearn.preprocessing
label_binarize
label_binarize
Binarize labels in a one-vs-all fashion.
sklearn.preprocessing
sklearn.preprocessing
maxabs_scale
maxabs_scale
Scale each feature to the [-1, 1] range without breaking the sparsity.
sklearn.preprocessing
sklearn.preprocessing
minmax_scale
minmax_scale
Transform features by scaling each feature to a given range.
sklearn.preprocessing
sklearn.preprocessing
normalize
normalize
Scale input vectors individually to unit norm (vector length).
sklearn.preprocessing
sklearn.preprocessing
power_transform
power_transform
Parametric, monotonic transformation to make data more Gaussian-like.
sklearn.preprocessing
sklearn.preprocessing
quantile_transform
quantile_transform
Transform features using quantiles information.
sklearn.preprocessing
sklearn.preprocessing
robust_scale
robust_scale
Standardize a dataset along any axis.
sklearn.preprocessing
sklearn.preprocessing
scale
scale
Standardize a dataset along any axis.
sklearn.preprocessing
sklearn.preprocessing
GaussianRandomProjection
GaussianRandomProjection
Reduce dimensionality through Gaussian random projection.
sklearn.random_projection
sklearn.random_projection
SparseRandomProjection
SparseRandomProjection
Reduce dimensionality through sparse random projection.
sklearn.random_projection
sklearn.random_projection
johnson_lindenstrauss_min_dim
johnson_lindenstrauss_min_dim
Find a ‘safe’ number of components to randomly project to.
sklearn.random_projection
sklearn.random_projection
LabelPropagation
LabelPropagation
Label Propagation classifier.
sklearn.semi_supervised
sklearn.semi_supervised
LabelSpreading
LabelSpreading
LabelSpreading model for semi-supervised learning.
sklearn.semi_supervised
sklearn.semi_supervised
SelfTrainingClassifier
SelfTrainingClassifier
Self-training classifier.
sklearn.semi_supervised
sklearn.semi_supervised
LinearSVC
LinearSVC
Linear Support Vector Classification.
sklearn.svm
sklearn.svm
LinearSVR
LinearSVR
Linear Support Vector Regression.
sklearn.svm
sklearn.svm
NuSVC
NuSVC
Nu-Support Vector Classification.
sklearn.svm
sklearn.svm
NuSVR
NuSVR
Nu Support Vector Regression.
sklearn.svm
sklearn.svm
OneClassSVM
OneClassSVM
Unsupervised Outlier Detection.
sklearn.svm
sklearn.svm
SVC
SVC
C-Support Vector Classification.
sklearn.svm
sklearn.svm
SVR
SVR
Epsilon-Support Vector Regression.
sklearn.svm
sklearn.svm
l1_min_c
l1_min_c
Return the lowest bound for C.
sklearn.svm
sklearn.svm
DecisionTreeClassifier
DecisionTreeClassifier
A decision tree classifier.
sklearn.tree
sklearn.tree
DecisionTreeRegressor
DecisionTreeRegressor
A decision tree regressor.
sklearn.tree
sklearn.tree
ExtraTreeClassifier
ExtraTreeClassifier
An extremely randomized tree classifier.
sklearn.tree
sklearn.tree
ExtraTreeRegressor
ExtraTreeRegressor
An extremely randomized tree regressor.
sklearn.tree
sklearn.tree
export_graphviz
export_graphviz
Export a decision tree in DOT format.
sklearn.tree
sklearn.tree
export_text
export_text
Build a text report showing the rules of a decision tree.
sklearn.tree
sklearn.tree
plot_tree
plot_tree
Plot a decision tree.
sklearn.tree
sklearn.tree
Bunch
Bunch
Container object exposing keys as attributes.
sklearn.utils
sklearn.utils
_safe_indexing
_safe_indexing
Return rows, items or columns of X using indices.
sklearn.utils
sklearn.utils
as_float_array
as_float_array
Convert an array-like to an array of floats.
sklearn.utils
sklearn.utils
assert_all_finite
assert_all_finite
Throw a ValueError if X contains NaN or infinity.
sklearn.utils
sklearn.utils
deprecated
deprecated
Decorator to mark a function or class as deprecated.
sklearn.utils
sklearn.utils
estimator_html_repr
estimator_html_repr
Build a HTML representation of an estimator.
sklearn.utils
sklearn.utils
gen_batches
gen_batches
Generator to create slices containingbatch_sizeelements from 0 ton.
batch_size
n
sklearn.utils
sklearn.utils
gen_even_slices
gen_even_slices
Generator to createn_packsevenly spaced slices going up ton.
n_packs
n
sklearn.utils
sklearn.utils
indexable
indexable
Make arrays indexable for cross-validation.
sklearn.utils
sklearn.utils
murmurhash3_32
murmurhash3_32
Compute the 32bit murmurhash3 of key at seed.
sklearn.utils
sklearn.utils
resample
resample
Resample arrays or sparse matrices in a consistent way.
sklearn.utils
sklearn.utils
safe_mask
safe_mask
Return a mask which is safe to use on X.
sklearn.utils
sklearn.utils
safe_sqr
safe_sqr
Element wise squaring of array-likes and sparse matrices.
sklearn.utils
sklearn.utils
shuffle
shuffle
Shuffle arrays or sparse matrices in a consistent way.
sklearn.utils
sklearn.utils
Tags
Tags
Tags for the estimator.
sklearn.utils
sklearn.utils
InputTags
InputTags
Tags for the input data.
sklearn.utils
sklearn.utils
TargetTags
TargetTags
Tags for the target data.
sklearn.utils
sklearn.utils
ClassifierTags
ClassifierTags
Tags for the classifier.
sklearn.utils
sklearn.utils
RegressorTags
RegressorTags
Tags for the regressor.
sklearn.utils
sklearn.utils
TransformerTags
TransformerTags
Tags for the transformer.
sklearn.utils
sklearn.utils
get_tags
get_tags
Get estimator tags.
sklearn.utils
sklearn.utils
check_X_y
check_X_y
Input validation for standard estimators.
sklearn.utils
sklearn.utils
check_array
check_array
Input validation on an array, list, sparse matrix or similar.
sklearn.utils
sklearn.utils
check_consistent_length
check_consistent_length
Check that all arrays have consistent first dimensions.
sklearn.utils
sklearn.utils
check_random_state
check_random_state
Turn seed into a np.random.RandomState instance.
sklearn.utils
sklearn.utils
check_scalar
check_scalar
Validate scalar parameters type and value.
sklearn.utils
sklearn.utils
check_is_fitted
check_is_fitted
Perform is_fitted validation for estimator.
sklearn.utils.validation
sklearn.utils.validation
check_memory
check_memory
Check thatmemoryis joblib.Memory-like.
memory
sklearn.utils.validation
sklearn.utils.validation
check_symmetric
check_symmetric
Make sure that array is 2D, square and symmetric.
sklearn.utils.validation
sklearn.utils.validation
column_or_1d
column_or_1d
Ravel column or 1d numpy array, else raises an error.
sklearn.utils.validation
sklearn.utils.validation
has_fit_parameter
has_fit_parameter
Check whether the estimator’s fit method supports the given parameter.
sklearn.utils.validation
sklearn.utils.validation
validate_data
validate_data
Validate input data and set or check feature names and counts of the input.
sklearn.utils.validation
sklearn.utils.validation
available_if
available_if
An attribute that is available only if check returns a truthy value.
sklearn.utils.metaestimators
sklearn.utils.metaestimators
compute_class_weight
compute_class_weight
Estimate class weights for unbalanced datasets.
sklearn.utils.class_weight
sklearn.utils.class_weight
compute_sample_weight
compute_sample_weight
Estimate sample weights by class for unbalanced datasets.
sklearn.utils.class_weight
sklearn.utils.class_weight
is_multilabel
is_multilabel
Check ifyis in a multilabel format.
y
sklearn.utils.multiclass
sklearn.utils.multiclass
type_of_target
type_of_target
Determine the type of data indicated by the target.
sklearn.utils.multiclass
sklearn.utils.multiclass
unique_labels
unique_labels
Extract an ordered array of unique labels.
sklearn.utils.multiclass
sklearn.utils.multiclass
density
density
Compute density of a sparse vector.
sklearn.utils.extmath
sklearn.utils.extmath
fast_logdet
fast_logdet
Compute logarithm of determinant of a square matrix.
sklearn.utils.extmath
sklearn.utils.extmath
randomized_range_finder
randomized_range_finder
Compute an orthonormal matrix whose range approximates the range of A.
sklearn.utils.extmath
sklearn.utils.extmath
randomized_svd
randomized_svd
Compute a truncated randomized SVD.
sklearn.utils.extmath
sklearn.utils.extmath
safe_sparse_dot
safe_sparse_dot
Dot product that handle the sparse matrix case correctly.
sklearn.utils.extmath
sklearn.utils.extmath
weighted_mode
weighted_mode
Return an array of the weighted modal (most common) value in the passed array.
sklearn.utils.extmath
sklearn.utils.extmath
incr_mean_variance_axis
incr_mean_variance_axis
Compute incremental mean and variance along an axis on a CSR or CSC matrix.
sklearn.utils.sparsefuncs
sklearn.utils.sparsefuncs
inplace_column_scale
inplace_column_scale
Inplace column scaling of a CSC/CSR matrix.
sklearn.utils.sparsefuncs
sklearn.utils.sparsefuncs
inplace_csr_column_scale
inplace_csr_column_scale
Inplace column scaling of a CSR matrix.
sklearn.utils.sparsefuncs
sklearn.utils.sparsefuncs
inplace_row_scale
inplace_row_scale
Inplace row scaling of a CSR or CSC matrix.
sklearn.utils.sparsefuncs
sklearn.utils.sparsefuncs
inplace_swap_column
inplace_swap_column
Swap two columns of a CSC/CSR matrix in-place.
sklearn.utils.sparsefuncs
sklearn.utils.sparsefuncs
inplace_swap_row
inplace_swap_row
Swap two rows of a CSC/CSR matrix in-place.
sklearn.utils.sparsefuncs
sklearn.utils.sparsefuncs
mean_variance_axis
mean_variance_axis
Compute mean and variance along an axis on a CSR or CSC matrix.
sklearn.utils.sparsefuncs
sklearn.utils.sparsefuncs
inplace_csr_row_normalize_l1
inplace_csr_row_normalize_l1
Normalize inplace the rows of a CSR matrix or array by their L1 norm.
sklearn.utils.sparsefuncs_fast
sklearn.utils.sparsefuncs_fast
inplace_csr_row_normalize_l2
inplace_csr_row_normalize_l2
Normalize inplace the rows of a CSR matrix or array by their L2 norm.
sklearn.utils.sparsefuncs_fast
sklearn.utils.sparsefuncs_fast
single_source_shortest_path_length
single_source_shortest_path_length
Return the length of the shortest path from source to all reachable nodes.
sklearn.utils.graph
sklearn.utils.graph
sample_without_replacement
sample_without_replacement
Sample integers without replacement.
sklearn.utils.random
sklearn.utils.random
min_pos
min_pos
Find the minimum value of an array over positive values.
sklearn.utils.arrayfuncs
sklearn.utils.arrayfuncs
MetadataRequest
MetadataRequest
Contains the metadata request info of a consumer.
sklearn.utils.metadata_routing
sklearn.utils.metadata_routing
MetadataRouter
MetadataRouter
Stores and handles metadata routing for a router object.
sklearn.utils.metadata_routing
sklearn.utils.metadata_routing
MethodMapping
MethodMapping
Stores the mapping between caller and callee methods for a router.
sklearn.utils.metadata_routing
sklearn.utils.metadata_routing
get_routing_for_object
get_routing_for_object
Get aMetadata{Router,Request}instance from the given object.
Metadata{Router,Request}
sklearn.utils.metadata_routing
sklearn.utils.metadata_routing
process_routing
process_routing
Validate and route input parameters.
sklearn.utils.metadata_routing
sklearn.utils.metadata_routing
all_displays
all_displays
Get a list of all displays fromsklearn.
sklearn
sklearn.utils.discovery
sklearn.utils.discovery
all_estimators
all_estimators
Get a list of all estimators fromsklearn.
sklearn
sklearn.utils.discovery
sklearn.utils.discovery
all_functions
all_functions
Get a list of all functions fromsklearn.
sklearn
sklearn.utils.discovery
sklearn.utils.discovery
check_estimator
check_estimator
Check if estimator adheres to scikit-learn conventions.
sklearn.utils.estimator_checks
sklearn.utils.estimator_checks
parametrize_with_checks
parametrize_with_checks
Pytest specific decorator for parametrizing estimator checks.
sklearn.utils.estimator_checks
sklearn.utils.estimator_checks
estimator_checks_generator
estimator_checks_generator
Iteratively yield all check callables for an estimator.
sklearn.utils.estimator_checks
sklearn.utils.estimator_checks
Parallel
Parallel
Tweak ofjoblib.Parallelthat propagates the scikit-learn configuration.
joblib.Parallel
sklearn.utils.parallel
sklearn.utils.parallel
delayed
delayed
Decorator used to capture the arguments of a function.
sklearn.utils.parallel
sklearn.utils.parallel
parallel_backend
parallel_backend
Change the default backend used by Parallel inside a with block.
sklearn.utilsDeprecated in version 1.7
sklearn.utils
register_parallel_backend
register_parallel_backend
Register a new Parallel backend factory.
sklearn.utilsDeprecated in version 1.7
sklearn.utils

GitHub
GitHub
Examples#
This is the gallery of examples that showcase how scikit-learn can be used. Some
examples demonstrate the use of theAPIin general and some
demonstrate specific applications in tutorial form. Also check out ouruser guidefor more detailed illustrations.
Release Highlights#
These examples illustrate the main features of the releases of scikit-learn.
Release Highlights for scikit-learn 1.6
Release Highlights for scikit-learn 1.5
Release Highlights for scikit-learn 1.4
Release Highlights for scikit-learn 1.3
Release Highlights for scikit-learn 1.2
Release Highlights for scikit-learn 1.1
Release Highlights for scikit-learn 1.0
Release Highlights for scikit-learn 0.24
Release Highlights for scikit-learn 0.23
Release Highlights for scikit-learn 0.22
Biclustering#
Examples concerning biclustering techniques.
A demo of the Spectral Biclustering algorithm
A demo of the Spectral Co-Clustering algorithm
Biclustering documents with the Spectral Co-clustering algorithm
Calibration#
Examples illustrating the calibration of predicted probabilities of classifiers.
Comparison of Calibration of Classifiers
Probability Calibration curves
Probability Calibration for 3-class classification
Probability calibration of classifiers
Classification#
General examples about classification algorithms.
Classifier comparison
Linear and Quadratic Discriminant Analysis with covariance ellipsoid
Normal, Ledoit-Wolf and OAS Linear Discriminant Analysis for classification
Plot classification probability
Recognizing hand-written digits
Clustering#
Examples concerning thesklearn.clustermodule.
sklearn.cluster
A demo of K-Means clustering on the handwritten digits data
A demo of structured Ward hierarchical clustering on an image of coins
A demo of the mean-shift clustering algorithm
Adjustment for chance in clustering performance evaluation
Agglomerative clustering with and without structure
Agglomerative clustering with different metrics
An example of K-Means++ initialization
Bisecting K-Means and Regular K-Means Performance Comparison
Compare BIRCH and MiniBatchKMeans
Comparing different clustering algorithms on toy datasets
Comparing different hierarchical linkage methods on toy datasets
Comparison of the K-Means and MiniBatchKMeans clustering algorithms
Demo of DBSCAN clustering algorithm
Demo of HDBSCAN clustering algorithm
Demo of OPTICS clustering algorithm
Demo of affinity propagation clustering algorithm
Demonstration of k-means assumptions
Empirical evaluation of the impact of k-means initialization
Feature agglomeration
Feature agglomeration vs. univariate selection
Hierarchical clustering: structured vs unstructured ward
Inductive Clustering
Online learning of a dictionary of parts of faces
Plot Hierarchical Clustering Dendrogram
Segmenting the picture of greek coins in regions
Selecting the number of clusters with silhouette analysis on KMeans clustering
Spectral clustering for image segmentation
Various Agglomerative Clustering on a 2D embedding of digits
Vector Quantization Example
Covariance estimation#
Examples concerning thesklearn.covariancemodule.
sklearn.covariance
Ledoit-Wolf vs OAS estimation
Robust covariance estimation and Mahalanobis distances relevance
Robust vs Empirical covariance estimate
Shrinkage covariance estimation: LedoitWolf vs OAS and max-likelihood
Sparse inverse covariance estimation
Cross decomposition#
Examples concerning thesklearn.cross_decompositionmodule.
sklearn.cross_decomposition
Compare cross decomposition methods
Principal Component Regression vs Partial Least Squares Regression
Dataset examples#
Examples concerning thesklearn.datasetsmodule.
sklearn.datasets
Plot randomly generated multilabel dataset
Decision Trees#
Examples concerning thesklearn.treemodule.
sklearn.tree
Decision Tree Regression
Plot the decision surface of decision trees trained on the iris dataset
Post pruning decision trees with cost complexity pruning
Understanding the decision tree structure
Decomposition#
Examples concerning thesklearn.decompositionmodule.
sklearn.decomposition
Blind source separation using FastICA
Comparison of LDA and PCA 2D projection of Iris dataset
Faces dataset decompositions
Factor Analysis (with rotation) to visualize patterns
FastICA on 2D point clouds
Image denoising using dictionary learning
Incremental PCA
Kernel PCA
Model selection with Probabilistic PCA and Factor Analysis (FA)
Principal Component Analysis (PCA) on Iris Dataset
Sparse coding with a precomputed dictionary
Developing Estimators#
Examples concerning the development of Custom Estimator.
__sklearn_is_fitted__ as Developer API
Ensemble methods#
Examples concerning thesklearn.ensemblemodule.
sklearn.ensemble
Categorical Feature Support in Gradient Boosting
Combine predictors using stacking
Comparing Random Forests and Histogram Gradient Boosting models
Comparing random forests and the multi-output meta estimator
Decision Tree Regression with AdaBoost
Early stopping in Gradient Boosting
Feature importances with a forest of trees
Feature transformations with ensembles of trees
Features in Histogram Gradient Boosting Trees
Gradient Boosting Out-of-Bag estimates
Gradient Boosting regression
Gradient Boosting regularization
Hashing feature transformation using Totally Random Trees
IsolationForest example
Monotonic Constraints
Multi-class AdaBoosted Decision Trees
OOB Errors for Random Forests
Plot class probabilities calculated by the VotingClassifier
Plot individual and voting regression predictions
Plot the decision boundaries of a VotingClassifier
Plot the decision surfaces of ensembles of trees on the iris dataset
Prediction Intervals for Gradient Boosting Regression
Single estimator versus bagging: bias-variance decomposition
Two-class AdaBoost
Examples based on real world datasets#
Applications to real world problems with some medium sized datasets or
interactive user interface.
Compressive sensing: tomography reconstruction with L1 prior (Lasso)
Faces recognition example using eigenfaces and SVMs
Image denoising using kernel PCA
Lagged features for time series forecasting
Model Complexity Influence
Out-of-core classification of text documents
Outlier detection on a real data set
Prediction Latency
Species distribution modeling
Time-related feature engineering
Topic extraction with Non-negative Matrix Factorization and Latent Dirichlet Allocation
Visualizing the stock market structure
Wikipedia principal eigenvector
Feature Selection#
Examples concerning thesklearn.feature_selectionmodule.
sklearn.feature_selection
Comparison of F-test and mutual information
Model-based and sequential feature selection
Pipeline ANOVA SVM
Recursive feature elimination
Recursive feature elimination with cross-validation
Univariate Feature Selection
Frozen Estimators#
Examples concerning thesklearn.frozenmodule.
sklearn.frozen
Examples of Using FrozenEstimator
Gaussian Mixture Models#
Examples concerning thesklearn.mixturemodule.
sklearn.mixture
Concentration Prior Type Analysis of Variation Bayesian Gaussian Mixture
Density Estimation for a Gaussian mixture
GMM Initialization Methods
GMM covariances
Gaussian Mixture Model Ellipsoids
Gaussian Mixture Model Selection
Gaussian Mixture Model Sine Curve
Gaussian Process for Machine Learning#
Examples concerning thesklearn.gaussian_processmodule.
sklearn.gaussian_process
Ability of Gaussian process regression (GPR) to estimate data noise-level
Comparison of kernel ridge and Gaussian process regression
Forecasting of CO2 level on Mona Loa dataset using Gaussian process regression (GPR)
Gaussian Processes regression: basic introductory example
Gaussian process classification (GPC) on iris dataset
Gaussian processes on discrete data structures
Illustration of Gaussian process classification (GPC) on the XOR dataset
Illustration of prior and posterior Gaussian process for different kernels
Iso-probability lines for Gaussian Processes classification (GPC)
Probabilistic predictions with Gaussian process classification (GPC)
Generalized Linear Models#
Examples concerning thesklearn.linear_modelmodule.
sklearn.linear_model
Comparing Linear Bayesian Regressors
Comparing various online solvers
Curve Fitting with Bayesian Ridge Regression
Decision Boundaries of Multinomial and One-vs-Rest Logistic Regression
Early stopping of Stochastic Gradient Descent
Fitting an Elastic Net with a precomputed Gram Matrix and Weighted Samples
HuberRegressor vs Ridge on dataset with strong outliers
Joint feature selection with multi-task Lasso
L1 Penalty and Sparsity in Logistic Regression
L1-based models for Sparse Signals
Lasso model selection via information criteria
Lasso model selection: AIC-BIC / cross-validation
Lasso on dense and sparse data
Lasso, Lasso-LARS, and Elastic Net paths
Logistic function
MNIST classification using multinomial logistic + L1
Multiclass sparse logistic regression on 20newgroups
Non-negative least squares
One-Class SVM versus One-Class SVM using Stochastic Gradient Descent
Ordinary Least Squares Example
Ordinary Least Squares and Ridge Regression Variance
Orthogonal Matching Pursuit
Plot Ridge coefficients as a function of the regularization
Plot multi-class SGD on the iris dataset
Poisson regression and non-normal loss
Polynomial and Spline interpolation
Quantile regression
Regularization path of L1- Logistic Regression
Ridge coefficients as a function of the L2 Regularization
Robust linear estimator fitting
Robust linear model estimation using RANSAC
SGD: Maximum margin separating hyperplane
SGD: Penalties
SGD: Weighted samples
SGD: convex loss functions
Theil-Sen Regression
Tweedie regression on insurance claims
Inspection#
Examples related to thesklearn.inspectionmodule.
sklearn.inspection
Common pitfalls in the interpretation of coefficients of linear models
Failure of Machine Learning to infer causal effects
Partial Dependence and Individual Conditional Expectation Plots
Permutation Importance vs Random Forest Feature Importance (MDI)
Permutation Importance with Multicollinear or Correlated Features
Kernel Approximation#
Examples concerning thesklearn.kernel_approximationmodule.
sklearn.kernel_approximation
Scalable learning with polynomial kernel approximation
Manifold learning#
Examples concerning thesklearn.manifoldmodule.
sklearn.manifold
Comparison of Manifold Learning methods
Manifold Learning methods on a severed sphere
Manifold learning on handwritten digits: Locally Linear Embedding, Isomap…
Multi-dimensional scaling
Swiss Roll And Swiss-Hole Reduction
t-SNE: The effect of various perplexity values on the shape
Miscellaneous#
Miscellaneous and introductory examples for scikit-learn.
Advanced Plotting With Partial Dependence
Comparing anomaly detection algorithms for outlier detection on toy datasets
Comparison of kernel ridge regression and SVR
Displaying Pipelines
Displaying estimators and complex pipelines
Evaluation of outlier detection estimators
Explicit feature map approximation for RBF kernels
Face completion with a multi-output estimators
Introducing the set_output API
Isotonic Regression
Metadata Routing
Multilabel classification
ROC Curve with Visualization API
The Johnson-Lindenstrauss bound for embedding with random projections
Visualizations with Display Objects
Missing Value Imputation#
Examples concerning thesklearn.imputemodule.
sklearn.impute
Imputing missing values before building an estimator
Imputing missing values with variants of IterativeImputer
Model Selection#
Examples related to thesklearn.model_selectionmodule.
sklearn.model_selection
Balance model complexity and cross-validated score
Class Likelihood Ratios to measure classification performance
Comparing randomized search and grid search for hyperparameter estimation
Comparison between grid search and successive halving
Confusion matrix
Custom refit strategy of a grid search with cross-validation
Demonstration of multi-metric evaluation on cross_val_score and GridSearchCV
Detection error tradeoff (DET) curve
Effect of model regularization on training and test error
Multiclass Receiver Operating Characteristic (ROC)
Nested versus non-nested cross-validation
Plotting Cross-Validated Predictions
Plotting Learning Curves and Checking Models’ Scalability
Post-hoc tuning the cut-off point of decision function
Post-tuning the decision threshold for cost-sensitive learning
Precision-Recall
Receiver Operating Characteristic (ROC) with cross validation
Sample pipeline for text feature extraction and evaluation
Statistical comparison of models using grid search
Successive Halving Iterations
Test with permutations the significance of a classification score
Underfitting vs. Overfitting
Visualizing cross-validation behavior in scikit-learn
Multiclass methods#
Examples concerning thesklearn.multiclassmodule.
sklearn.multiclass
Overview of multiclass training meta-estimators
Multioutput methods#
Examples concerning thesklearn.multioutputmodule.
sklearn.multioutput
Multilabel classification using a classifier chain
Nearest Neighbors#
Examples concerning thesklearn.neighborsmodule.
sklearn.neighbors
Approximate nearest neighbors in TSNE
Caching nearest neighbors
Comparing Nearest Neighbors with and without Neighborhood Components Analysis
Dimensionality Reduction with Neighborhood Components Analysis
Kernel Density Estimate of Species Distributions
Kernel Density Estimation
Nearest Centroid Classification
Nearest Neighbors Classification
Nearest Neighbors regression
Neighborhood Components Analysis Illustration
Novelty detection with Local Outlier Factor (LOF)
Outlier detection with Local Outlier Factor (LOF)
Simple 1D Kernel Density Estimation
Neural Networks#
Examples concerning thesklearn.neural_networkmodule.
sklearn.neural_network
Compare Stochastic learning strategies for MLPClassifier
Restricted Boltzmann Machine features for digit classification
Varying regularization in Multi-layer Perceptron
Visualization of MLP weights on MNIST
Pipelines and composite estimators#
Examples of how to compose transformers and pipelines from other estimators. See theUser Guide.
Column Transformer with Heterogeneous Data Sources
Column Transformer with Mixed Types
Concatenating multiple feature extraction methods
Effect of transforming the targets in regression model
Pipelining: chaining a PCA and a logistic regression
Selecting dimensionality reduction with Pipeline and GridSearchCV
Preprocessing#
Examples concerning thesklearn.preprocessingmodule.
sklearn.preprocessing
Compare the effect of different scalers on data with outliers
Comparing Target Encoder with Other Encoders
Demonstrating the different strategies of KBinsDiscretizer
Feature discretization
Importance of Feature Scaling
Map data to a normal distribution
Target Encoder’s Internal Cross fitting
Using KBinsDiscretizer to discretize continuous features
Semi Supervised Classification#
Examples concerning thesklearn.semi_supervisedmodule.
sklearn.semi_supervised
Decision boundary of semi-supervised classifiers versus SVM on the Iris dataset
Effect of varying threshold for self-training
Label Propagation digits active learning
Label Propagation digits: Demonstrating performance
Label Propagation learning a complex structure
Semi-supervised Classification on a Text Dataset
Support Vector Machines#
Examples concerning thesklearn.svmmodule.
sklearn.svm
One-class SVM with non-linear kernel (RBF)
Plot classification boundaries with different SVM Kernels
Plot different SVM classifiers in the iris dataset
Plot the support vectors in LinearSVC
RBF SVM parameters
SVM Margins Example
SVM Tie Breaking Example
SVM with custom kernel
SVM-Anova: SVM with univariate feature selection
SVM: Maximum margin separating hyperplane
SVM: Separating hyperplane for unbalanced classes
SVM: Weighted samples
Scaling the regularization parameter for SVCs
Support Vector Regression (SVR) using linear and non-linear kernels
Working with text documents#
Examples concerning thesklearn.feature_extraction.textmodule.
sklearn.feature_extraction.text
Classification of text documents using sparse features
Clustering text documents using k-means
FeatureHasher and DictVectorizer Comparison
DownloadallexamplesinPythonsourcecode:auto_examples_python.zip
DownloadallexamplesinPythonsourcecode:auto_examples_python.zip
DownloadallexamplesinJupyternotebooks:auto_examples_jupyter.zip
DownloadallexamplesinJupyternotebooks:auto_examples_jupyter.zip
Gallery generated by Sphinx-Gallery

GitHub
GitHub
Getting Started#
The purpose of this guide is to illustrate some of the main features thatscikit-learnprovides. It assumes a very basic working knowledge of
machine learning practices (model fitting, predicting, cross-validation,
etc.). Please refer to ourinstallation instructionsfor installingscikit-learn.
scikit-learn
scikit-learn
Scikit-learnis an open source machine learning library that supports
supervised and unsupervised learning. It also provides various tools for
model fitting, data preprocessing, model selection, model evaluation,
and many other utilities.
Scikit-learn
Fitting and predicting: estimator basics#
Scikit-learnprovides dozens of built-in machine learning algorithms and
models, calledestimators. Each estimator can be fitted to some data
using itsfitmethod.
Scikit-learn
Here is a simple example where we fit aRandomForestClassifierto some very basic data:
RandomForestClassifier
>>>fromsklearn.ensembleimportRandomForestClassifier>>>clf=RandomForestClassifier(random_state=0)>>>X=[[1,2,3],# 2 samples, 3 features...[11,12,13]]>>>y=[0,1]# classes of each sample>>>clf.fit(X,y)RandomForestClassifier(random_state=0)
Thefitmethod generally accepts 2 inputs:
The samples matrix (or design matrix)X. The size ofXis typically(n_samples,n_features), which means that samples are
represented as rows and features are represented as columns.
The samples matrix (or design matrix)X. The size ofXis typically(n_samples,n_features), which means that samples are
represented as rows and features are represented as columns.
X
(n_samples,n_features)
The target valuesywhich are real numbers for regression tasks, or
integers for classification (or any other discrete set of values). For
unsupervised learning tasks,ydoes not need to be specified.yis
usually a 1d array where theith entry corresponds to the target of theith sample (row) ofX.
The target valuesywhich are real numbers for regression tasks, or
integers for classification (or any other discrete set of values). For
unsupervised learning tasks,ydoes not need to be specified.yis
usually a 1d array where theith entry corresponds to the target of theith sample (row) ofX.
y
y
i
i
X
BothXandyare usually expected to be numpy arrays or equivalentarray-likedata types, though some estimators work with other
formats such as sparse matrices.
X
y
Once the estimator is fitted, it can be used for predicting target values of
new data. You don’t need to re-train the estimator:
>>>clf.predict(X)# predict classes of the training dataarray([0, 1])>>>clf.predict([[4,5,6],[14,15,16]])# predict classes of new dataarray([0, 1])
You can checkChoosing the right estimatoron how to choose the right model for your use case.
Transformers and pre-processors#
Machine learning workflows are often composed of different parts. A typical
pipeline consists of a pre-processing step that transforms or imputes the
data, and a final predictor that predicts target values.
Inscikit-learn, pre-processors and transformers follow the same API as
the estimator objects (they actually all inherit from the sameBaseEstimatorclass). The transformer objects don’t have apredictmethod but rather atransformmethod that outputs a
newly transformed sample matrixX:
scikit-learn
BaseEstimator
X
>>>fromsklearn.preprocessingimportStandardScaler>>>X=[[0,15],...[1,-10]]>>># scale data according to computed scaling values>>>StandardScaler().fit(X).transform(X)array([[-1.,  1.],[ 1., -1.]])
Sometimes, you want to apply different transformations to different features:
theColumnTransformeris designed for these
use-cases.
Pipelines: chaining pre-processors and estimators#
Transformers and estimators (predictors) can be combined together into a
single unifying object: aPipeline. The pipeline
offers the same API as a regular estimator: it can be fitted and used for
prediction withfitandpredict. As we will see later, using a
pipeline will also prevent you from data leakage, i.e. disclosing some
testing data in your training data.
Pipeline
fit
predict
In the following example, weload the Iris dataset, split it
into train and test sets, and compute the accuracy score of a pipeline on
the test data:
>>>fromsklearn.preprocessingimportStandardScaler>>>fromsklearn.linear_modelimportLogisticRegression>>>fromsklearn.pipelineimportmake_pipeline>>>fromsklearn.datasetsimportload_iris>>>fromsklearn.model_selectionimporttrain_test_split>>>fromsklearn.metricsimportaccuracy_score...>>># create a pipeline object>>>pipe=make_pipeline(...StandardScaler(),...LogisticRegression()...)...>>># load the iris dataset and split it into train and test sets>>>X,y=load_iris(return_X_y=True)>>>X_train,X_test,y_train,y_test=train_test_split(X,y,random_state=0)...>>># fit the whole pipeline>>>pipe.fit(X_train,y_train)Pipeline(steps=[('standardscaler', StandardScaler()),('logisticregression', LogisticRegression())])>>># we can now use it like any other estimator>>>accuracy_score(pipe.predict(X_test),y_test)0.97...
Model evaluation#
Fitting a model to some data does not entail that it will predict well on
unseen data. This needs to be directly evaluated. We have just seen thetrain_test_splithelper that splits a
dataset into train and test sets, butscikit-learnprovides many other
tools for model evaluation, in particular forcross-validation.
train_test_split
scikit-learn
We here briefly show how to perform a 5-fold cross-validation procedure,
using thecross_validatehelper. Note that
it is also possible to manually iterate over the folds, use different
data splitting strategies, and use custom scoring functions. Please refer to
ourUser Guidefor more details:
cross_validate
>>>fromsklearn.datasetsimportmake_regression>>>fromsklearn.linear_modelimportLinearRegression>>>fromsklearn.model_selectionimportcross_validate...>>>X,y=make_regression(n_samples=1000,random_state=0)>>>lr=LinearRegression()...>>>result=cross_validate(lr,X,y)# defaults to 5-fold CV>>>result['test_score']# r_squared score is high because dataset is easyarray([1., 1., 1., 1., 1.])
Automatic parameter searches#
All estimators have parameters (often called hyper-parameters in the
literature) that can be tuned. The generalization power of an estimator
often critically depends on a few parameters. For example aRandomForestRegressorhas an_estimatorsparameter that determines the number of trees in the forest, and amax_depthparameter that determines the maximum depth of each tree.
Quite often, it is not clear what the exact values of these parameters
should be since they depend on the data at hand.
RandomForestRegressor
n_estimators
max_depth
Scikit-learnprovides tools to automatically find the best parameter
combinations (via cross-validation). In the following example, we randomly
search over the parameter space of a random forest with aRandomizedSearchCVobject. When the search
is over, theRandomizedSearchCVbehaves as
aRandomForestRegressorthat has been fitted with
the best set of parameters. Read more in theUser Guide:
Scikit-learn
RandomizedSearchCV
RandomizedSearchCV
RandomForestRegressor
>>>fromsklearn.datasetsimportfetch_california_housing>>>fromsklearn.ensembleimportRandomForestRegressor>>>fromsklearn.model_selectionimportRandomizedSearchCV>>>fromsklearn.model_selectionimporttrain_test_split>>>fromscipy.statsimportrandint...>>>X,y=fetch_california_housing(return_X_y=True)>>>X_train,X_test,y_train,y_test=train_test_split(X,y,random_state=0)...>>># define the parameter space that will be searched over>>>param_distributions={'n_estimators':randint(1,5),...'max_depth':randint(5,10)}...>>># now create a searchCV object and fit it to the data>>>search=RandomizedSearchCV(estimator=RandomForestRegressor(random_state=0),...n_iter=5,...param_distributions=param_distributions,...random_state=0)>>>search.fit(X_train,y_train)RandomizedSearchCV(estimator=RandomForestRegressor(random_state=0), n_iter=5,param_distributions={'max_depth': ...,'n_estimators': ...},random_state=0)>>>search.best_params_{'max_depth': 9, 'n_estimators': 4}>>># the search object now acts like a normal random forest estimator>>># with max_depth=9 and n_estimators=4>>>search.score(X_test,y_test)0.73...
Note
In practice, you almost always want tosearch over a pipeline, instead of a single estimator. One of the main
reasons is that if you apply a pre-processing step to the whole dataset
without using a pipeline, and then perform any kind of cross-validation,
you would be breaking the fundamental assumption of independence between
training and testing data. Indeed, since you pre-processed the data
using the whole dataset, some information about the test sets are
available to the train sets. This will lead to over-estimating the
generalization power of the estimator (you can read more in thisKaggle
post).
Using a pipeline for cross-validation and searching will largely keep
you from this common pitfall.
Next steps#
We have briefly covered estimator fitting and predicting, pre-processing
steps, pipelines, cross-validation tools and automatic hyper-parameter
searches. This guide should give you an overview of some of the main
features of the library, but there is much more toscikit-learn!
scikit-learn
Please refer to ourUser Guidefor details on all the tools that we
provide. You can also find an exhaustive list of the public API in theAPI Reference.
You can also look at our numerousexamplesthat
illustrate the use ofscikit-learnin many different contexts.
scikit-learn
This Page
Show Source

GitHub
GitHub
Glossary of Common Terms and API Elements#
This glossary hopes to definitively represent the tacit and explicit
conventions applied in Scikit-learn and its API, while providing a reference
for users and contributors. It aims to describe the concepts and either detail
their corresponding API or link to other relevant parts of the documentation
which do so. By linking to glossary entries from the API Reference and User
Guide, we may minimize redundancy and inconsistency.
We begin by listing general concepts (and any that didn’t fit elsewhere), but
more specific sets of related terms are listed below:Class APIs and Estimator Types,Target Types,Methods,Parameters,Attributes,Data and sample properties.
General Concepts#
One-dimensional array. A NumPy array whose.shapehas length 1.
A vector.
.shape
Two-dimensional array. A NumPy array whose.shapehas length 2.
Often represents a matrix.
.shape
Refers to both thespecificinterfaces for estimators implemented in
Scikit-learn and thegeneralizedconventions across types of
estimators as described in this glossary andoverviewed in the
contributor documentation.
The specific interfaces that constitute Scikit-learn’s public API are
largely documented inAPI Reference. However, we less formally consider
anything as public API if none of the identifiers required to access it
begins with_.  We generally try to maintainbackwards
compatibilityfor all objects in the public API.
_
Private API, including functions, modules and methods beginning_are not assured to be stable.
_
The most common data format forinputto Scikit-learn estimators and
functions, array-like is any type object for whichnumpy.asarraywill produce an array of appropriate shape
(usually 1 or 2-dimensional) of appropriate dtype (usually numeric).
numpy.asarray
This includes:
a numpy array
a numpy array
a list of numbers
a list of numbers
a list of length-k lists of numbers for some fixed length k
a list of length-k lists of numbers for some fixed length k
apandas.DataFramewith all columns numeric
apandas.DataFramewith all columns numeric
pandas.DataFrame
a numericpandas.Series
a numericpandas.Series
pandas.Series
It excludes:
asparse matrix
asparse matrix
a sparse array
a sparse array
an iterator
an iterator
a generator
a generator
Note thatoutputfrom scikit-learn estimators and functions (e.g.
predictions) should generally be arrays or sparse matrices, or lists
thereof (as in multi-outputtree.DecisionTreeClassifier’spredict_proba). An estimator wherepredict()returns a list or
apandas.Seriesis not valid.
tree.DecisionTreeClassifier
predict_proba
predict()
pandas.Series
We mostly use attribute to refer to how model information is stored on
an estimator during fitting.  Any public attribute stored on an
estimator instance is required to begin with an alphabetic character
and end in a single underscore if it is set infitorpartial_fit.  These are what is documented under an estimator’sAttributesdocumentation.  The information stored in attributes is
usually either: sufficient statistics used for prediction or
transformation;transductiveoutputs such aslabels_orembedding_; or diagnostic data, such asfeature_importances_.
Common attributes are listedbelow.
A public attribute may have the same name as a constructorparameter, with a_appended.  This is used to store a
validated or estimated version of the user’s input. For example,decomposition.PCAis constructed with ann_componentsparameter. From this, together with other parameters and the data,
PCA estimates the attributen_components_.
_
decomposition.PCA
n_components
n_components_
Further private attributes used in prediction/transformation/etc. may
also be set when fitting.  These begin with a single underscore and are
not assured to be stable for public access.
A public attribute on an estimator instance that does not end in an
underscore should be the stored, unmodified value of an__init__parameterof the same name.  Because of this equivalence, these
are documented under an estimator’sParametersdocumentation.
__init__
We generally try to maintain backward compatibility (i.e. interfaces
and behaviors may be extended but not changed or removed) from release
to release but this comes with some exceptions:
The behavior of objects accessed through private identifiers
(those beginning_) may be changed arbitrarily between
versions.
_
We will generally assume that the users have adhered to the
documented parameter types and ranges. If the documentation asks
for a list and the user gives a tuple, we do not assure consistent
behavior from version to version.
Behaviors may change following adeprecationperiod
(usually two releases long).  Warnings are issued using Python’swarningsmodule.
warnings
We may sometimes assume that all optional parameters (other than X
and y tofitand similar methods) are passed as keyword
arguments only and may be positionally reordered.
Bug fixes and – less often – enhancements may change the behavior
of estimators, including the predictions of an estimator trained on
the same data andrandom_state.  When this happens, we
attempt to note it clearly in the changelog.
We make no assurances that pickling an estimator in one version
will allow it to be unpickled to an equivalent model in the
subsequent version.  (For estimators in the sklearn package, we
issue a warning when this unpickling is attempted, even if it may
happen to work.)  SeeSecurity & Maintainability Limitations.
utils.estimator_checks.check_estimator
We provide limited backwards compatibility assurances for the
estimator checks: we may add extra requirements on estimators
tested with this function, usually when these were informally
assumed but not formally tested.
Despite this informal contract with our users, the software is provided
as is, as stated in the license.  When a release inadvertently
introduces changes that are not backward compatible, these are known
as software regressions.
A function, class or an object which implements the__call__method; anything that returns True when the argument ofcallable().
__call__
A categorical or nominalfeatureis one that has a
finite set of discrete values across the population of data.
These are commonly represented as columns of integers or
strings. Strings will be rejected by most scikit-learn
estimators, and integers will be treated as ordinal or
count-valued. For the use with most estimators, categorical
variables should be one-hot encoded. Notable exceptions include
tree-based models such as random forests and gradient boosting
models that often work better and faster with integer-coded
categorical variables.OrdinalEncoderhelps encoding
string-valued categorical features as ordinal integers, andOneHotEncodercan be used to
one-hot encode categorical features.
See alsoEncoding categorical featuresand thecategorical-encodingpackage for tools related to encoding categorical features.
OrdinalEncoder
OneHotEncoder
To copy anestimator instanceand create a new one with
identicalparameters, but without any fittedattributes, usingclone.
clone
Whenfitis called, ameta-estimatorusually clones
a wrapped estimator instance before fitting the cloned instance.
(Exceptions, for legacy reasons, includePipelineandFeatureUnion.)
fit
Pipeline
FeatureUnion
If the estimator’srandom_stateparameter is an integer (or if the
estimator doesn’t have arandom_stateparameter), anexact cloneis returned: the clone and the original estimator will give the exact
same results. Otherwise,statistical cloneis returned: the clone
might yield different results from the original estimator. More
details can be found inControlling randomness.
random_state
random_state
This refers to the tests run on almost every estimator class in
Scikit-learn to check they comply with basic API conventions.  They are
available for external use throughutils.estimator_checks.check_estimatororutils.estimator_checks.parametrize_with_checks, with most of the
implementation insklearn/utils/estimator_checks.py.
utils.estimator_checks.check_estimator
utils.estimator_checks.parametrize_with_checks
sklearn/utils/estimator_checks.py
Note: Some exceptions to the common testing regime are currently
hard-coded into the library, but we hope to replace this by marking
exceptional behaviours on the estimator using semanticestimator
tags.
A resampling method that iteratively partitions data into mutually
exclusive subsets to fit two stages. During the first stage, the
mutually exclusive subsets enable predictions or transformations to be
computed on data not seen during training. The computed data is then
used in the second stage. The objective is to avoid having any
overfitting in the first stage introduce bias into the input data
distribution of the second stage.
For examples of its use, see:TargetEncoder,StackingClassifier,StackingRegressorandCalibratedClassifierCV.
TargetEncoder
StackingClassifier
StackingRegressor
CalibratedClassifierCV
A resampling method that iteratively partitions data into mutually
exclusive ‘train’ and ‘test’ subsets so model performance can be
evaluated on unseen data. This conserves data as avoids the need to hold
out a ‘validation’ dataset and accounts for variability as multiple
rounds of cross validation are generally performed.
SeeUser Guidefor more details.
We use deprecation to slowly violate ourbackwards
compatibilityassurances, usually to:
change the default value of a parameter; or
change the default value of a parameter; or
remove a parameter, attribute, method, class, etc.
remove a parameter, attribute, method, class, etc.
We will ordinarily issue a warning when a deprecated element is used,
although there may be limitations to this.  For instance, we will raise
a warning when someone sets a parameter that has been deprecated, but
may not when they access that parameter’s attribute on the estimator
instance.
See theContributors’ Guide.
May be used to refer to the number offeatures(i.e.n_features), or columns in a 2d feature matrix.
Dimensions are, however, also used to refer to the length of a NumPy
array’s shape, distinguishing a 1d array from a 2d matrix.
The embedded documentation for a module, class, function, etc., usually
in code as a string at the beginning of the object’s definition, and
accessible as the object’s__doc__attribute.
__doc__
We try to adhere toPEP257, and followNumpyDoc
conventions.
When specifying parameter names for nested estimators,__may be
used to separate between parent and child in some contexts. The most
common use is when setting parameters through a meta-estimator withset_paramsand hence in specifying a search grid inparameter search. Seeparameter.
It is also used inpipeline.Pipeline.fitfor passingsample propertiesto thefitmethods of estimators in
the pipeline.
__
pipeline.Pipeline.fit
fit
NumPy arrays assume a homogeneous data type throughout, available in
the.dtypeattribute of an array (or sparse matrix). We generally
assume simple data types for scikit-learn data: float or integer.
We may support object or string data types for arrays before encoding
or vectorizing.  Our estimators do not work with struct arrays, for
instance.
.dtype
Our documentation can sometimes give information about the dtype
precision, e.g.np.int32,np.int64, etc. When the precision is
provided, it refers to the NumPy dtype. If an arbitrary precision is
used, the documentation will refer to dtypeintegerorfloating.
Note that in this case, the precision can be platform dependent.
Thenumericdtype refers to accepting bothintegerandfloating.
np.int32
np.int64
integer
floating
numeric
integer
floating
When it comes to choosing between 64-bit dtype (i.e.np.float64andnp.int64) and 32-bit dtype (i.e.np.float32andnp.int32), it
boils down to a trade-off between efficiency and precision. The 64-bit
types offer more accurate results due to their lower floating-point
error, but demand more computational resources, resulting in slower
operations and increased memory usage. In contrast, 32-bit types
promise enhanced operation speed and reduced memory consumption, but
introduce a larger floating-point error. The efficiency improvement are
dependent on lower level optimization such as like vectorization,
single instruction multiple dispatch (SIMD), or cache optimization but
crucially on the compatibility of the algorithm in use.
np.float64
np.int64
np.float32
np.int32
Specifically, the choice of precision should account for whether the
employed algorithm can effectively leveragenp.float32. Some
algorithms, especially certain minimization methods, are exclusively
coded fornp.float64, meaning that even ifnp.float32is passed, it
triggers an automatic conversion back tonp.float64. This not only
negates the intended computational savings but also introduces
additional overhead, making operations withnp.float32unexpectedly
slower and more memory-intensive due to this extra conversion step.
np.float32
np.float64
np.float32
np.float64
np.float32
We try to applyduck typingto determine how to
handle some input values (e.g. checking whether a given estimator is
a classifier).  That is, we avoid usingisinstancewhere possible,
and rely on the presence or absence of attributes to determine an
object’s behaviour.  Some nuance is required when following this
approach:
isinstance
For some estimators, an attribute may only be available once it isfitted.  For instance, we cannot a priori determine ifpredict_probais available in a grid search where the grid
includes alternating between a probabilistic and a non-probabilistic
predictor in the final step of the pipeline.  In the following, we
can only determine ifclfis probabilistic after fitting it on
some data:>>>fromsklearn.model_selectionimportGridSearchCV>>>fromsklearn.linear_modelimportSGDClassifier>>>clf=GridSearchCV(SGDClassifier(),...param_grid={'loss':['log_loss','hinge']})This means that we can only check for duck-typed attributes after
fitting, and that we must be careful to makemeta-estimatorsonly present attributes according to the state of the underlying
estimator after fitting.
For some estimators, an attribute may only be available once it isfitted.  For instance, we cannot a priori determine ifpredict_probais available in a grid search where the grid
includes alternating between a probabilistic and a non-probabilistic
predictor in the final step of the pipeline.  In the following, we
can only determine ifclfis probabilistic after fitting it on
some data:
clf
>>>fromsklearn.model_selectionimportGridSearchCV>>>fromsklearn.linear_modelimportSGDClassifier>>>clf=GridSearchCV(SGDClassifier(),...param_grid={'loss':['log_loss','hinge']})
This means that we can only check for duck-typed attributes after
fitting, and that we must be careful to makemeta-estimatorsonly present attributes according to the state of the underlying
estimator after fitting.
Checking if an attribute is present (usinghasattr) is in general
just as expensive as getting the attribute (getattror dot
notation).  In some cases, getting the attribute may indeed be
expensive (e.g. for some implementations offeature_importances_, which may suggest this is an API design
flaw).  So code which doeshasattrfollowed bygetattrshould
be avoided;getattrwithin a try-except block is preferred.
Checking if an attribute is present (usinghasattr) is in general
just as expensive as getting the attribute (getattror dot
notation).  In some cases, getting the attribute may indeed be
expensive (e.g. for some implementations offeature_importances_, which may suggest this is an API design
flaw).  So code which doeshasattrfollowed bygetattrshould
be avoided;getattrwithin a try-except block is preferred.
hasattr
getattr
hasattr
getattr
getattr
For determining some aspects of an estimator’s expectations or
support for some feature, we useestimator tagsinstead of
duck typing.
For determining some aspects of an estimator’s expectations or
support for some feature, we useestimator tagsinstead of
duck typing.
This consists in stopping an iterative optimization method before the
convergence of the training loss, to avoid over-fitting. This is
generally done by monitoring the generalization score on a validation
set. When available, it is activated through the parameterearly_stoppingor by setting a positiven_iter_no_change.
early_stopping
We sometimes use this terminology to distinguish anestimatorclass from a constructed instance. For example, in the following,clsis an estimator class, whileest1andest2are
instances:
cls
est1
est2
cls=RandomForestClassifierest1=cls()est2=RandomForestClassifier()
We try to give examples of basic usage for most functions and
classes in the API:
as doctests in their docstrings (i.e. within thesklearn/library
code itself).
as doctests in their docstrings (i.e. within thesklearn/library
code itself).
sklearn/
as examples in theexample galleryrendered (usingsphinx-gallery) from scripts in theexamples/directory, exemplifying key features or parameters
of the estimator/function.  These should also be referenced from the
User Guide.
as examples in theexample galleryrendered (usingsphinx-gallery) from scripts in theexamples/directory, exemplifying key features or parameters
of the estimator/function.  These should also be referenced from the
User Guide.
examples/
sometimes in theUser Guide(built fromdoc/)
alongside a technical description of the estimator.
sometimes in theUser Guide(built fromdoc/)
alongside a technical description of the estimator.
doc/
An experimental tool is already usable but its public API, such as
default parameter values or fitted attributes, is still subject to
change in future versions without the usualdeprecationwarning policy.
Evaluation metrics give a measure of how well a model performs.  We may
use this term specifically to refer to the functions inmetrics(disregardingpairwise), as distinct from thescoremethod and thescoringAPI used in cross
validation. SeeMetrics and scoring: quantifying the quality of predictions.
metrics
pairwise
These functions usually accept a ground truth (or the raw data
where the metric evaluates clustering without a ground truth) and a
prediction, be it the output ofpredict(y_pred),
ofpredict_proba(y_proba), or of an arbitrary score
function includingdecision_function(y_score).
Functions are usually named to end with_scoreif a greater
score indicates a better model, and_lossif a lesser score
indicates a better model.  This diversity of interface motivates
the scoring API.
y_pred
y_proba
y_score
_score
_loss
Note that some estimators can calculate metrics that are not included
inmetricsand are estimator-specific, notably model
likelihoods.
metrics
Estimator tags describe certain capabilities of an estimator.  This would
enable some runtime behaviors based on estimator inspection, but it
also allows each estimator to be tested for appropriate invariances
while being excepted from othercommon tests.
Some aspects of estimator tags are currently determined through
theduck typingof methods likepredict_probaand through
some special attributes on estimator objects:
predict_proba
For more detailed info, seeEstimator Tags.
In the abstract, a feature is a function (in its mathematical sense)
mapping a sampled object to a numeric or categorical quantity.
“Feature” is also commonly used to refer to these quantities, being the
individual elements of a vector representing a sample. In a data
matrix, features are represented as columns: each column contains the
result of applying a feature function to a set of samples.
Elsewhere features are known as attributes, predictors, regressors, or
independent variables.
Nearly all estimators in scikit-learn assume that features are numeric,
finite and not missing, even when they have semantically distinct
domains and distributions (categorical, ordinal, count-valued,
real-valued, interval). See alsocategorical featureandmissing values.
n_featuresindicates the number of features in a dataset.
n_features
Callingfit(orfit_transform,fit_predict,
etc.) on an estimator.
The state of an estimator afterfitting.
There is no conventional procedure for checking if an estimator
is fitted.  However, an estimator that is not fitted:
should raiseexceptions.NotFittedErrorwhen a prediction
method (predict,transform, etc.) is called.
(utils.validation.check_is_fittedis used internally
for this purpose.)
should raiseexceptions.NotFittedErrorwhen a prediction
method (predict,transform, etc.) is called.
(utils.validation.check_is_fittedis used internally
for this purpose.)
exceptions.NotFittedError
utils.validation.check_is_fitted
should not have anyattributesbeginning with an alphabetic
character and ending with an underscore. (Note that a descriptor for
the attribute may still be present on the class, but hasattr should
return False)
should not have anyattributesbeginning with an alphabetic
character and ending with an underscore. (Note that a descriptor for
the attribute may still be present on the class, but hasattr should
return False)
We provide ad hoc function interfaces for many algorithms, whileestimatorclasses provide a more consistent interface.
In particular, Scikit-learn may provide a function interface that fits
a model to some data and returns the learnt model parameters, as inlinear_model.enet_path.  For transductive models, this also
returns the embedding or cluster labels, as inmanifold.spectral_embeddingorcluster.dbscan.  Many
preprocessing transformers also provide a function interface, akin to
callingfit_transform, as inpreprocessing.maxabs_scale.  Users should be careful to avoiddata leakagewhen making use of thesefit_transform-equivalent functions.
linear_model.enet_path
manifold.spectral_embedding
cluster.dbscan
preprocessing.maxabs_scale
fit_transform
We do not have a strict policy about when to or when not to provide
function forms of estimators, but maintainers should consider
consistency with existing interfaces, and whether providing a function
would lead users astray from best practices (as regards data leakage,
etc.)
Seeexamples.
Seeparameter.
Most machine learning algorithms require that their inputs have nomissing values, and will not work if this requirement is
violated. Algorithms that attempt to fill in (or impute) missing values
are referred to as imputation algorithms.
Anarray-like,sparse matrix, pandas DataFrame or
sequence (usually a list).
Inductive (contrasted withtransductive) machine learning
builds a model of some data that can then be applied to new instances.
Most estimators in Scikit-learn are inductive, havingpredictand/ortransformmethods.
A Python library (https://joblib.readthedocs.io) used in Scikit-learn to
facilite simple parallelism and caching.  Joblib is oriented towards
efficiently working with numpy arrays, such as through use ofmemory mapping. SeeParallelismfor more
information.
The format used to represent multilabel data, where each row of a 2d
array or sparse matrix corresponds to a sample, each column
corresponds to a class, and each element is 1 if the sample is labeled
with the class and 0 if not.
A problem in cross validation where generalization performance can be
over-estimated since knowledge of the test data was inadvertently
included in training a model.  This is a risk, for instance, when
applying atransformerto the entirety of a dataset rather
than each training portion in a cross validation split.
We aim to provide interfaces (such aspipelineandmodel_selection) that shield the user from data leakage.
pipeline
model_selection
A memory efficiency strategy that keeps data on disk rather than
copying it into main memory.  Memory maps can be created for arrays
that can be read, written, or both, usingnumpy.memmap. When
usingjoblibto parallelize operations in Scikit-learn, it
may automatically memmap large arrays to reduce memory duplication
overhead in multiprocessing.
numpy.memmap
Most Scikit-learn estimators do not work with missing values. When they
do (e.g. inimpute.SimpleImputer), NaN is the preferred
representation of missing values in float arrays.  If the array has
integer dtype, NaN cannot be represented. For this reason, we support
specifying anothermissing_valuesvalue whenimputationor
learning can be performed in integer space.Unlabeled datais a special case of missing
values in thetarget.
impute.SimpleImputer
missing_values
n_features
The number offeatures.
n_outputs
The number ofoutputsin thetarget.
n_samples
The number ofsamples.
n_targets
Synonym forn_outputs.
An alias forUser Guide, i.e. documentation written
indoc/modules/. Unlike theAPI referenceprovided
through docstrings, the User Guide aims to:
doc/modules/
group tools provided by Scikit-learn together thematically or in
terms of usage;
group tools provided by Scikit-learn together thematically or in
terms of usage;
motivate why someone would use each particular tool, often through
comparison;
motivate why someone would use each particular tool, often through
comparison;
provide both intuitive and technical descriptions of tools;
provide both intuitive and technical descriptions of tools;
provide or link toexamplesof using key features of a
tool.
provide or link toexamplesof using key features of a
tool.
A shorthand for Numpy due to the conventional import statement:
importnumpyasnp
Where a model is iteratively updated by receiving each batch of ground
truthtargetssoon after making predictions on corresponding
batch of data.  Intrinsically, the model must be usable for prediction
after each batch. Seepartial_fit.
An efficiency strategy where not all the data is stored in main memory
at once, usually by performing learning on batches of data. Seepartial_fit.
Individual scalar/categorical variables per sample in thetarget.  For example, in multilabel classification each
possible label corresponds to a binary output. Also calledresponses,tasksortargets.
Seemulticlass multioutputandcontinuous multioutput.
A tuple of length two.
We mostly useparameterto refer to the aspects of an estimator that
can be specified in its construction. For example,max_depthandrandom_stateare parameters ofRandomForestClassifier.
Parameters to an estimator’s constructor are stored unmodified as
attributes on the estimator instance, and conventionally start with an
alphabetic character and end with an alphanumeric character.  Each
estimator’s constructor parameters are described in the estimator’s
docstring.
max_depth
random_state
RandomForestClassifier
We do not use parameters in the statistical sense, where parameters are
values that specify a model and can be estimated from data. What we
call parameters might be what statisticians call hyperparameters to the
model: aspects for configuring model structure that are often not
directly learnt from data.  However, our parameters are also used to
prescribe modeling operations that do not affect the learnt model, such
asn_jobsfor controlling parallelism.
When talking about the parameters of ameta-estimator, we may
also be including the parameters of the estimators wrapped by the
meta-estimator.  Ordinarily, these nested parameters are denoted by
using adouble underscore(__) to separate between the
estimator-as-parameter and its parameter.  Thusclf=BaggingClassifier(estimator=DecisionTreeClassifier(max_depth=3))has a deep parameterestimator__max_depthwith value3,
which is accessible withclf.estimator.max_depthorclf.get_params()['estimator__max_depth'].
__
clf=BaggingClassifier(estimator=DecisionTreeClassifier(max_depth=3))
estimator__max_depth
3
clf.estimator.max_depth
clf.get_params()['estimator__max_depth']
The list of parameters and their current values can be retrieved from
anestimator instanceusing itsget_paramsmethod.
Between construction and fitting, parameters may be modified usingset_params.  To enable this, parameters are not ordinarily
validated or altered when the estimator is constructed, or when each
parameter is set. Parameter validation is performed whenfitis
called.
Common parameters are listedbelow.
In its broad sense, a pairwise metric defines a function for measuring
similarity or dissimilarity between two samples (with each ordinarily
represented as afeature vector).  We particularly provide
implementations of distance metrics (as well as improper metrics like
Cosine Distance) throughmetrics.pairwise_distances, and of
kernel functions (a constrained class of similarity functions) inmetrics.pairwise.pairwise_kernels.  These can compute pairwise distance
matrices that are symmetric and hence store data redundantly.
metrics.pairwise_distances
metrics.pairwise.pairwise_kernels
See alsoprecomputedandmetric.
Note that for most distance metrics, we rely on implementations fromscipy.spatial.distance, but may reimplement for efficiency in
our context. Themetrics.DistanceMetricinterface is used to implement
distance metrics for integration with efficient neighbors search.
scipy.spatial.distance
metrics.DistanceMetric
A shorthand forPandasdue to the
conventional import statement:
importpandasaspd
Where algorithms rely onpairwise metrics, and can be computed
from pairwise metrics alone, we often allow the user to specify that
theXprovided is already in the pairwise (dis)similarity
space, rather than in a feature space.  That is, when passed tofit, it is a square, symmetric matrix, with each vector
indicating (dis)similarity to every sample, and when passed to
prediction/transformation methods, each row corresponds to a testing
sample and each column to a training sample.
Use of precomputed X is usually indicated by setting ametric,affinityorkernelparameter to the string ‘precomputed’. If
this is the case, then the estimator should set thepairwiseestimator tag as True.
metric
affinity
kernel
pairwise
Data that can be represented as a matrix withsampleson the
first axis and a fixed, finite set offeatureson the second
is called rectangular.
This term excludes samples with non-vectorial structures, such as text,
an image of arbitrary size, a time series of arbitrary length, a set of
vectors, etc. The purpose of avectorizeris to produce
rectangular forms of such data.
We usually use this term as a noun to indicate a single feature vector.
Elsewhere a sample is called an instance, data point, or observation.n_samplesindicates the number of samples in a dataset, being the
number of rows in a data arrayX.
Note that this definition is standard in machine learning and deviates from
statistics where it meansa set of individuals or objects collected or
selected.
n_samples
A sample property is data for each sample (e.g. an array of length
n_samples) passed to an estimator method or a similar function,
alongside but distinct from thefeatures(X) andtarget(y). The most prominent example issample_weight; see others atData and sample properties.
X
y
As of version 0.19 we do not have a consistent approach to handling
sample properties and their routing inmeta-estimators, though
afit_paramsparameter is often used.
fit_params
A venue for publishing Scikit-learn-compatible libraries that are
broadly authorized by the core developers and the contrib community,
but not maintained by the core developer team.
Seehttps://scikit-learn-contrib.github.io.
Changes to the API principles and changes to dependencies or supported
versions happen via aSLEPand follows the
decision-making process outlined inScikit-learn governance and decision-making.
For all votes, a proposal must have been made public and discussed before the
vote. Such a proposal must be a consolidated document, in the form of a
“Scikit-Learn Enhancement Proposal” (SLEP), rather than a long discussion on an
issue. A SLEP must be submitted as a pull-request toenhancement proposalsusing theSLEP template.
Learning where the expected prediction (label or ground truth) is only
available for some samples provided as training data whenfittingthe model.  We conventionally apply the label-1tounlabeledsamples in semi-supervised classification.
-1
A representation of two-dimensional numeric data that is more memory
efficient the corresponding dense numpy array where almost all elements
are zero. We use thescipy.sparseframework, which provides
several underlying sparse data representations, orformats.
Some formats are more efficient than others for particular tasks, and
when a particular format provides especial benefit, we try to document
this fact in Scikit-learn parameter descriptions.
scipy.sparse
Some sparse matrix formats (notably CSR, CSC, COO and LIL) distinguish
betweenimplicitandexplicitzeros. Explicit zeros are stored
(i.e. they consume memory in adataarray) in the data structure,
while implicit zeros correspond to every element not otherwise defined
in explicit storage.
data
Two semantics for sparse matrices are used in Scikit-learn:
The sparse matrix is interpreted as an array with implicit and
explicit zeros being interpreted as the number 0.  This is the
interpretation most often adopted, e.g. when sparse matrices
are used for feature matrices ormultilabel indicator
matrices.
As withscipy.sparse.csgraph, explicit zeros are
interpreted as the number 0, but implicit zeros indicate a masked
or absent value, such as the absence of an edge between two
vertices of a graph, where an explicit value indicates an edge’s
weight. This interpretation is adopted to represent connectivity
in clustering, in representations of nearest neighborhoods
(e.g.neighbors.kneighbors_graph), and for precomputed
distance representation where only distances in the neighborhood
of each point are required.
scipy.sparse.csgraph
neighbors.kneighbors_graph
When working with sparse matrices, we assume that it is sparse for a
good reason, and avoid writing code that densifies a user-provided
sparse matrix, instead maintaining sparsity or raising an error if not
possible (i.e. if an estimator does not / cannot support sparse
matrices).
An estimator is stateless if it does not store any information that is
obtained duringfit. This information can be either parameters
learned duringfitor statistics computed from the
training data. An estimator is stateless if it has noattributesapart from ones set in__init__. Callingfitfor these
estimators will only validate the publicattributespassed
in__init__.
__init__
__init__
Learning where the expected prediction (label or ground truth) is
available for each sample whenfittingthe model, provided asy.  This is the approach taken in aclassifierorregressoramong other estimators.
Thedependent variableinsupervised(andsemisupervised) learning, passed asyto an estimator’sfitmethod.  Also known asdependent variable,outcome
variable,response variable,ground truthorlabel. Scikit-learn
works with targets that have minimal structure: a class from a finite
set, a finite real-valued number, multiple classes, or multiple
numbers. SeeTarget Types.
A transductive (contrasted withinductive) machine learning
method is designed to model a specific dataset, but not to apply that
model to unseen data.  Examples includemanifold.TSNE,cluster.AgglomerativeClusteringandneighbors.LocalOutlierFactor.
manifold.TSNE
cluster.AgglomerativeClustering
neighbors.LocalOutlierFactor
Samples with an unknown ground truth when fitting; equivalently,missing valuesin thetarget.  See alsosemisupervisedandunsupervisedlearning.
Learning where the expected prediction (label or ground truth) is not
available for each sample whenfittingthe model, as inclusterersandoutlier detectors.  Unsupervised
estimators ignore anyypassed tofit.
Class APIs and Estimator Types#
Asupervised(orsemi-supervised)predictorwith a finite set of discrete possible output values.
A classifier supports modeling some ofbinary,multiclass,multilabel, ormulticlass
multioutputtargets.  Within scikit-learn, all classifiers support
multi-class classification, defaulting to using a one-vs-rest
strategy over the binary classification problem.
Classifiers must store aclasses_attribute after fitting,
and inherit frombase.ClassifierMixin, which sets
their correspondingestimator tagscorrectly.
base.ClassifierMixin
A classifier can be distinguished from other estimators withis_classifier.
is_classifier
A classifier must implement:
fit
fit
predict
predict
score
score
It may also be appropriate to implementdecision_function,predict_probaandpredict_log_proba.
Aunsupervisedpredictorwith a finite set of discrete
output values.
A clusterer usually storeslabels_after fitting, and must do
so if it istransductive.
A clusterer must implement:
fit
fit
fit_predictiftransductive
fit_predictiftransductive
predictifinductive
predictifinductive
Anunsupervisedestimation of input probability density
function. Commonly used techniques are:
Kernel Density Estimation- uses a kernel function, controlled by the
bandwidth parameter to represent density;
Kernel Density Estimation- uses a kernel function, controlled by the
bandwidth parameter to represent density;
Gaussian mixture- uses mixture of Gaussian models
to represent density.
Gaussian mixture- uses mixture of Gaussian models
to represent density.
An object which manages the estimation and decoding of a model. The
model is estimated as a deterministic function of:
parametersprovided in object construction or withset_params;
parametersprovided in object construction or withset_params;
the globalnumpy.randomrandom state if the estimator’srandom_stateparameter is set to None; and
the globalnumpy.randomrandom state if the estimator’srandom_stateparameter is set to None; and
numpy.random
any data orsample propertiespassed to the most recent
call tofit,fit_transformorfit_predict,
or data similarly passed in a sequence of calls topartial_fit.
any data orsample propertiespassed to the most recent
call tofit,fit_transformorfit_predict,
or data similarly passed in a sequence of calls topartial_fit.
The estimated model is stored in public and privateattributeson the estimator instance, facilitating decoding through prediction
and transformation methods.
Estimators must provide afitmethod, and should provideset_paramsandget_params, although these are usually
provided by inheritance frombase.BaseEstimator.
base.BaseEstimator
The core functionality of some estimators may also be available as afunction.
Atransformerwhich takes input where each sample is not
represented as anarray-likeobject of fixed length, and
produces anarray-likeobject offeaturesfor each
sample (and thus a 2-dimensional array-like for a set of samples).  In
other words, it (lossily) maps a non-rectangular data representation
intorectangulardata.
Feature extractors must implement at least:
fit
fit
transform
transform
get_feature_names_out
get_feature_names_out
Anestimatorwhich takes another estimator as a parameter.
Examples includepipeline.Pipeline,model_selection.GridSearchCV,feature_selection.SelectFromModelandensemble.BaggingClassifier.
pipeline.Pipeline
model_selection.GridSearchCV
feature_selection.SelectFromModel
ensemble.BaggingClassifier
In a meta-estimator’sfitmethod, any contained estimators
should beclonedbefore they are fit (although FIXME: Pipeline
and FeatureUnion do not do this currently). An exception to this is
that an estimator may explicitly document that it accepts a pre-fitted
estimator (e.g. usingprefit=Trueinfeature_selection.SelectFromModel). One known issue with this
is that the pre-fitted estimator will lose its model if the
meta-estimator is cloned.  A meta-estimator should havefitcalled
before prediction, even if all contained estimators are pre-fitted.
prefit=True
feature_selection.SelectFromModel
fit
In cases where a meta-estimator’s primary behaviors (e.g.predictortransformimplementation) are functions of
prediction/transformation methods of the providedbase estimator(or
multiple base estimators), a meta-estimator should provide at least the
standard methods provided by the base estimator.  It may not be
possible to identify which methods are provided by the underlying
estimator until the meta-estimator has beenfitted(see alsoduck typing), for whichutils.metaestimators.available_ifmay help.  It
should also provide (or modify) theestimator tagsandclasses_attribute provided by the base estimator.
utils.metaestimators.available_if
Meta-estimators should be careful to validate data as minimally as
possible before passing it to an underlying estimator. This saves
computation time, and may, for instance, allow the underlying
estimator to easily work with data that is notrectangular.
Anunsupervisedbinarypredictorwhich models the
distinction between core and outlying samples.
Outlier detectors must implement:
fit
fit
fit_predictiftransductive
fit_predictiftransductive
predictifinductive
predictifinductive
Inductive outlier detectors may also implementdecision_functionto give a normalized inlier score where
outliers have score below 0.score_samplesmay provide an
unnormalized score per sample.
Anestimatorsupportingpredictand/orfit_predict. This encompassesclassifier,regressor,outlier detectorandclusterer.
In statistics, “predictors” refers tofeatures.
Asupervised(orsemi-supervised)predictorwithcontinuousoutput values.
Regressors inherit frombase.RegressorMixin, which sets theirestimator tagscorrectly.
base.RegressorMixin
A regressor can be distinguished from other estimators withis_regressor.
is_regressor
A regressor must implement:
fit
fit
predict
predict
score
score
An estimator supportingtransformand/orfit_transform.
A purelytransductivetransformer, such asmanifold.TSNE, may not implementtransform.
manifold.TSNE
transform
Seefeature extractor.
There are further APIs specifically related to a small family of estimators,
such as:
A non-estimator family of classes used to split a dataset into a
sequence of train and test portions (seeCross-validation: evaluating estimator performance),
by providingsplitandget_n_splitsmethods.
Note that unlike estimators, these do not havefitmethods
and do not provideset_paramsorget_params.
Parameter validation may be performed in__init__.
__init__
An estimator that has built-in cross-validation capabilities to
automatically select the best hyper-parameters (see theUser
Guide). Some example of cross-validation estimators
areElasticNetCVandLogisticRegressionCV.
Cross-validation estimators are namedEstimatorCVand tend to be
roughly equivalent toGridSearchCV(Estimator(),...). The
advantage of using a cross-validation estimator over the canonicalestimatorclass along withgrid searchis
that they can take advantage of warm-starting by reusing precomputed
results in the previous steps of the cross-validation process. This
generally leads to speed improvements. An exception is theRidgeCVclass, which can instead
perform efficient Leave-One-Out (LOO) CV. By default, all these
estimators, apart fromRidgeCVwith an
LOO-CV, will be refitted on the full training dataset after finding the
best combination of hyper-parameters.
ElasticNetCV
LogisticRegressionCV
EstimatorCV
GridSearchCV(Estimator(),...)
RidgeCV
RidgeCV
A non-estimator callable object which evaluates an estimator on given
test data, returning a number. Unlikeevaluation metrics,
a greater returned number must correspond with abetterscore.
SeeThe scoring parameter: defining model evaluation rules.
Further examples:
metrics.DistanceMetric
metrics.DistanceMetric
metrics.DistanceMetric
gaussian_process.kernels.Kernel
gaussian_process.kernels.Kernel
gaussian_process.kernels.Kernel
tree.Criterion
tree.Criterion
tree.Criterion
Metadata Routing#
An object which consumesmetadata. This object is usually anestimator, ascorer, or aCV splitter. Consuming
metadata means using it in calculations, e.g. usingsample_weightto calculate a certain type of score. Being a
consumer doesn’t mean that the object always receives a certain
metadata, rather it means it can use it if it is provided.
Data which is related to the givenXandydata, but
is not directly a part of the data, e.g.sample_weightorgroups, and is passed along to different objects and methods,
e.g. to ascoreror aCV splitter.
An object which routes metadata toconsumers. This
object is usually ameta-estimator, e.g.PipelineorGridSearchCV.
Some routers can also be a consumer. This happens for example when a
meta-estimator uses the givengroups, and it also passes it
along to some of its sub-objects, such as aCV splitter.
Pipeline
GridSearchCV
Please refer toMetadata Routing User Guidefor more
information.
Target Types#
A classification problem consisting of two classes.  A binary target
may  be represented as for amulticlassproblem but with only two
labels.  A binary decision function is represented as a 1d array.
Semantically, one class is often considered the “positive” class.
Unless otherwise specified (e.g. usingpos_labelinevaluation metrics), we consider the class label with the
greater value (numerically or lexicographically) as the positive class:
of labels [0, 1], 1 is the positive class; of [1, 2], 2 is the positive
class; of [‘no’, ‘yes’], ‘yes’ is the positive class; of [‘no’, ‘YES’],
‘no’ is the positive class.  This affects the output ofdecision_function, for instance.
Note that a dataset sampled from a multiclassyor a continuousymay appear to be binary.
y
y
type_of_targetwill return ‘binary’ for
binary input, or a similar array with only a single class present.
type_of_target
A regression problem where each sample’s target is a finite floating
point number represented as a 1-dimensional array of floats (or
sometimes ints).
type_of_targetwill return ‘continuous’ for
continuous input, but if the data is all integers, it will be
identified as ‘multiclass’.
type_of_target
A regression problem where each sample’s target consists ofn_outputsoutputs, each one a finite floating point number, for a
fixed intn_outputs>1in a particular dataset.
n_outputs
n_outputs>1
Continuous multioutput targets are represented as multiplecontinuoustargets, horizontally stacked into an array
of shape(n_samples,n_outputs).
(n_samples,n_outputs)
type_of_targetwill return
‘continuous-multioutput’ for continuous multioutput input, but if the
data is all integers, it will be identified as
‘multiclass-multioutput’.
type_of_target
A classification problem consisting of more than two classes.  A
multiclass target may be represented as a 1-dimensional array of
strings or integers.  A 2d column vector of integers (i.e. a
single output inmultioutputterms) is also accepted.
We do not officially support other orderable, hashable objects as class
labels, even if estimators may happen to work when given classification
targets of such type.
For semi-supervised classification,unlabeledsamples should
have the special label -1 iny.
y
Within scikit-learn, all estimators supporting binary classification
also support multiclass classification, using One-vs-Rest by default.
Apreprocessing.LabelEncoderhelps to canonicalize multiclass
targets as integers.
preprocessing.LabelEncoder
type_of_targetwill return ‘multiclass’ for
multiclass input. The user may also want to handle ‘binary’ input
identically to ‘multiclass’.
type_of_target
A classification problem where each sample’s target consists ofn_outputsoutputs, each a class label, for a fixed intn_outputs>1in a particular dataset.  Each output has a
fixed set of available classes, and each sample is labeled with a
class for each output. An output may be binary or multiclass, and in
the case where all outputs are binary, the target ismultilabel.
n_outputs
n_outputs>1
Multiclass multioutput targets are represented as multiplemulticlasstargets, horizontally stacked into an array
of shape(n_samples,n_outputs).
(n_samples,n_outputs)
XXX: For simplicity, we may not always support string class labels
for multiclass multioutput, and integer class labels should be used.
multioutputprovides estimators which estimate multi-output
problems using multiple single-output estimators.  This may not fully
account for dependencies among the different outputs, which methods
natively handling the multioutput case (e.g. decision trees, nearest
neighbors, neural networks) may do better.
multioutput
type_of_targetwill return
‘multiclass-multioutput’ for multiclass multioutput input.
type_of_target
Amulticlass multioutputtarget where each output isbinary.  This may be represented as a 2d (dense) array or
sparse matrix of integers, such that each column is a separate binary
target, where positive labels are indicated with 1 and negative labels
are usually -1 or 0.  Sparse multilabel targets are not supported
everywhere that dense multilabel targets are supported.
Semantically, a multilabel target can be thought of as a set of labels
for each sample.  While not used internally,preprocessing.MultiLabelBinarizeris provided as a utility to
convert from a list of sets representation to a 2d array or sparse
matrix. One-hot encoding a multiclass target withpreprocessing.LabelBinarizerturns it into a multilabel
problem.
preprocessing.MultiLabelBinarizer
preprocessing.LabelBinarizer
type_of_targetwill return
‘multilabel-indicator’ for multilabel input, whether sparse or dense.
type_of_target
A target where each sample has multiple classification/regression
labels. Seemulticlass multioutputandcontinuous
multioutput. We do not currently support modelling mixed
classification and regression targets.
Methods#
decision_function
In a fittedclassifieroroutlier detector, predicts a
“soft” score for each sample in relation to each class, rather than the
“hard” categorical prediction produced bypredict.  Its input
is usually only some observed data,X.
If the estimator was not alreadyfitted, calling this method
should raise aexceptions.NotFittedError.
exceptions.NotFittedError
Output conventions:
A 1-dimensional array, where values strictly greater than zero
indicate the positive class (i.e. the last class inclasses_).
A 2-dimensional array, where the row-wise arg-maximum is the
predicted class.  Columns are ordered according toclasses_.
Scikit-learn is inconsistent in its representation ofmultilabeldecision functions. It may be represented one of two ways:
List of 2d arrays, each array of shape: (n_samples, 2), like in
multiclass multioutput. List is of lengthn_labels.
List of 2d arrays, each array of shape: (n_samples, 2), like in
multiclass multioutput. List is of lengthn_labels.
n_samples
n_labels
Single 2d array of shape (n_samples,n_labels), with each
‘column’ in the array corresponding to the individual binary
classification decisions. This is identical to the
multiclass classification format, though its semantics differ: it
should be interpreted, like in the binary case, by thresholding at
0.
Single 2d array of shape (n_samples,n_labels), with each
‘column’ in the array corresponding to the individual binary
classification decisions. This is identical to the
multiclass classification format, though its semantics differ: it
should be interpreted, like in the binary case, by thresholding at
0.
n_samples
n_labels
A list of 2d arrays, corresponding to each multiclass decision
function.
A 1-dimensional array, where a value greater than or equal to zero
indicates an inlier.
fit
Thefitmethod is provided on every estimator. It usually takes somesamplesX,targetsyif the model is supervised,
and potentially othersample propertiessuch assample_weight.  It should:
fit
X
y
clear any priorattributesstored on the estimator, unlesswarm_startis used;
clear any priorattributesstored on the estimator, unlesswarm_startis used;
validate and interpret anyparameters, ideally raising an
error if invalid;
validate and interpret anyparameters, ideally raising an
error if invalid;
validate the input data;
validate the input data;
estimate and store model attributes from the estimated parameters and
provided data; and
estimate and store model attributes from the estimated parameters and
provided data; and
return the nowfittedestimator to facilitate method
chaining.
return the nowfittedestimator to facilitate method
chaining.
Target Typesdescribes possible formats fory.
y
fit_predict
Used especially forunsupervised,transductiveestimators, this fits the model and returns the predictions (similar topredict) on the training data. In clusterers, these predictions
are also stored in thelabels_attribute, and the output of.fit_predict(X)is usually equivalent to.fit(X).predict(X).
The parameters tofit_predictare the same as those tofit.
.fit_predict(X)
.fit(X).predict(X)
fit_predict
fit
fit_transform
A method ontransformerswhich fits the estimator and returns
the transformed training data. It takes parameters as infitand its output should have the same shape as calling.fit(X,...).transform(X). There are nonetheless rare cases where.fit_transform(X,...)and.fit(X,...).transform(X)do not
return the same value, wherein training data needs to be handled
differently (due to model blending in stacked ensembles, for instance;
such cases should be clearly documented).Transductivetransformers may also providefit_transformbut nottransform.
.fit(X,...).transform(X)
.fit_transform(X,...)
.fit(X,...).transform(X)
fit_transform
One reason to implementfit_transformis that performingfitandtransformseparately would be less efficient than together.base.TransformerMixinprovides a default implementation,
providing a consistent interface across transformers wherefit_transformis or is not specialized.
fit_transform
fit
transform
base.TransformerMixin
fit_transform
Ininductivelearning – where the goal is to learn a
generalized model that can be applied to new data – users should be
careful not to applyfit_transformto the entirety of a dataset
(i.e. training and test data together) before further modelling, as
this results indata leakage.
fit_transform
get_feature_names_out
Primarily forfeature extractors, but also used for other
transformers to provide string names for each column in the output of
the estimator’stransformmethod.  It outputs an array of
strings and may take an array-like of strings as input, corresponding
to the names of input columns from which output column names can
be generated.  Ifinput_featuresis not passed in, then thefeature_names_in_attribute will be used. If thefeature_names_in_attribute is not defined, then the
input names are named[x0,x1,...,x(n_features_in_-1)].
input_features
feature_names_in_
feature_names_in_
[x0,x1,...,x(n_features_in_-1)]
get_n_splits
On aCV splitter(not an estimator), returns the number of
elements one would get if iterating through the return value ofsplitgiven the same parameters.  Takes the same parameters as
split.
get_params
Gets allparameters, and their values, that can be set usingset_params.  A parameterdeepcan be used, when set to
False to only return those parameters not including__, i.e.  not
due to indirection via contained estimators.
deep
__
Most estimators adopt the definition frombase.BaseEstimator,
which simply adopts the parameters defined for__init__.pipeline.Pipeline, among others, reimplementsget_paramsto declare the estimators named in itsstepsparameters as
themselves being parameters.
base.BaseEstimator
__init__
pipeline.Pipeline
get_params
steps
partial_fit
Facilitates fitting an estimator in an online fashion.  Unlikefit,
repeatedly callingpartial_fitdoes not clear the model, but
updates it with the data provided. The portion of data
provided topartial_fitmay be called a mini-batch.
Each mini-batch must be of consistent shape, etc. In iterative
estimators,partial_fitoften only performs a single iteration.
fit
partial_fit
partial_fit
partial_fit
partial_fitmay also be used forout-of-corelearning,
although usually limited to the case where learning can be performed
online, i.e. the model is usable after eachpartial_fitand there
is no separate processing needed to finalize the model.cluster.Birchintroduces the convention that callingpartial_fit(X)will produce a model that is not finalized, but the
model can be finalized by callingpartial_fit()i.e. without
passing a further mini-batch.
partial_fit
partial_fit
cluster.Birch
partial_fit(X)
partial_fit()
Generally, estimator parameters should not be modified between calls
topartial_fit, althoughpartial_fitshould validate them
as well as the new mini-batch of data.  In contrast,warm_startis used to repeatedly fit the same estimator with the same data
but varying parameters.
partial_fit
partial_fit
warm_start
Likefit,partial_fitshould return the estimator object.
fit
partial_fit
To clear the model, a new estimator should be constructed, for instance
withbase.clone.
base.clone
NOTE: Usingpartial_fitafterfitresults in undefined behavior.
partial_fit
fit
predict
Makes a prediction for each sample, usually only takingXas
input (but see under regressor output conventions below). In aclassifierorregressor, this prediction is in the same
target space used in fitting (e.g. one of {‘red’, ‘amber’, ‘green’} if
theyin fitting consisted of these strings).  Despite this, even
whenypassed tofitis a list or other array-like, the
output ofpredictshould always be an array or sparse matrix. In aclustereroroutlier detectorthe prediction is an
integer.
y
y
predict
If the estimator was not alreadyfitted, calling this method
should raise aexceptions.NotFittedError.
exceptions.NotFittedError
Output conventions:
An array of shape(n_samples,)(n_samples,n_outputs).Multilabeldata may be represented as a sparse
matrix if a sparse matrix was used in fitting. Each element should
be one of the values in the classifier’sclasses_attribute.
(n_samples,)
(n_samples,n_outputs)
An array of shape(n_samples,)where each value is from 0 ton_clusters-1if the corresponding sample is clustered,
and -1 if the sample is not clustered, as incluster.dbscan.
(n_samples,)
n_clusters-1
cluster.dbscan
An array of shape(n_samples,)where each value is -1 for an
outlier and 1 otherwise.
(n_samples,)
A numeric array of shape(n_samples,), usually float64.
Some regressors have extra options in theirpredictmethod,
allowing them to return standard deviation (return_std=True)
or covariance (return_cov=True) relative to the predicted
value.  In this case, the return value is a tuple of arrays
corresponding to (prediction mean, std, cov) as required.
(n_samples,)
predict
return_std=True
return_cov=True
predict_log_proba
The natural logarithm of the output ofpredict_proba, provided
to facilitate numerical stability.
predict_proba
A method inclassifiersandclusterersthat can
return probability estimates for each class/cluster.  Its input is
usually only some observed data,X.
If the estimator was not alreadyfitted, calling this method
should raise aexceptions.NotFittedError.
exceptions.NotFittedError
Output conventions are like those fordecision_functionexcept
in thebinaryclassification case, where one column is output
for each class (whiledecision_functionoutputs a 1d array). For
binary and multiclass predictions, each row should add to 1.
decision_function
Like other methods,predict_probashould only be present when the
estimator can make probabilistic predictions (seeduck typing).
This means that the presence of the method may depend on estimator
parameters (e.g. inlinear_model.SGDClassifier) or training
data (e.g. inmodel_selection.GridSearchCV) and may only
appear after fitting.
predict_proba
linear_model.SGDClassifier
model_selection.GridSearchCV
score
A method on an estimator, usually apredictor, which evaluates
its predictions on a given dataset, and returns a single numerical
score.  A greater return value should indicate better predictions;
accuracy is used for classifiers and R^2 for regressors by default.
If the estimator was not alreadyfitted, calling this method
should raise aexceptions.NotFittedError.
exceptions.NotFittedError
Some estimators implement a custom, estimator-specific score function,
often the likelihood of the data under the model.
score_samples
A method that returns a score for each given sample. The exact
definition ofscorevaries from one class to another. In the case of
density estimation, it can be the log density model on the data, and in
the case of outlier detection, it can be the opposite of the outlier
factor of the data.
If the estimator was not alreadyfitted, calling this method
should raise aexceptions.NotFittedError.
exceptions.NotFittedError
set_params
Available in any estimator, takes keyword arguments corresponding to
keys inget_params.  Each is provided a new value to assign
such that callingget_paramsafterset_paramswill reflect the
changedparameters.  Most estimators use the implementation inbase.BaseEstimator, which handles nested parameters and
otherwise sets the parameter as an attribute on the estimator.
The method is overridden inpipeline.Pipelineand related
estimators.
get_params
set_params
base.BaseEstimator
pipeline.Pipeline
split
On aCV splitter(not an estimator), this method accepts
parameters (X,y,groups), where all may be
optional, and returns an iterator over(train_idx,test_idx)pairs.  Each of {train,test}_idx is a 1d integer array, with values
from 0 fromX.shape[0]-1of any length, such that no values
appear in both sometrain_idxand its correspondingtest_idx.
(train_idx,test_idx)
X.shape[0]-1
train_idx
test_idx
transform
In atransformer, transforms the input, usually onlyX,
into some transformed space (conventionally notated asXt).
Output is an array or sparse matrix of lengthn_samplesand
with the number of columns fixed afterfitting.
If the estimator was not alreadyfitted, calling this method
should raise aexceptions.NotFittedError.
exceptions.NotFittedError
Parameters#
These common parameter names, specifically used in estimator construction
(see conceptparameter), sometimes also appear as parameters of
functions or non-estimator constructors.
class_weight
Used to specify sample weights when fitting classifiers as a function
of thetargetclass.  Wheresample_weightis also
supported and given, it is multiplied by theclass_weightcontribution. Similarly, whereclass_weightis used in amultioutput(includingmultilabel) tasks, the weights
are multiplied across outputs (i.e. columns ofy).
class_weight
class_weight
y
By default, all samples have equal weight such that classes are
effectively weighted by their prevalence in the training data.
This could be achieved explicitly withclass_weight={label1:1,label2:1,...}for all class labels.
class_weight={label1:1,label2:1,...}
More generally,class_weightis specified as a dict mapping class
labels to weights ({class_label:weight}), such that each sample
of the named class is given that weight.
class_weight
{class_label:weight}
class_weight='balanced'can be used to give all classes
equal weight by giving each sample a weight inversely related
to its class’s prevalence in the training data:n_samples/(n_classes*np.bincount(y)). Class weights will be
used differently depending on the algorithm: for linear models (such
as linear SVM or logistic regression), the class weights will alter the
loss function by weighting the loss of each sample by its class weight.
For tree-based algorithms, the class weights will be used for
reweighting the splitting criterion.Notehowever that this rebalancing does not take the weight of
samples in each class into account.
class_weight='balanced'
n_samples/(n_classes*np.bincount(y))
For multioutput classification, a list of dicts is used to specify
weights for each output. For example, for four-class multilabel
classification weights should be[{0:1,1:1},{0:1,1:5},{0:1,1:1},{0:1,1:1}]instead of[{1:1},{2:5},{3:1},{4:1}].
[{0:1,1:1},{0:1,1:5},{0:1,1:1},{0:1,1:1}]
[{1:1},{2:5},{3:1},{4:1}]
Theclass_weightparameter is validated and interpreted withutils.class_weight.compute_class_weight.
class_weight
utils.class_weight.compute_class_weight
cv
Determines a cross validation splitting strategy, as used in
cross-validation based routines.cvis also available in estimators
such asmultioutput.ClassifierChainorcalibration.CalibratedClassifierCVwhich use the predictions
of one estimator as training data for another, to not overfit the
training supervision.
cv
multioutput.ClassifierChain
calibration.CalibratedClassifierCV
Possible inputs forcvare usually:
cv
An integer, specifying the number of folds in K-fold cross
validation. K-fold will be stratified over classes if the estimator
is a classifier (determined bybase.is_classifier) and thetargetsmay represent a binary or multiclass (but not
multioutput) classification problem (determined byutils.multiclass.type_of_target).
An integer, specifying the number of folds in K-fold cross
validation. K-fold will be stratified over classes if the estimator
is a classifier (determined bybase.is_classifier) and thetargetsmay represent a binary or multiclass (but not
multioutput) classification problem (determined byutils.multiclass.type_of_target).
base.is_classifier
utils.multiclass.type_of_target
Across-validation splitterinstance. Refer to theUser Guidefor splitters available
within Scikit-learn.
Across-validation splitterinstance. Refer to theUser Guidefor splitters available
within Scikit-learn.
An iterable yielding train/test splits.
An iterable yielding train/test splits.
With some exceptions (especially where not using cross validation at
all is an option), the default is 5-fold.
cvvalues are validated and interpreted withmodel_selection.check_cv.
cv
model_selection.check_cv
kernel
Specifies the kernel function to be used by Kernel Method algorithms.
For example, the estimatorssvm.SVCandgaussian_process.GaussianProcessClassifierboth have akernelparameter that takes the name of the kernel to use as string
or a callable kernel function used to compute the kernel matrix. For
more reference, see theKernel Approximationand theGaussian Processesuser guides.
svm.SVC
gaussian_process.GaussianProcessClassifier
kernel
max_iter
For estimators involving iterative optimization, this determines the
maximum number of iterations to be performed infit.  Ifmax_iteriterations are run without convergence, aexceptions.ConvergenceWarningshould be raised.  Note that the
interpretation of “a single iteration” is inconsistent across
estimators: some, but not all, use it to mean a single epoch (i.e. a
pass over every sample in the data).
max_iter
exceptions.ConvergenceWarning
FIXME perhaps we should have some common tests about the relationship
between ConvergenceWarning and max_iter.
memory
Some estimators make use ofjoblib.Memoryto
store partial solutions during fitting. Thus whenfitis called
again, those partial solutions have been memoized and can be reused.
joblib.Memory
fit
Amemoryparameter can be specified as a string with a path to a
directory, or ajoblib.Memoryinstance (or an object with a
similar interface, i.e. acachemethod) can be used.
memory
joblib.Memory
cache
memoryvalues are validated and interpreted withutils.validation.check_memory.
memory
utils.validation.check_memory
metric
As a parameter, this is the scheme for determining the distance between
two data points.  Seemetrics.pairwise_distances.  In practice,
for some algorithms, an improper distance metric (one that does not
obey the triangle inequality, such as Cosine Distance) may be used.
metrics.pairwise_distances
XXX: hierarchical clustering usesaffinitywith this meaning.
affinity
We also usemetricto refer toevaluation metrics, but avoid
using this sense as a parameter name.
n_components
The number of features which atransformershould transform the
input into. Seecomponents_for the special case of affine
projection.
n_iter_no_change
Number of iterations with no improvement to wait before stopping the
iterative procedure. This is also known as apatienceparameter. It
is typically used withearly stoppingto avoid stopping too
early.
n_jobs
This parameter is used to specify how many concurrent processes or
threads should be used for routines that are parallelized withjoblib.
n_jobsis an integer, specifying the maximum number of concurrently
running workers. If 1 is given, no joblib parallelism is used at all,
which is useful for debugging. If set to -1, all CPUs are used. Forn_jobsbelow -1, (n_cpus + 1 + n_jobs) are used. For example withn_jobs=-2, all CPUs but one are used.
n_jobs
n_jobs
n_jobs=-2
n_jobsisNoneby default, which meansunset; it will
generally be interpreted asn_jobs=1, unless the currentjoblib.Parallelbackend context specifies otherwise.
n_jobs
None
n_jobs=1
joblib.Parallel
Note that even ifn_jobs=1, low-level parallelism (via Numpy and OpenMP)
might be used in some configuration.
n_jobs=1
For more details on the use ofjobliband its interactions with
scikit-learn, please refer to ourparallelism notes.
joblib
pos_label
Value with which positive labels must be encoded in binary
classification problems in which the positive class is not assumed.
This value is typically required to compute asymmetric evaluation
metrics such as precision and recall.
random_state
Whenever randomization is part of a Scikit-learn algorithm, arandom_stateparameter may be provided to control the random number
generator used.  Note that the mere presence ofrandom_statedoesn’t
mean that randomization is always used, as it may be dependent on
another parameter, e.g.shuffle, being set.
random_state
random_state
shuffle
The passed value will have an effect on the reproducibility of the
results returned by the function (fit,split, or any
other function likek_means).random_state’s
value may be:
k_means
random_state
Use the global random state instance fromnumpy.random.
Calling the function multiple times will reuse
the same instance, and will produce different results.
numpy.random
Use a new random number generator seeded by the given integer.
Using an int will produce the same results across different calls.
However, it may be
worthwhile checking that your results are stable across a
number of different distinct random seeds. Popular integer
random seeds are 0 and42.
Integer values must be in the range[0,2**32-1].
[0,2**32-1]
numpy.random.RandomState
Use the provided random state, only affecting other users
of that same random state instance. Calling the function
multiple times will reuse the same instance, and
will produce different results.
utils.check_random_stateis used internally to validate the
inputrandom_stateand return aRandomStateinstance.
utils.check_random_state
random_state
RandomState
For more details on how to control the randomness of scikit-learn
objects and avoid common pitfalls, you may refer toControlling randomness.
scoring
Depending on the object, can specify:
the score function to be maximized (usually bycross validation),
the score function to be maximized (usually bycross validation),
the multiple score functions to be reported,
the multiple score functions to be reported,
the score function to be used to check early stopping, or
the score function to be used to check early stopping, or
for visualization related objects, the score function to output or plot
for visualization related objects, the score function to output or plot
The score function can be a string accepted
bymetrics.get_scoreror a callablescorer, not to be
confused with anevaluation metric, as the latter have a more
diverse API.scoringmay also be set to None, in which case the
estimator’sscoremethod is used.  SeeThe scoring parameter: defining model evaluation rulesin the User Guide.
metrics.get_scorer
scoring
Where multiple metrics can be evaluated,scoringmay be given
either as a list of unique strings, a dictionary with names as keys and
callables as values or a callable that returns a dictionary. Note that
this doesnotspecify which score function is to be maximized, and
another parameter such asrefitmaybe used for this purpose.
scoring
refit
Thescoringparameter is validated and interpreted usingmetrics.check_scoring.
scoring
metrics.check_scoring
verbose
Logging is not handled very consistently in Scikit-learn at present,
but when it is provided as an option, theverboseparameter is
usually available to choose no logging (set to False). Any True value
should enable some logging, but larger integers (e.g. above 10) may be
needed for full verbosity.  Verbose logs are usually printed to
Standard Output.
Estimators should not produce any output on Standard Output with the
defaultverbosesetting.
verbose
verbose
warm_start
When fitting an estimator repeatedly on the same dataset, but for
multiple parameter values (such as to find the value maximizing
performance as ingrid search), it may be possible
to reuse aspects of the model learned from the previous parameter value,
saving time.  Whenwarm_startis true, the existingfittedmodelattributesare used to initialize the new model
in a subsequent call tofit.
warm_start
Note that this is only applicable for some models and some
parameters, and even some orders of parameter values. In general, there
is an interaction betweenwarm_startand the parameter controlling
the number of iterations of the estimator.
warm_start
For estimators imported fromensemble,warm_startwill interact withn_estimatorsormax_iter.
For these models, the number of iterations, reported vialen(estimators_)orn_iter_, corresponds the total number of
estimators/iterations learnt since the initialization of the model.
Thus, if a model was already initialized withNestimators, andfitis called withn_estimatorsormax_iterset toM, the model
will trainM-Nnew estimators.
ensemble
warm_start
n_estimators
max_iter
len(estimators_)
n_iter_
N
fit
n_estimators
max_iter
M
M-N
Other models, usually using gradient-based solvers, have a different
behavior. They all expose amax_iterparameter. The reportedn_iter_corresponds to the number of iteration done during the last
call tofitand will be at mostmax_iter. Thus, we do not
consider the state of the estimator since the initialization.
max_iter
n_iter_
fit
max_iter
partial_fitalso retains the model between calls, but differs:
withwarm_startthe parameters change and the data is
(more-or-less) constant across calls tofit; withpartial_fit,
the mini-batch of data changes and model parameters stay fixed.
warm_start
fit
partial_fit
There are cases where you want to usewarm_startto fit on
different, but closely related data. For example, one may initially fit
to a subset of the data, then fine-tune the parameter search on the
full dataset. For classification, all data in a sequence ofwarm_startcalls tofitmust include samples from each class.
warm_start
warm_start
fit
Attributes#
See conceptattribute.
classes_
A list of class labels known to theclassifier, mapping each
label to a numerical index used in the model representation our output.
For instance, the array output frompredict_probahas columns
aligned withclasses_. Formulti-outputclassifiers,classes_should be a list of lists, with one class listing for
each output.  For each output, the classes should be sorted
(numerically, or lexicographically for strings).
classes_
classes_
classes_and the mapping to indices is often managed withpreprocessing.LabelEncoder.
classes_
preprocessing.LabelEncoder
components_
An affine transformation matrix of shape(n_components,n_features)used in many lineartransformerswheren_componentsis
the number of output features andn_featuresis the number of
input features.
(n_components,n_features)
See alsocoef_which is a similar attribute for linear
predictors.
coef_
The weight/coefficient matrix of a generalized linear modelpredictor, of shape(n_features,)for binary classification
and single-output regression,(n_classes,n_features)for
multiclass classification and(n_targets,n_features)for
multi-output regression. Note this does not include the intercept
(or bias) term, which is stored inintercept_.
(n_features,)
(n_classes,n_features)
(n_targets,n_features)
intercept_
When available,feature_importances_is not usually provided as
well, but can be calculated as the  norm of each feature’s entry incoef_.
feature_importances_
coef_
See alsocomponents_which is a similar attribute for linear
transformers.
embedding_
An embedding of the training data inmanifold learningestimators, with shape(n_samples,n_components),
identical to the output offit_transform.  See alsolabels_.
(n_samples,n_components)
n_iter_
The number of iterations actually performed when fitting an iterative
estimator that may stop upon convergence. See alsomax_iter.
feature_importances_
A vector of shape(n_features,)available in somepredictorsto provide a relative measure of the importance of
each feature in the predictions of the model.
(n_features,)
labels_
A vector containing a cluster label for each sample of the training
data inclusterers, identical to the output offit_predict.  See alsoembedding_.
Data and sample properties#
See conceptsample property.
groups
Used in cross-validation routines to identify samples that are correlated.
Each value is an identifier such that, in a supportingCV splitter, samples from somegroupsvalue may not
appear in both a training set and its corresponding test set.
SeeCross-validation iterators for grouped data.
groups
sample_weight
A relative weight for each sample.  Intuitively, if all weights are
integers, a weighted model or score should be equivalent to that
calculated when repeating the sample the number of times specified in
the weight.  Weights may be specified as floats, so that sample weights
are usually equivalent up to a constant positive scaling factor.
FIXME  Is this interpretation always the case in practice? We have no
common tests.
Some estimators, such as decision trees, support negative weights.
FIXME: This feature or its absence may not be tested or documented in
many estimators.
This is not entirely the case where other parameters of the model
consider the number of samples in a region, as withmin_samplesincluster.DBSCAN.  In this case, a count of samples becomes
to a sum of their weights.
min_samples
cluster.DBSCAN
In classification, sample weights can also be specified as a function
of class with theclass_weightestimatorparameter.
X
Denotes data that is observed at training and prediction time, used as
independent variables in learning.  The notation is uppercase to denote
that it is ordinarily a matrix (seerectangular).
When a matrix, each sample may be represented by afeaturevector, or a vector ofprecomputed(dis)similarity with each
training sample.Xmay also not be a matrix, and may require afeature extractoror apairwise metricto turn it into
one before learning a model.
X
Xt
Shorthand for “transformedX”.
y
Y
Denotes data that may be observed at training time as the dependent
variable in learning, but which is unavailable at prediction time, and
is usually thetargetof prediction.  The notation may be
uppercase to denote that it is a matrix, representingmulti-outputtargets, for instance; but usually we useyand sometimes do so even when multiple outputs are assumed.
y
This Page
Show Source

GitHub
GitHub
Developer’s Guide#
ContributingWays to contributeAutomated Contributions PolicySubmitting a bug report or a feature requestHow to make a good bug reportContributing codeVideo resourcesHow to contributePull request checklistContinuous Integration (CI)Commit message markersResolve conflicts in lock filesStalled pull requestsStalled and Unclaimed IssuesIssues for New ContributorsDocumentationBuilding the documentationGenerated documentation on GitHub ActionsTesting and improving test coverageMonitoring performanceIssue Tracker TagsMaintaining backwards compatibilityDeprecationChange the default value of a parameterCode Review GuidelinesReading the existing code base
Ways to contribute
Automated Contributions Policy
Submitting a bug report or a feature requestHow to make a good bug report
How to make a good bug report
Contributing codeVideo resourcesHow to contributePull request checklistContinuous Integration (CI)Commit message markersResolve conflicts in lock filesStalled pull requestsStalled and Unclaimed IssuesIssues for New Contributors
Video resources
How to contribute
Pull request checklist
Continuous Integration (CI)Commit message markersResolve conflicts in lock files
Commit message markers
Resolve conflicts in lock files
Stalled pull requests
Stalled and Unclaimed Issues
Issues for New Contributors
DocumentationBuilding the documentationGenerated documentation on GitHub Actions
Building the documentation
Generated documentation on GitHub Actions
Testing and improving test coverage
Monitoring performance
Issue Tracker Tags
Maintaining backwards compatibilityDeprecationChange the default value of a parameter
Deprecation
Change the default value of a parameter
Code Review Guidelines
Reading the existing code base
Crafting a minimal reproducer for scikit-learnGood practicesProvide a failing code example with minimal commentsBoil down your script to something as small as possibleDO NOTreport your data unless it is extremely necessaryUse markdown formattingSynthetic datasetNumPyPandasmake_regressionmake_classificationmake_blobsDataset loading utilities
Good practicesProvide a failing code example with minimal commentsBoil down your script to something as small as possibleDO NOTreport your data unless it is extremely necessaryUse markdown formatting
Provide a failing code example with minimal comments
Boil down your script to something as small as possible
DO NOTreport your data unless it is extremely necessary
Use markdown formatting
Synthetic datasetNumPyPandasmake_regressionmake_classificationmake_blobsDataset loading utilities
NumPy
Pandas
make_regression
make_regression
make_classification
make_classification
make_blobs
make_blobs
Dataset loading utilities
Developing scikit-learn estimatorsAPIs of scikit-learn objectsDifferent objectsEstimatorsInstantiationFittingEstimated AttributesUniversal attributesRolling your own estimatorget_params and set_paramsCloningEstimator typesEstimator TagsDeveloper API forset_outputDeveloper API forcheck_is_fittedDeveloper API for HTML representationCoding guidelinesInput validationRandom NumbersNumerical assertions in tests
APIs of scikit-learn objectsDifferent objectsEstimatorsInstantiationFittingEstimated AttributesUniversal attributes
Different objects
EstimatorsInstantiationFittingEstimated AttributesUniversal attributes
Instantiation
Fitting
Estimated Attributes
Universal attributes
Rolling your own estimatorget_params and set_paramsCloningEstimator typesEstimator Tags
get_params and set_params
Cloning
Estimator types
Estimator Tags
Developer API forset_output
set_output
Developer API forcheck_is_fitted
check_is_fitted
Developer API for HTML representation
Coding guidelinesInput validationRandom NumbersNumerical assertions in tests
Input validation
Random Numbers
Numerical assertions in tests
Developers’ Tips and TricksProductivity and sanity-preserving tipsFolding and unfolding outdated diffs on pull requestsChecking out pull requests as remote-tracking branchesDisplay code coverage in pull requestsUseful pytest aliases and flagsStandard replies for reviewingDebugging CI issuesUsing a lock-file to get an environment close to the CIDebugging memory errors in Cython with valgrindBuilding and testing for the ARM64 platform on a x86_64 machineThe Meson Build Backend
Productivity and sanity-preserving tipsFolding and unfolding outdated diffs on pull requestsChecking out pull requests as remote-tracking branchesDisplay code coverage in pull requestsUseful pytest aliases and flagsStandard replies for reviewingDebugging CI issuesUsing a lock-file to get an environment close to the CI
Folding and unfolding outdated diffs on pull requests
Checking out pull requests as remote-tracking branches
Display code coverage in pull requests
Useful pytest aliases and flags
Standard replies for reviewing
Debugging CI issuesUsing a lock-file to get an environment close to the CI
Using a lock-file to get an environment close to the CI
Debugging memory errors in Cython with valgrind
Building and testing for the ARM64 platform on a x86_64 machine
The Meson Build Backend
Utilities for DevelopersValidation ToolsEfficient Linear Algebra & Array OperationsEfficient Random SamplingEfficient Routines for Sparse MatricesGraph RoutinesTesting FunctionsMulticlass and multilabel utility functionHelper FunctionsHash FunctionsWarnings and Exceptions
Validation Tools
Efficient Linear Algebra & Array Operations
Efficient Random Sampling
Efficient Routines for Sparse Matrices
Graph Routines
Testing Functions
Multiclass and multilabel utility function
Helper Functions
Hash Functions
Warnings and Exceptions
How to optimize for speedPython, Cython or C/C++?Profiling Python codeMemory usage profilingUsing CythonProfiling compiled extensionsUsing yep and gperftoolsUsing a debugger, gdbUsing gprofUsing valgrind / callgrind / kcachegrindkcachegrindMulti-core parallelism usingjoblib.ParallelA simple algorithmic trick: warm restarts
Python, Cython or C/C++?
Profiling Python code
Memory usage profiling
Using Cython
Profiling compiled extensionsUsing yep and gperftoolsUsing a debugger, gdbUsing gprofUsing valgrind / callgrind / kcachegrindkcachegrind
Using yep and gperftools
Using a debugger, gdb
Using gprof
Using valgrind / callgrind / kcachegrindkcachegrind
kcachegrind
Multi-core parallelism usingjoblib.Parallel
joblib.Parallel
A simple algorithmic trick: warm restarts
Cython Best Practices, Conventions and KnowledgeTips for developing with Cython in scikit-learnTips to ease developmentTips for performanceUsing OpenMPTypes
Tips for developing with Cython in scikit-learnTips to ease developmentTips for performanceUsing OpenMPTypes
Tips to ease development
Tips for performance
Using OpenMPTypes
Types
Installing the development version of scikit-learnInstalling nightly buildsBuilding from sourceBuilding a specific version from a tagPlatform-specific instructionsWindowsmacOSmacOS compilers from conda-forgemacOS compilers from HomebrewLinuxLinux compilers from the systemLinux compilers from conda-forgeFreeBSD
Installing nightly builds
Building from sourceBuilding a specific version from a tag
Building a specific version from a tag
Platform-specific instructionsWindowsmacOSmacOS compilers from conda-forgemacOS compilers from HomebrewLinuxLinux compilers from the systemLinux compilers from conda-forgeFreeBSD
Windows
macOSmacOS compilers from conda-forgemacOS compilers from Homebrew
macOS compilers from conda-forge
macOS compilers from Homebrew
LinuxLinux compilers from the systemLinux compilers from conda-forge
Linux compilers from the system
Linux compilers from conda-forge
FreeBSD
Bug triaging and issue curationWorking on issues to improve themWorking on PRs to help reviewTriaging operations for members of the core and contributor experience teamsA typical workflow for triaging issues
Working on issues to improve them
Working on PRs to help review
Triaging operations for members of the core and contributor experience teams
A typical workflow for triaging issues
Maintainer InformationReleasingReference StepsUpdating Authors ListGuideline for bumping minimum versions of our dependenciesMerging Pull RequestsThescikit-learn.orgWebsiteExperimental Features
ReleasingReference Steps
Reference Steps
Updating Authors List
Guideline for bumping minimum versions of our dependencies
Merging Pull Requests
Thescikit-learn.orgWebsite
scikit-learn.org
Experimental Features
Developing with the Plotting APIPlotting API OverviewPlotting with Multiple Axes
Plotting API Overview
Plotting with Multiple Axes
This Page
Show Source

GitHub
GitHub
Frequently Asked Questions#
Here we try to give some answers to questions that regularly pop up on the mailing list.
About the project#
What is the project name (a lot of people get it wrong)?#
scikit-learn, but not scikit or SciKit nor sci-kit learn.
Also not scikits.learn or scikits-learn, which were previously used.
How do you pronounce the project name?#
sy-kit learn. sci stands for science!
Why scikit?#
There are multiple scikits, which are scientific toolboxes built around SciPy.
Apart from scikit-learn, another popular one isscikit-image.
Do you support PyPy?#
Due to limited maintainer resources and small number of users, using
scikit-learn withPyPy(an alternative Python
implementation with a built-in just-in-time compiler) is not officially
supported.
How can I obtain permission to use the images in scikit-learn for my work?#
The images contained in thescikit-learn repositoryand the images generated within
thescikit-learn documentationcan be used via theBSD 3-Clause Licensefor
your work. Citations of scikit-learn are highly encouraged and appreciated. Seeciting scikit-learn.
Implementation decisions#
Why is there no support for deep or reinforcement learning? Will there be such support in the future?#
Deep learning and reinforcement learning both require a rich vocabulary to
define an architecture, with deep learning additionally requiring
GPUs for efficient computing. However, neither of these fit within
the design constraints of scikit-learn. As a result, deep learning
and reinforcement learning are currently out of scope for what
scikit-learn seeks to achieve.
You can find more information about the addition of GPU support atWill you add GPU support?.
Note that scikit-learn currently implements a simple multilayer perceptron
insklearn.neural_network. We will only accept bug fixes for this module.
If you want to implement more complex deep learning models, please turn to
popular deep learning frameworks such astensorflow,keras,
andpytorch.
sklearn.neural_network
Will you add graphical models or sequence prediction to scikit-learn?#
Not in the foreseeable future.
scikit-learn tries to provide a unified API for the basic tasks in machine
learning, with pipelines and meta-algorithms like grid search to tie
everything together. The required concepts, APIs, algorithms and
expertise required for structured learning are different from what
scikit-learn has to offer. If we started doing arbitrary structured
learning, we’d need to redesign the whole package and the project
would likely collapse under its own weight.
There are two projects with API similar to scikit-learn that
do structured prediction:
pystructhandles general structured
learning (focuses on SSVMs on arbitrary graph structures with
approximate inference; defines the notion of sample as an instance of
the graph structure).
pystructhandles general structured
learning (focuses on SSVMs on arbitrary graph structures with
approximate inference; defines the notion of sample as an instance of
the graph structure).
seqlearnhandles sequences only
(focuses on exact inference; has HMMs, but mostly for the sake of
completeness; treats a feature vector as a sample and uses an offset encoding
for the dependencies between feature vectors).
seqlearnhandles sequences only
(focuses on exact inference; has HMMs, but mostly for the sake of
completeness; treats a feature vector as a sample and uses an offset encoding
for the dependencies between feature vectors).
Why did you remove HMMs from scikit-learn?#
SeeWill you add graphical models or sequence prediction to scikit-learn?.
Will you add GPU support?#
Adding GPU support by default would introduce heavy hardware-specific software
dependencies and existing algorithms would need to be reimplemented. This would
make it both harder for the average user to install scikit-learn and harder for
the developers to maintain the code.
However, since 2023, a limited but growinglist of scikit-learn
estimatorscan already run on GPUs if the input data is
provided as a PyTorch or CuPy array and if scikit-learn has been configured to
accept such inputs as explained inArray API support (experimental). This Array API support
allows scikit-learn to run on GPUs without introducing heavy and
hardware-specific software dependencies to the main package.
Most estimators that rely on NumPy for their computationally intensive operations
can be considered for Array API support and therefore GPU support.
However, not all scikit-learn estimators are amenable to efficiently running
on GPUs via the Array API for fundamental algorithmic reasons. For instance,
tree-based models currently implemented with Cython in scikit-learn are
fundamentally not array-based algorithms. Other algorithms such as k-means or
k-nearest neighbors rely on array-based algorithms but are also implemented in
Cython. Cython is used to manually interleave consecutive array operations to
avoid introducing performance killing memory access to large intermediate
arrays: this low-level algorithmic rewrite is called “kernel fusion” and cannot
be expressed via the Array API for the foreseeable future.
Adding efficient GPU support to estimators that cannot be efficiently
implemented with the Array API would require designing and adopting a more
flexible extension system for scikit-learn. This possibility is being
considered in the following GitHub issue (under discussion):
scikit-learn/scikit-learn#22438
scikit-learn/scikit-learn#22438
Why do categorical variables need preprocessing in scikit-learn, compared to other tools?#
Most of scikit-learn assumes data is in NumPy arrays or SciPy sparse matrices
of a single numeric dtype. These do not explicitly represent categorical
variables at present. Thus, unlike R’sdata.framesorpandas.DataFrame,
we require explicit conversion of categorical features to numeric values, as
discussed inEncoding categorical features.
See alsoColumn Transformer with Mixed Typesfor an
example of working with heterogeneous (e.g. categorical and numeric) data.
data.frames
pandas.DataFrame
Note that recently,HistGradientBoostingClassifierandHistGradientBoostingRegressorgained native support for
categorical features through the optioncategorical_features="from_dtype". This
option relies on inferring which columns of the data are categorical based on thepandas.CategoricalDtypeandpolars.datatypes.Categoricaldtypes.
HistGradientBoostingClassifier
HistGradientBoostingRegressor
categorical_features="from_dtype"
pandas.CategoricalDtype
polars.datatypes.Categorical
Does scikit-learn work natively with various types of dataframes?#
Scikit-learn has limited support forpandas.DataFrameandpolars.DataFrame. Scikit-learn estimators can accept both these dataframe types
as input, and scikit-learn transformers can output dataframes using theset_outputAPI. For more details, refer toIntroducing the set_output API.
pandas.DataFrame
polars.DataFrame
set_output
However, the internal computations in scikit-learn estimators rely on numerical
operations that are more efficiently performed on homogeneous data structures such as
NumPy arrays or SciPy sparse matrices. As a result, most scikit-learn estimators will
internally convert dataframe inputs into these homogeneous data structures. Similarly,
dataframe outputs are generated from these homogeneous data structures.
Also note thatColumnTransformermakes it convenient to handle
heterogeneous pandas dataframes by mapping homogeneous subsets of dataframe columns
selected by name or dtype to dedicated scikit-learn transformers. ThereforeColumnTransformerare often used in the first step of
scikit-learn pipelines when dealing with heterogeneous dataframes (seePipeline: chaining estimatorsfor more details).
ColumnTransformer
ColumnTransformer
See alsoColumn Transformer with Mixed Typesfor an example of working with heterogeneous (e.g. categorical and numeric) data.
Do you plan to implement transform for targetyin a pipeline?#
y
Currently transform only works for featuresXin a pipeline. There’s a
long-standing discussion about not being able to transformyin a pipeline.
Follow on GitHub issue#4143. Meanwhile, you can check outTransformedTargetRegressor,pipegraph,
andimbalanced-learn.
Note that scikit-learn solved for the case whereyhas an invertible transformation applied before training
and inverted after prediction. scikit-learn intends to solve for
use cases whereyshould be transformed at training time
and not at test time, for resampling and similar uses, like atimbalanced-learn.
In general, these use cases can be solved
with a custom meta estimator rather than aPipeline.
X
y
TransformedTargetRegressor
y
y
Pipeline
Why are there so many different estimators for linear models?#
Usually, there is one classifier and one regressor per model type, e.g.GradientBoostingClassifierandGradientBoostingRegressor. Both have similar options and
both have the parameterloss, which is especially useful in the regression
case as it enables the estimation of conditional mean as well as conditional
quantiles.
GradientBoostingClassifier
GradientBoostingRegressor
loss
For linear models, there are many estimator classes which are very close to
each other. Let us have a look at
LinearRegression, no penalty
LinearRegression, no penalty
LinearRegression
Ridge, L2 penalty
Ridge, L2 penalty
Ridge
Lasso, L1 penalty (sparse models)
Lasso, L1 penalty (sparse models)
Lasso
ElasticNet, L1 + L2 penalty (less sparse models)
ElasticNet, L1 + L2 penalty (less sparse models)
ElasticNet
SGDRegressorwithloss="squared_loss"
SGDRegressorwithloss="squared_loss"
SGDRegressor
loss="squared_loss"
Maintainer perspective:They all do in principle the same and are different only by the penalty they
impose. This, however, has a large impact on the way the underlying
optimization problem is solved. In the end, this amounts to usage of different
methods and tricks from linear algebra. A special case isSGDRegressorwhich
comprises all 4 previous models and is different by the optimization procedure.
A further side effect is that the different estimators favor different data
layouts (XC-contiguous or F-contiguous, sparse csr or csc). This complexity
of the seemingly simple linear models is the reason for having different
estimator classes for different penalties.
SGDRegressor
X
User perspective:First, the current design is inspired by the scientific literature where linear
regression models with different regularization/penalty were given different
names, e.g.ridge regression. Having different model classes with according
names makes it easier for users to find those regression models.
Secondly, if all the 5 above mentioned linear models were unified into a single
class, there would be parameters with a lot of options like thesolverparameter. On top of that, there would be a lot of exclusive interactions
between different parameters. For example, the possible options of the
parameterssolver,precomputeandselectionwould depend on the
chosen values of the penalty parametersalphaandl1_ratio.
solver
solver
precompute
selection
alpha
l1_ratio
Contributing#
How can I contribute to scikit-learn?#
SeeContributing. Before wanting to add a new algorithm, which is
usually a major and lengthy undertaking, it is recommended to start withknown issues. Please do not contact the contributors
of scikit-learn directly regarding contributing to scikit-learn.
Why is my pull request not getting any attention?#
The scikit-learn review process takes a significant amount of time, and
contributors should not be discouraged by a lack of activity or review on
their pull request. We care a lot about getting things right
the first time, as maintenance and later change comes at a high cost.
We rarely release any “experimental” code, so all of our contributions
will be subject to high use immediately and should be of the highest
quality possible initially.
Beyond that, scikit-learn is limited in its reviewing bandwidth; many of the
reviewers and core developers are working on scikit-learn on their own time.
If a review of your pull request comes slowly, it is likely because the
reviewers are busy. We ask for your understanding and request that you
not close your pull request or discontinue your work solely because of
this reason.
What are the inclusion criteria for new algorithms?#
We only consider well-established algorithms for inclusion. A rule of thumb is
at least 3 years since publication, 200+ citations, and wide use and
usefulness. A technique that provides a clear-cut improvement (e.g. an
enhanced data structure or a more efficient approximation technique) on
a widely-used method will also be considered for inclusion.
From the algorithms or techniques that meet the above criteria, only those
which fit well within the current API of scikit-learn, that is afit,predict/transforminterface and ordinarily having input/output that is a
numpy array or sparse matrix, are accepted.
fit
predict/transform
The contributor should support the importance of the proposed addition with
research papers and/or implementations in other similar packages, demonstrate
its usefulness via common use-cases/applications and corroborate performance
improvements, if any, with benchmarks and/or plots. It is expected that the
proposed algorithm should outperform the methods that are already implemented
in scikit-learn at least in some areas.
Inclusion of a new algorithm speeding up an existing model is easier if:
it does not introduce new hyper-parameters (as it makes the library
more future-proof),
it does not introduce new hyper-parameters (as it makes the library
more future-proof),
it is easy to document clearly when the contribution improves the speed
and when it does not, for instance, “whenn_features>>n_samples”,
it is easy to document clearly when the contribution improves the speed
and when it does not, for instance, “whenn_features>>n_samples”,
n_features>>n_samples
benchmarks clearly show a speed up.
benchmarks clearly show a speed up.
Also, note that your implementation need not be in scikit-learn to be used
together with scikit-learn tools. You can implement your favorite algorithm
in a scikit-learn compatible way, upload it to GitHub and let us know. We
will be happy to list it underRelated Projects. If you already have
a package on GitHub following the scikit-learn API, you may also be
interested to look atscikit-learn-contrib.
Why are you so selective on what algorithms you include in scikit-learn?#
Code comes with maintenance cost, and we need to balance the amount of
code we have with the size of the team (and add to this the fact that
complexity scales non linearly with the number of features).
The package relies on core developers using their free time to
fix bugs, maintain code and review contributions.
Any algorithm that is added needs future attention by the developers,
at which point the original author might long have lost interest.
See alsoWhat are the inclusion criteria for new algorithms?. For a great read about
long-term maintenance issues in open-source software, look atthe Executive Summary of Roads and Bridges.
Using scikit-learn#
What’s the best way to get help on scikit-learn usage?#
General machine learning questions: useCross Validatedwith the[machine-learning]tag.
General machine learning questions: useCross Validatedwith the[machine-learning]tag.
[machine-learning]
scikit-learn usage questions: useStack Overflowwith the[scikit-learn]and[python]tags. You can alternatively use themailing list.
scikit-learn usage questions: useStack Overflowwith the[scikit-learn]and[python]tags. You can alternatively use themailing list.
[scikit-learn]
[python]
Please make sure to include a minimal reproduction code snippet (ideally shorter
than 10 lines) that highlights your problem on a toy dataset (for instance fromsklearn.datasetsor randomly generated with functions ofnumpy.randomwith
a fixed random seed). Please remove any line of code that is not necessary to
reproduce your problem.
sklearn.datasets
numpy.random
The problem should be reproducible by simply copy-pasting your code snippet in a Python
shell with scikit-learn installed. Do not forget to include the import statements.
More guidance to write good reproduction code snippets can be found at:https://stackoverflow.com/help/mcve.
If your problem raises an exception that you do not understand (even after googling it),
please make sure to include the full traceback that you obtain when running the
reproduction script.
For bug reports or feature requests, please make use of theissue tracker on GitHub.
Warning
Please do not email any authors directly to ask for assistance, report bugs,
or for any other issue related to scikit-learn.
How should I save, export or deploy estimators for production?#
SeeModel persistence.
How can I create a bunch object?#
Bunch objects are sometimes used as an output for functions and methods. They
extend dictionaries by enabling values to be accessed by key,bunch["value_key"], or by an attribute,bunch.value_key.
bunch["value_key"]
bunch.value_key
They should not be used as an input. Therefore you almost never need to create
aBunchobject, unless you are extending scikit-learn’s API.
Bunch
How can I load my own datasets into a format usable by scikit-learn?#
Generally, scikit-learn works on any numeric data stored as numpy arrays
or scipy sparse matrices. Other types that are convertible to numeric
arrays such aspandas.DataFrameare also acceptable.
pandas.DataFrame
For more information on loading your data files into these usable data
structures, please refer toloading external datasets.
How do I deal with string data (or trees, graphs…)?#
scikit-learn estimators assume you’ll feed them real-valued feature vectors.
This assumption is hard-coded in pretty much all of the library.
However, you can feed non-numerical inputs to estimators in several ways.
If you have text documents, you can use a term frequency features; seeText feature extractionfor the built-intext vectorizers.
For more general feature extraction from any kind of data, seeLoading features from dictsandFeature hashing.
Another common case is when you have non-numerical data and a custom distance
(or similarity) metric on these data. Examples include strings with edit
distance (aka. Levenshtein distance), for instance, DNA or RNA sequences. These can be
encoded as numbers, but doing so is painful and error-prone. Working with
distance metrics on arbitrary data can be done in two ways.
Firstly, many estimators take precomputed distance/similarity matrices, so if
the dataset is not too large, you can compute distances for all pairs of inputs.
If the dataset is large, you can use feature vectors with only one “feature”,
which is an index into a separate data structure, and supply a custom metric
function that looks up the actual data in this data structure. For instance, to usedbscanwith Levenshtein distances:
dbscan
>>>importnumpyasnp>>>fromlevenimportlevenshtein>>>fromsklearn.clusterimportdbscan>>>data=["ACCTCCTAGAAG","ACCTACTAGAAGTT","GAATATTAGGCCGA"]>>>deflev_metric(x,y):...i,j=int(x[0]),int(y[0])# extract indices...returnlevenshtein(data[i],data[j])...>>>X=np.arange(len(data)).reshape(-1,1)>>>Xarray([[0],[1],[2]])>>># We need to specify algorithm='brute' as the default assumes>>># a continuous feature space.>>>dbscan(X,metric=lev_metric,eps=5,min_samples=2,algorithm='brute')(array([0, 1]), array([ 0,  0, -1]))
Note that the example above uses the third-party edit distance packageleven. Similar tricks can be used,
with some care, for tree kernels, graph kernels, etc.
Why do I sometimes get a crash/freeze withn_jobs>1under OSX or Linux?#
n_jobs>1
Several scikit-learn tools such asGridSearchCVandcross_val_scorerely internally on Python’smultiprocessingmodule to parallelize execution
onto several Python processes by passingn_jobs>1as an argument.
GridSearchCV
cross_val_score
multiprocessing
n_jobs>1
The problem is that Pythonmultiprocessingdoes aforksystem call
without following it with anexecsystem call for performance reasons. Many
libraries like (some versions of) Accelerate or vecLib under OSX, (some versions
of) MKL, the OpenMP runtime of GCC, nvidia’s Cuda (and probably many others),
manage their own internal thread pool. Upon a call tofork, the thread pool
state in the child process is corrupted: the thread pool believes it has many
threads while only the main thread state has been forked. It is possible to
change the libraries to make them detect when a fork happens and reinitialize
the thread pool in that case: we did that for OpenBLAS (merged upstream in
main since 0.2.10) and we contributed apatchto GCC’s OpenMP runtime
(not yet reviewed).
multiprocessing
fork
exec
fork
But in the end the real culprit is Python’smultiprocessingthat doesforkwithoutexecto reduce the overhead of starting and using new
Python processes for parallel computing. Unfortunately this is a violation of
the POSIX standard and therefore some software editors like Apple refuse to
consider the lack of fork-safety in Accelerate and vecLib as a bug.
multiprocessing
fork
exec
In Python 3.4+ it is now possible to configuremultiprocessingto
use the"forkserver"or"spawn"start methods (instead of the default"fork") to manage the process pools. To work around this issue when
using scikit-learn, you can set theJOBLIB_START_METHODenvironment
variable to"forkserver". However the user should be aware that using
the"forkserver"method preventsjoblib.Parallelto call function
interactively defined in a shell session.
multiprocessing
"forkserver"
"spawn"
"fork"
JOBLIB_START_METHOD
"forkserver"
"forkserver"
joblib.Parallel
If you have custom code that usesmultiprocessingdirectly instead of using
it viajoblibyou can enable the"forkserver"mode globally for your
program. Insert the following instructions in your main script:
multiprocessing
joblib
"forkserver"
importmultiprocessing# other imports, custom code, load data, define model...if__name__=="__main__":multiprocessing.set_start_method("forkserver")# call scikit-learn utils with n_jobs > 1 here
You can find more default on the new start methods in themultiprocessing
documentation.
Why does my job use more cores than specified withn_jobs?#
n_jobs
This is becausen_jobsonly controls the number of jobs for
routines that are parallelized withjoblib, but parallel code can come
from other sources:
n_jobs
joblib
some routines may be parallelized with OpenMP (for code written in C or
Cython),
some routines may be parallelized with OpenMP (for code written in C or
Cython),
scikit-learn relies a lot on numpy, which in turn may rely on numerical
libraries like MKL, OpenBLAS or BLIS which can provide parallel
implementations.
scikit-learn relies a lot on numpy, which in turn may rely on numerical
libraries like MKL, OpenBLAS or BLIS which can provide parallel
implementations.
For more details, please refer to ournotes on parallelism.
How do I set arandom_statefor an entire execution?#
random_state
Please refer toControlling randomness.
This Page
Show Source

GitHub
GitHub
Support#
There are several channels to connect with scikit-learn developers for assistance, feedback, or contributions.
Note: Communications on all channels should respect ourCode of Conduct.
Mailing Lists#
Main Mailing List: Join the primary discussion
platform for scikit-learn atscikit-learn Mailing List.
Main Mailing List: Join the primary discussion
platform for scikit-learn atscikit-learn Mailing List.
Commit Updates: Stay informed about repository
updates and test failures on thescikit-learn-commits list.
Commit Updates: Stay informed about repository
updates and test failures on thescikit-learn-commits list.
User Questions#
If you have questions, this is our general workflow.
Stack Overflow: Some scikit-learn developers support users using the[scikit-learn]tag.
Stack Overflow: Some scikit-learn developers support users using the[scikit-learn]tag.
General Machine Learning Queries: For broader machine learning
discussions, visitStack Exchange.
General Machine Learning Queries: For broader machine learning
discussions, visitStack Exchange.
When posting questions:
Please use a descriptive question in the title field (e.g. no “Please
help with scikit-learn!” as this is not a question)
Please use a descriptive question in the title field (e.g. no “Please
help with scikit-learn!” as this is not a question)
Provide detailed context, expected results, and actual observations.
Provide detailed context, expected results, and actual observations.
Include code and data snippets (preferably minimalistic scripts,
up to ~20 lines).
Include code and data snippets (preferably minimalistic scripts,
up to ~20 lines).
Describe your data and preprocessing steps, including sample size,
feature types (categorical or numerical), and the target for supervised
learning tasks (classification type or regression).
Describe your data and preprocessing steps, including sample size,
feature types (categorical or numerical), and the target for supervised
learning tasks (classification type or regression).
Note: Avoid asking user questions on the bug tracker to keep
the focus on development.
GitHub DiscussionsUsage questions such as methodological
GitHub DiscussionsUsage questions such as methodological
Stack OverflowProgramming/user questions with[scikit-learn]tag
Stack OverflowProgramming/user questions with[scikit-learn]tag
[scikit-learn]
GitHub Bug TrackerBug reports - Please do not ask usage questions on the issue tracker.
GitHub Bug TrackerBug reports - Please do not ask usage questions on the issue tracker.
Discord ServerCurrent pull requests - Post any specific PR-related questions on your PR,
and you can share a link to your PR on this server.
Discord ServerCurrent pull requests - Post any specific PR-related questions on your PR,
and you can share a link to your PR on this server.
Bug Tracker#
Encountered a bug? Report it on ourissue tracker
Include in your report:
Steps or scripts to reproduce the bug.
Steps or scripts to reproduce the bug.
Expected and observed outcomes.
Expected and observed outcomes.
Python or gdb tracebacks, if applicable.
Python or gdb tracebacks, if applicable.
The ideal bug report contains ashort reproducible code snippet, this way anyone can try to reproduce the bug easily.
The ideal bug report contains ashort reproducible code snippet, this way anyone can try to reproduce the bug easily.
If your snippet is longer than around 50 lines, please link to agistor a github repo.
If your snippet is longer than around 50 lines, please link to agistor a github repo.
Tip: Gists are Git repositories; you can push data files to them using Git.
Social Media#
scikit-learn has presence on various social media platforms to share
updates with the community. The platforms are not monitored for user
questions.
Gitter#
Note: The scikit-learn Gitter room is no longer an active community.
For live discussions and support, please refer to the other channels
mentioned in this document.
Documentation Resources#
This documentation is for 1.6.1. Find documentation for other versionshere.
Older versions’ printable PDF documentation is availablehere.
Building the PDF documentation is no longer supported in the website,
but you can still generate it locally by following thebuilding documentation instructions.
This Page
Show Source

GitHub
GitHub
Related Projects#
Projects implementing the scikit-learn estimator API are encouraged to use
thescikit-learn-contrib templatewhich facilitates best practices for testing and documenting estimators.
Thescikit-learn-contrib GitHub organizationalso accepts high-quality contributions of repositories conforming to this
template.
Below is a list of sister-projects, extensions and domain specific packages.
Interoperability and framework enhancements#
These tools adapt scikit-learn for use with other technologies or otherwise
enhance the functionality of scikit-learn’s estimators.
Auto-ML
auto-sklearnAn automated machine learning toolkit and a drop-in replacement for a
scikit-learn estimator
auto-sklearnAn automated machine learning toolkit and a drop-in replacement for a
scikit-learn estimator
autovimlAutomatically Build Multiple Machine Learning Models with a Single Line of Code.
Designed as a faster way to use scikit-learn models without having to preprocess data.
autovimlAutomatically Build Multiple Machine Learning Models with a Single Line of Code.
Designed as a faster way to use scikit-learn models without having to preprocess data.
TPOTAn automated machine learning toolkit that optimizes a series of scikit-learn
operators to design a machine learning pipeline, including data and feature
preprocessors as well as the estimators. Works as a drop-in replacement for a
scikit-learn estimator.
TPOTAn automated machine learning toolkit that optimizes a series of scikit-learn
operators to design a machine learning pipeline, including data and feature
preprocessors as well as the estimators. Works as a drop-in replacement for a
scikit-learn estimator.
FeaturetoolsA framework to perform automated feature engineering. It can be used for
transforming temporal and relational datasets into feature matrices for
machine learning.
FeaturetoolsA framework to perform automated feature engineering. It can be used for
transforming temporal and relational datasets into feature matrices for
machine learning.
EvalMLEvalML is an AutoML library which builds, optimizes, and evaluates
machine learning pipelines using domain-specific objective functions.
It incorporates multiple modeling libraries under one API, and
the objects that EvalML creates use an sklearn-compatible API.
EvalMLEvalML is an AutoML library which builds, optimizes, and evaluates
machine learning pipelines using domain-specific objective functions.
It incorporates multiple modeling libraries under one API, and
the objects that EvalML creates use an sklearn-compatible API.
MLJAR AutoMLPython package for AutoML on Tabular Data with Feature Engineering,
Hyper-Parameters Tuning, Explanations and Automatic Documentation.
MLJAR AutoMLPython package for AutoML on Tabular Data with Feature Engineering,
Hyper-Parameters Tuning, Explanations and Automatic Documentation.
Experimentation and model registry frameworks
MLFlowMLflow is an open source platform to manage the ML
lifecycle, including experimentation, reproducibility, deployment, and a central
model registry.
MLFlowMLflow is an open source platform to manage the ML
lifecycle, including experimentation, reproducibility, deployment, and a central
model registry.
NeptuneMetadata store for MLOps,
built for teams that run a lot of experiments. It gives you a single
place to log, store, display, organize, compare, and query all your
model building metadata.
NeptuneMetadata store for MLOps,
built for teams that run a lot of experiments. It gives you a single
place to log, store, display, organize, compare, and query all your
model building metadata.
SacredTool to help you configure,
organize, log and reproduce experiments
SacredTool to help you configure,
organize, log and reproduce experiments
Scikit-Learn LaboratoryA command-line
wrapper around scikit-learn that makes it easy to run machine learning
experiments with multiple learners and large feature sets.
Scikit-Learn LaboratoryA command-line
wrapper around scikit-learn that makes it easy to run machine learning
experiments with multiple learners and large feature sets.
Model inspection and visualization
dtreevizA python library for
decision tree visualization and model interpretation.
dtreevizA python library for
decision tree visualization and model interpretation.
sklearn-evaluationMachine learning model evaluation made easy: plots, tables, HTML reports,
experiment tracking and Jupyter notebook analysis. Visual analysis, model
selection, evaluation and diagnostics.
sklearn-evaluationMachine learning model evaluation made easy: plots, tables, HTML reports,
experiment tracking and Jupyter notebook analysis. Visual analysis, model
selection, evaluation and diagnostics.
yellowbrickA suite of
custom matplotlib visualizers for scikit-learn estimators to support visual feature
analysis, model selection, evaluation, and diagnostics.
yellowbrickA suite of
custom matplotlib visualizers for scikit-learn estimators to support visual feature
analysis, model selection, evaluation, and diagnostics.
Model export for production
sklearn-onnxSerialization of many
Scikit-learn pipelines toONNXfor interchange and
prediction.
sklearn-onnxSerialization of many
Scikit-learn pipelines toONNXfor interchange and
prediction.
skops.ioA
persistence model more secure than pickle, which can be used instead of
pickle in most common cases.
skops.ioA
persistence model more secure than pickle, which can be used instead of
pickle in most common cases.
sklearn2pmmlSerialization of a wide variety of scikit-learn estimators and transformers
into PMML with the help ofJPMML-SkLearnlibrary.
sklearn2pmmlSerialization of a wide variety of scikit-learn estimators and transformers
into PMML with the help ofJPMML-SkLearnlibrary.
treeliteCompiles tree-based ensemble models into C code for minimizing prediction
latency.
treeliteCompiles tree-based ensemble models into C code for minimizing prediction
latency.
emlearnImplements scikit-learn estimators in C99 for embedded devices and microcontrollers.
Supports several classifier, regression and outlier detection models.
emlearnImplements scikit-learn estimators in C99 for embedded devices and microcontrollers.
Supports several classifier, regression and outlier detection models.
Model throughput
Intel(R) Extension for scikit-learnMostly on high end Intel(R) hardware, accelerates some scikit-learn models
for both training and inference under certain circumstances. This project is
maintained by Intel(R) and scikit-learn’s maintainers are not involved in the
development of this project. Also note that in some cases using the tools and
estimators underscikit-learn-intelexwould give different results thanscikit-learnitself. If you encounter issues while using this project,
make sure you report potential issues in their respective repositories.
Intel(R) Extension for scikit-learnMostly on high end Intel(R) hardware, accelerates some scikit-learn models
for both training and inference under certain circumstances. This project is
maintained by Intel(R) and scikit-learn’s maintainers are not involved in the
development of this project. Also note that in some cases using the tools and
estimators underscikit-learn-intelexwould give different results thanscikit-learnitself. If you encounter issues while using this project,
make sure you report potential issues in their respective repositories.
scikit-learn-intelex
scikit-learn
Interface to R with genomic applications
BiocSklearnExposes a small number of dimension reduction facilities as an illustration
of the basilisk protocol for interfacing python with R. Intended as a
springboard for more complete interop.
BiocSklearnExposes a small number of dimension reduction facilities as an illustration
of the basilisk protocol for interfacing python with R. Intended as a
springboard for more complete interop.
Other estimators and tasks#
Not everything belongs or is mature enough for the central scikit-learn
project. The following are projects providing interfaces similar to
scikit-learn for additional learning algorithms, infrastructures
and tasks.
Time series and forecasting
DartsDarts is a Python library for
user-friendly forecasting and anomaly detection on time series. It contains a variety
of models, from classics such as ARIMA to deep neural networks. The forecasting
models can all be used in the same way, using fit() and predict() functions, similar
to scikit-learn.
DartsDarts is a Python library for
user-friendly forecasting and anomaly detection on time series. It contains a variety
of models, from classics such as ARIMA to deep neural networks. The forecasting
models can all be used in the same way, using fit() and predict() functions, similar
to scikit-learn.
sktimeA scikit-learn compatible
toolbox for machine learning with time series including time series
classification/regression and (supervised/panel) forecasting.
sktimeA scikit-learn compatible
toolbox for machine learning with time series including time series
classification/regression and (supervised/panel) forecasting.
skforecastA python library
that eases using scikit-learn regressors as multi-step forecasters. It also works
with any regressor compatible with the scikit-learn API.
skforecastA python library
that eases using scikit-learn regressors as multi-step forecasters. It also works
with any regressor compatible with the scikit-learn API.
tslearnA machine learning library for
time series that offers tools for pre-processing and feature extraction as well as
dedicated models for clustering, classification and regression.
tslearnA machine learning library for
time series that offers tools for pre-processing and feature extraction as well as
dedicated models for clustering, classification and regression.
Gradient (tree) boosting
Note scikit-learn own modern gradient boosting estimatorsHistGradientBoostingClassifierandHistGradientBoostingRegressor.
HistGradientBoostingClassifier
HistGradientBoostingRegressor
XGBoostXGBoost is an optimized distributed
gradient boosting library designed to be highly efficient, flexible and portable.
XGBoostXGBoost is an optimized distributed
gradient boosting library designed to be highly efficient, flexible and portable.
LightGBMLightGBM is a gradient boosting
framework that uses tree based learning algorithms. It is designed to be distributed
and efficient.
LightGBMLightGBM is a gradient boosting
framework that uses tree based learning algorithms. It is designed to be distributed
and efficient.
Structured learning
HMMLearnImplementation of hidden
markov models that was previously part of scikit-learn.
HMMLearnImplementation of hidden
markov models that was previously part of scikit-learn.
pomegranateProbabilistic modelling
for Python, with an emphasis on hidden Markov models.
pomegranateProbabilistic modelling
for Python, with an emphasis on hidden Markov models.
Deep neural networks etc.
skorchA scikit-learn compatible
neural network library that wraps PyTorch.
skorchA scikit-learn compatible
neural network library that wraps PyTorch.
scikerasprovides a wrapper around
Keras to interface it with scikit-learn. SciKeras is the successor
oftf.keras.wrappers.scikit_learn.
scikerasprovides a wrapper around
Keras to interface it with scikit-learn. SciKeras is the successor
oftf.keras.wrappers.scikit_learn.
tf.keras.wrappers.scikit_learn
Federated Learning
FlowerA friendly federated learning framework with a
unified approach that can federate any workload, any ML framework, and any programming language.
FlowerA friendly federated learning framework with a
unified approach that can federate any workload, any ML framework, and any programming language.
Privacy Preserving Machine Learning
Concrete MLA privacy preserving
ML framework built on top ofConcrete, with bindings to traditional ML
frameworks, thanks to fully homomorphic encryption. APIs of so-called
Concrete ML built-in models are very close to scikit-learn APIs.
Concrete MLA privacy preserving
ML framework built on top ofConcrete, with bindings to traditional ML
frameworks, thanks to fully homomorphic encryption. APIs of so-called
Concrete ML built-in models are very close to scikit-learn APIs.
Broad scope
mlxtendIncludes a number of additional
estimators as well as model visualization utilities.
mlxtendIncludes a number of additional
estimators as well as model visualization utilities.
scikit-legoA number of scikit-learn compatible
custom transformers, models and metrics, focusing on solving practical industry tasks.
scikit-legoA number of scikit-learn compatible
custom transformers, models and metrics, focusing on solving practical industry tasks.
Other regression and classification
py-earthMultivariate
adaptive regression splines
py-earthMultivariate
adaptive regression splines
gplearnGenetic Programming
for symbolic regression tasks.
gplearnGenetic Programming
for symbolic regression tasks.
scikit-multilearnMulti-label classification with focus on label space manipulation.
scikit-multilearnMulti-label classification with focus on label space manipulation.
Decomposition and clustering
lda: Fast implementation of latent
Dirichlet allocation in Cython which usesGibbs samplingto sample from the true
posterior distribution. (scikit-learn’sLatentDirichletAllocationimplementation usesvariational inferenceto sample from
a tractable approximation of a topic model’s posterior distribution.)
lda: Fast implementation of latent
Dirichlet allocation in Cython which usesGibbs samplingto sample from the true
posterior distribution. (scikit-learn’sLatentDirichletAllocationimplementation usesvariational inferenceto sample from
a tractable approximation of a topic model’s posterior distribution.)
LatentDirichletAllocation
kmodesk-modes clustering algorithm for
categorical data, and several of its variations.
kmodesk-modes clustering algorithm for
categorical data, and several of its variations.
hdbscanHDBSCAN and Robust Single
Linkage clustering algorithms for robust variable density clustering.
As of scikit-learn version 1.3.0, there isHDBSCAN.
hdbscanHDBSCAN and Robust Single
Linkage clustering algorithms for robust variable density clustering.
As of scikit-learn version 1.3.0, there isHDBSCAN.
HDBSCAN
Pre-processing
categorical-encodingA
library of sklearn compatible categorical variable encoders.
As of scikit-learn version 1.3.0, there isTargetEncoder.
categorical-encodingA
library of sklearn compatible categorical variable encoders.
As of scikit-learn version 1.3.0, there isTargetEncoder.
TargetEncoder
imbalanced-learnVarious
methods to under- and over-sample datasets.
imbalanced-learnVarious
methods to under- and over-sample datasets.
Feature-engineA library
of sklearn compatible transformers for missing data imputation, categorical
encoding, variable transformation, discretization, outlier handling and more.
Feature-engine allows the application of preprocessing steps to selected groups
of variables and it is fully compatible with the Scikit-learn Pipeline.
Feature-engineA library
of sklearn compatible transformers for missing data imputation, categorical
encoding, variable transformation, discretization, outlier handling and more.
Feature-engine allows the application of preprocessing steps to selected groups
of variables and it is fully compatible with the Scikit-learn Pipeline.
Topological Data Analysis
giotto-tdaA library forTopological Data Analysisaiming to
provide a scikit-learn compatible API. It offers tools to transform data
inputs (point clouds, graphs, time series, images) into forms suitable for
computations of topological summaries, and components dedicated to
extracting sets of scalar features of topological origin, which can be used
alongside other feature extraction methods in scikit-learn.
giotto-tdaA library forTopological Data Analysisaiming to
provide a scikit-learn compatible API. It offers tools to transform data
inputs (point clouds, graphs, time series, images) into forms suitable for
computations of topological summaries, and components dedicated to
extracting sets of scalar features of topological origin, which can be used
alongside other feature extraction methods in scikit-learn.
Statistical learning with Python#
Other packages useful for data analysis and machine learning.
PandasTools for working with heterogeneous and
columnar data, relational queries, time series and basic statistics.
PandasTools for working with heterogeneous and
columnar data, relational queries, time series and basic statistics.
statsmodelsEstimating and analysing
statistical models. More focused on statistical tests and less on prediction
than scikit-learn.
statsmodelsEstimating and analysing
statistical models. More focused on statistical tests and less on prediction
than scikit-learn.
PyMCBayesian statistical models and
fitting algorithms.
PyMCBayesian statistical models and
fitting algorithms.
SeabornVisualization library based on
matplotlib. It provides a high-level interface for drawing attractive statistical graphics.
SeabornVisualization library based on
matplotlib. It provides a high-level interface for drawing attractive statistical graphics.
scikit-survivalA library implementing
models to learn from censored time-to-event data (also called survival analysis).
Models are fully compatible with scikit-learn.
scikit-survivalA library implementing
models to learn from censored time-to-event data (also called survival analysis).
Models are fully compatible with scikit-learn.
Recommendation Engine packages#
implicit, Library for implicit
feedback datasets.
implicit, Library for implicit
feedback datasets.
lightfmA Python/Cython
implementation of a hybrid recommender system.
lightfmA Python/Cython
implementation of a hybrid recommender system.
Surprise LibLibrary for explicit feedback
datasets.
Surprise LibLibrary for explicit feedback
datasets.
Domain specific packages#
scikit-networkMachine learning on graphs.
scikit-networkMachine learning on graphs.
scikit-imageImage processing and computer
vision in python.
scikit-imageImage processing and computer
vision in python.
Natural language toolkit (nltk)Natural language
processing and some machine learning.
Natural language toolkit (nltk)Natural language
processing and some machine learning.
gensimA library for topic modelling,
document indexing and similarity retrieval
gensimA library for topic modelling,
document indexing and similarity retrieval
NiLearnMachine learning for neuro-imaging.
NiLearnMachine learning for neuro-imaging.
AstroMLMachine learning for astronomy.
AstroMLMachine learning for astronomy.
Translations of scikit-learn documentation#
Translation’s purpose is to ease reading and understanding in languages
other than English. Its aim is to help people who do not understand English
or have doubts about its interpretation. Additionally, some people prefer
to read documentation in their native language, but please bear in mind that
the only official documentation is the English one[1].
Those translation efforts are community initiatives and we have no control
on them.
If you want to contribute or report an issue with the translation, please
contact the authors of the translation.
Some available translations are linked here to improve their dissemination
and promote community efforts.
Chinese translation(source)
Chinese translation(source)
Persian translation(source)
Persian translation(source)
Spanish translation(source)
Spanish translation(source)
Korean translation(source)
Korean translation(source)
Footnotes
followinglinux documentation Disclaimer
This Page
Show Source

GitHub
GitHub
Roadmap#
Purpose of this document#
This document list general directions that core contributors are interested
to see developed in scikit-learn. The fact that an item is listed here is in
no way a promise that it will happen, as resources are limited. Rather, it
is an indication that help is welcomed on this topic.
Statement of purpose: Scikit-learn in 2018#
Eleven years after the inception of Scikit-learn, much has changed in the
world of machine learning. Key changes include:
Computational tools: The exploitation of GPUs, distributed programming
frameworks like Scala/Spark, etc.
Computational tools: The exploitation of GPUs, distributed programming
frameworks like Scala/Spark, etc.
High-level Python libraries for experimentation, processing and data
management: Jupyter notebook, Cython, Pandas, Dask, Numba…
High-level Python libraries for experimentation, processing and data
management: Jupyter notebook, Cython, Pandas, Dask, Numba…
Changes in the focus of machine learning research: artificial intelligence
applications (where input structure is key) with deep learning,
representation learning, reinforcement learning, domain transfer, etc.
Changes in the focus of machine learning research: artificial intelligence
applications (where input structure is key) with deep learning,
representation learning, reinforcement learning, domain transfer, etc.
A more subtle change over the last decade is that, due to changing interests
in ML, PhD students in machine learning are more likely to contribute to
PyTorch, Dask, etc. than to Scikit-learn, so our contributor pool is very
different to a decade ago.
Scikit-learn remains very popular in practice for trying out canonical
machine learning techniques, particularly for applications in experimental
science and in data science. A lot of what we provide is now very mature.
But it can be costly to maintain, and we cannot therefore include arbitrary
new implementations. Yet Scikit-learn is also essential in defining an API
framework for the development of interoperable machine learning components
external to the core library.
Thus our main goals in this era are to:
continue maintaining a high-quality, well-documented collection of canonical
tools for data processing and machine learning within the current scope
(i.e. rectangular data largely invariant to column and row order;
predicting targets with simple structure)
continue maintaining a high-quality, well-documented collection of canonical
tools for data processing and machine learning within the current scope
(i.e. rectangular data largely invariant to column and row order;
predicting targets with simple structure)
improve the ease for users to develop and publish external components
improve the ease for users to develop and publish external components
improve interoperability with modern data science tools (e.g. Pandas, Dask)
and infrastructures (e.g. distributed processing)
improve interoperability with modern data science tools (e.g. Pandas, Dask)
and infrastructures (e.g. distributed processing)
Many of the more fine-grained goals can be found under theAPI tagon the issue tracker.
Architectural / general goals#
The list is numbered not as an indication of the order of priority, but to
make referring to specific points easier. Please add new entries only at the
bottom. Note that the crossed out entries are already done, and we try to keep
the document up to date as we work on these issues.
Improved handling of Pandas DataFramesdocument current handling
Improved handling of Pandas DataFrames
document current handling
document current handling
Improved handling of categorical featuresTree-based models should be able to handle both continuous and categorical
features#29437.Handling mixtures of categorical and continuous variables
Improved handling of categorical features
Tree-based models should be able to handle both continuous and categorical
features#29437.
Tree-based models should be able to handle both continuous and categorical
features#29437.
Handling mixtures of categorical and continuous variables
Handling mixtures of categorical and continuous variables
Improved handling of missing dataMaking sure meta-estimators are lenient towards missing data by implementing
a common test.An amputation sample generator to make parts of a dataset go missing#6284
Improved handling of missing data
Making sure meta-estimators are lenient towards missing data by implementing
a common test.
Making sure meta-estimators are lenient towards missing data by implementing
a common test.
An amputation sample generator to make parts of a dataset go missing#6284
An amputation sample generator to make parts of a dataset go missing#6284
More didactic documentationMore and more options have been added to scikit-learn. As a result, the
documentation is crowded which makes it hard for beginners to get the big
picture. Some work could be done in prioritizing the information.
More didactic documentation
More and more options have been added to scikit-learn. As a result, the
documentation is crowded which makes it hard for beginners to get the big
picture. Some work could be done in prioritizing the information.
More and more options have been added to scikit-learn. As a result, the
documentation is crowded which makes it hard for beginners to get the big
picture. Some work could be done in prioritizing the information.
Passing around information that is not (X, y): Feature propertiesPer-feature handling (e.g. “is this a nominal / ordinal / English language
text?”) should also not need to be provided to estimator constructors,
ideally, but should be available as metadata alongside X.#8480
Passing around information that is not (X, y): Feature properties
Per-feature handling (e.g. “is this a nominal / ordinal / English language
text?”) should also not need to be provided to estimator constructors,
ideally, but should be available as metadata alongside X.#8480
Per-feature handling (e.g. “is this a nominal / ordinal / English language
text?”) should also not need to be provided to estimator constructors,
ideally, but should be available as metadata alongside X.#8480
Passing around information that is not (X, y): Target informationWe have problems getting the full set of classes to all components when
the data is split/sampled.#6231#8100We have no way to handle a mixture of categorical and continuous targets.
Passing around information that is not (X, y): Target information
We have problems getting the full set of classes to all components when
the data is split/sampled.#6231#8100
We have problems getting the full set of classes to all components when
the data is split/sampled.#6231#8100
We have no way to handle a mixture of categorical and continuous targets.
We have no way to handle a mixture of categorical and continuous targets.
Make it easier for external users to write Scikit-learn-compatible
componentsMore self-sufficient running of scikit-learn-contrib or a similar resource
Make it easier for external users to write Scikit-learn-compatible
components
More self-sufficient running of scikit-learn-contrib or a similar resource
More self-sufficient running of scikit-learn-contrib or a similar resource
Support resampling and sample reductionAllow subsampling of majority classes (in a pipeline?)#3855
Support resampling and sample reduction
Allow subsampling of majority classes (in a pipeline?)#3855
Allow subsampling of majority classes (in a pipeline?)#3855
Better interfaces for interactive developmentImprove the HTML visualisations of estimators via theestimator_html_repr.Include more plotting tools, not just as examples.
Better interfaces for interactive development
Improve the HTML visualisations of estimators via theestimator_html_repr.
Improve the HTML visualisations of estimators via theestimator_html_repr.
estimator_html_repr
Include more plotting tools, not just as examples.
Include more plotting tools, not just as examples.
Improved tools for model diagnostics and basic inferencework on a unified interface for “feature importance”better ways to handle validation sets when fitting
Improved tools for model diagnostics and basic inference
work on a unified interface for “feature importance”
work on a unified interface for “feature importance”
better ways to handle validation sets when fitting
better ways to handle validation sets when fitting
Better tools for selecting hyperparameters with transductive estimatorsGrid search and cross validation are not applicable to most clustering
tasks. Stability-based selection is more relevant.
Better tools for selecting hyperparameters with transductive estimators
Grid search and cross validation are not applicable to most clustering
tasks. Stability-based selection is more relevant.
Grid search and cross validation are not applicable to most clustering
tasks. Stability-based selection is more relevant.
Better support for manual and automatic pipeline buildingEasier way to construct complex pipelines and valid search spaces#7608#5082#8243provide search ranges for common estimators??cf.searchgrid
Better support for manual and automatic pipeline building
Easier way to construct complex pipelines and valid search spaces#7608#5082#8243
Easier way to construct complex pipelines and valid search spaces#7608#5082#8243
provide search ranges for common estimators??
provide search ranges for common estimators??
cf.searchgrid
cf.searchgrid
Improved tracking of fittingVerbose is not very friendly and should use a standard logging library#6929,#78Callbacks or a similar system would facilitate logging and early stopping
Improved tracking of fitting
Verbose is not very friendly and should use a standard logging library#6929,#78
Verbose is not very friendly and should use a standard logging library#6929,#78
Callbacks or a similar system would facilitate logging and early stopping
Callbacks or a similar system would facilitate logging and early stopping
Distributed parallelismAccept data which complies with__array_function__
Distributed parallelism
Accept data which complies with__array_function__
Accept data which complies with__array_function__
__array_function__
A way forward for more out of coreDask enables easy out-of-core computation. While the Dask model probably
cannot be adaptable to all machine-learning algorithms, most machine
learning is on smaller data than ETL, hence we can maybe adapt to very
large scale while supporting only a fraction of the patterns.
A way forward for more out of core
Dask enables easy out-of-core computation. While the Dask model probably
cannot be adaptable to all machine-learning algorithms, most machine
learning is on smaller data than ETL, hence we can maybe adapt to very
large scale while supporting only a fraction of the patterns.
Dask enables easy out-of-core computation. While the Dask model probably
cannot be adaptable to all machine-learning algorithms, most machine
learning is on smaller data than ETL, hence we can maybe adapt to very
large scale while supporting only a fraction of the patterns.
Backwards-compatible de/serialization of some estimatorsCurrently serialization (with pickle) breaks across versions. While we may
not be able to get around other limitations of pickle re security etc, it
would be great to offer cross-version safety from version 1.0. Note: Gael
and Olivier think that this can cause heavy maintenance burden and we
should manage the trade-offs. A possible alternative is presented in the
following point.
Backwards-compatible de/serialization of some estimators
Currently serialization (with pickle) breaks across versions. While we may
not be able to get around other limitations of pickle re security etc, it
would be great to offer cross-version safety from version 1.0. Note: Gael
and Olivier think that this can cause heavy maintenance burden and we
should manage the trade-offs. A possible alternative is presented in the
following point.
Currently serialization (with pickle) breaks across versions. While we may
not be able to get around other limitations of pickle re security etc, it
would be great to offer cross-version safety from version 1.0. Note: Gael
and Olivier think that this can cause heavy maintenance burden and we
should manage the trade-offs. A possible alternative is presented in the
following point.
Documentation and tooling for model lifecycle managementDocument good practices for model deployments and lifecycle: before
deploying a model: snapshot the code versions (numpy, scipy, scikit-learn,
custom code repo), the training script and an alias on how to retrieve
historical training data + snapshot a copy of a small validation set +
snapshot of the predictions (predicted probabilities for classifiers)
on that validation set.Document and tools to make it easy to manage upgrade of scikit-learn
versions:Try to load the old pickle, if it works, use the validation set
prediction snapshot to detect that the serialized model still behave
the same;If joblib.load / pickle.load not work, use the versioned control
training script + historical training set to retrain the model and use
the validation set prediction snapshot to assert that it is possible to
recover the previous predictive performance: if this is not the case
there is probably a bug in scikit-learn that needs to be reported.
Documentation and tooling for model lifecycle management
Document good practices for model deployments and lifecycle: before
deploying a model: snapshot the code versions (numpy, scipy, scikit-learn,
custom code repo), the training script and an alias on how to retrieve
historical training data + snapshot a copy of a small validation set +
snapshot of the predictions (predicted probabilities for classifiers)
on that validation set.
Document good practices for model deployments and lifecycle: before
deploying a model: snapshot the code versions (numpy, scipy, scikit-learn,
custom code repo), the training script and an alias on how to retrieve
historical training data + snapshot a copy of a small validation set +
snapshot of the predictions (predicted probabilities for classifiers)
on that validation set.
Document and tools to make it easy to manage upgrade of scikit-learn
versions:Try to load the old pickle, if it works, use the validation set
prediction snapshot to detect that the serialized model still behave
the same;If joblib.load / pickle.load not work, use the versioned control
training script + historical training set to retrain the model and use
the validation set prediction snapshot to assert that it is possible to
recover the previous predictive performance: if this is not the case
there is probably a bug in scikit-learn that needs to be reported.
Document and tools to make it easy to manage upgrade of scikit-learn
versions:
Try to load the old pickle, if it works, use the validation set
prediction snapshot to detect that the serialized model still behave
the same;
Try to load the old pickle, if it works, use the validation set
prediction snapshot to detect that the serialized model still behave
the same;
If joblib.load / pickle.load not work, use the versioned control
training script + historical training set to retrain the model and use
the validation set prediction snapshot to assert that it is possible to
recover the previous predictive performance: if this is not the case
there is probably a bug in scikit-learn that needs to be reported.
If joblib.load / pickle.load not work, use the versioned control
training script + historical training set to retrain the model and use
the validation set prediction snapshot to assert that it is possible to
recover the previous predictive performance: if this is not the case
there is probably a bug in scikit-learn that needs to be reported.
Everything in scikit-learn should probably conform to our API contract.
We are still in the process of making decisions on some of these related
issues.Pipeline<pipeline.Pipeline>andFeatureUnionmodify their input
parameters in fit. Fixing this requires making sure we have a good
grasp of their use cases to make sure all current functionality is
maintained.#8157#7382
Everything in scikit-learn should probably conform to our API contract.
We are still in the process of making decisions on some of these related
issues.
Pipeline<pipeline.Pipeline>andFeatureUnionmodify their input
parameters in fit. Fixing this requires making sure we have a good
grasp of their use cases to make sure all current functionality is
maintained.#8157#7382
Pipeline<pipeline.Pipeline>andFeatureUnionmodify their input
parameters in fit. Fixing this requires making sure we have a good
grasp of their use cases to make sure all current functionality is
maintained.#8157#7382
Pipeline<pipeline.Pipeline>
FeatureUnion
(Optional) Improve scikit-learn common tests suite to make sure that (at
least for frequently used) models have stable predictions across-versions
(to be discussed);Extend documentation to mention how to deploy models in Python-free
environments for instanceONNX.
and use the above best practices to assess predictive consistency between
scikit-learn and ONNX prediction functions on validation set.Document good practices to detect temporal distribution drift for deployed
model and good practices for re-training on fresh data without causing
catastrophic predictive performance regressions.
(Optional) Improve scikit-learn common tests suite to make sure that (at
least for frequently used) models have stable predictions across-versions
(to be discussed);
Extend documentation to mention how to deploy models in Python-free
environments for instanceONNX.
and use the above best practices to assess predictive consistency between
scikit-learn and ONNX prediction functions on validation set.
Extend documentation to mention how to deploy models in Python-free
environments for instanceONNX.
and use the above best practices to assess predictive consistency between
scikit-learn and ONNX prediction functions on validation set.
Document good practices to detect temporal distribution drift for deployed
model and good practices for re-training on fresh data without causing
catastrophic predictive performance regressions.
Document good practices to detect temporal distribution drift for deployed
model and good practices for re-training on fresh data without causing
catastrophic predictive performance regressions.
This Page
Show Source

GitHub
GitHub
Scikit-learn governance and decision-making#
The purpose of this document is to formalize the governance process used by the
scikit-learn project, to clarify how decisions are made and how the various
elements of our community interact.
This document establishes a decision-making structure that takes into account
feedback from all members of the community and strives to find consensus, while
avoiding any deadlocks.
This is a meritocratic, consensus-based community project. Anyone with an
interest in the project can join the community, contribute to the project
design and participate in the decision making process. This document describes
how that participation takes place and how to set about earning merit within
the project community.
Roles And Responsibilities#
We distinguish between contributors, core contributors, and the technical
committee. A key distinction between them is their voting rights: contributors
have no voting rights, whereas the other two groups all have voting rights,
as well as permissions to the tools relevant to their roles.
Contributors#
Contributors are community members who contribute in concrete ways to the
project. Anyone can become a contributor, and contributions can take many forms
– not only code – as detailed in thecontributors guide.
There is no process to become a contributor: once somebody contributes to the
project in any way, they are a contributor.
Core Contributors#
All core contributor members have the same voting rights and right to propose
new members to any of the roles listed below. Their membership is represented
as being an organization member on the scikit-learnGitHub organization.
They are also welcome to join ourmonthly core contributor meetings.
New members can be nominated by any existing member. Once they have been
nominated, there will be a vote by the current core contributors. Voting on new
members is one of the few activities that takes place on the project’s private
mailing list. While it is expected that most votes will be unanimous, a
two-thirds majority of the cast votes is enough. The vote needs to be open for
at least 1 week.
Core contributors that have not contributed to the project, corresponding to
their role, in the past 12 months will be asked if they want to become emeritus
members and recant their rights until they become active again. The list of
members, active and emeritus (with dates at which they became active) is public
on the scikit-learn website. It is the responsibility of the active core
contributors to send such a yearly reminder email.
The following teams form the core contributors group:
Contributor Experience TeamThe contributor experience team improves the experience of contributors by
helping with the triage of issues and pull requests, as well as noticing any
repeating patterns where people might struggle, and to help with improving
those aspects of the project.To this end, they have the required permissions on GitHub to label and close
issues.Their workis crucial to improve the
communication in the project and limit the crowding of the issue tracker.
Contributor Experience TeamThe contributor experience team improves the experience of contributors by
helping with the triage of issues and pull requests, as well as noticing any
repeating patterns where people might struggle, and to help with improving
those aspects of the project.
To this end, they have the required permissions on GitHub to label and close
issues.Their workis crucial to improve the
communication in the project and limit the crowding of the issue tracker.
Communication TeamMembers of the communication team help with outreach and communication
for scikit-learn. The goal of the team is to develop public awareness of
scikit-learn, of its features and usage, as well as branding.For this, they can operate the scikit-learn accounts on various social networks
and produce materials. They also have the required rights to our blog
repository and other relevant accounts and platforms.
Communication TeamMembers of the communication team help with outreach and communication
for scikit-learn. The goal of the team is to develop public awareness of
scikit-learn, of its features and usage, as well as branding.
For this, they can operate the scikit-learn accounts on various social networks
and produce materials. They also have the required rights to our blog
repository and other relevant accounts and platforms.
Documentation TeamMembers of the documentation team engage with the documentation of the project
among other things. They might also be involved in other aspects of the
project, but their reviews on documentation contributions are considered
authoritative, and can merge such contributions.To this end, they have permissions to merge pull requests in scikit-learn’s
repository.
Documentation TeamMembers of the documentation team engage with the documentation of the project
among other things. They might also be involved in other aspects of the
project, but their reviews on documentation contributions are considered
authoritative, and can merge such contributions.
To this end, they have permissions to merge pull requests in scikit-learn’s
repository.
Maintainers TeamMaintainers are community members who have shown that they are dedicated to the
continued development of the project through ongoing engagement with the
community. They have shown they can be trusted to maintain scikit-learn with
care. Being a maintainer allows contributors to more easily carry on with their
project related activities by giving them direct access to the project’s
repository. Maintainers are expected to review code contributions, merge
approved pull requests, cast votes for and against merging a pull-request,
and to be involved in deciding major changes to the API.
Maintainers TeamMaintainers are community members who have shown that they are dedicated to the
continued development of the project through ongoing engagement with the
community. They have shown they can be trusted to maintain scikit-learn with
care. Being a maintainer allows contributors to more easily carry on with their
project related activities by giving them direct access to the project’s
repository. Maintainers are expected to review code contributions, merge
approved pull requests, cast votes for and against merging a pull-request,
and to be involved in deciding major changes to the API.
Technical Committee#
The Technical Committee (TC) members are maintainers who have additional
responsibilities to ensure the smooth running of the project. TC members are
expected to participate in strategic planning, and approve changes to the
governance model. The purpose of the TC is to ensure a smooth progress from the
big-picture perspective. Indeed changes that impact the full project require a
synthetic analysis and a consensus that is both explicit and informed. In cases
that the core contributor community (which includes the TC members) fails to
reach such a consensus in the required time frame, the TC is the entity to
resolve the issue. Membership of the TC is by nomination by a core contributor.
A nomination will result in discussion which cannot take more than a month and
then a vote by the core contributors which will stay open for a week. TC
membership votes are subject to a two-third majority of all cast votes as well
as a simple majority approval of all the current TC members. TC members who do
not actively engage with the TC duties are expected to resign.
The Technical Committee of scikit-learn consists ofThomas Fan,Alexandre Gramfort,Olivier Grisel,Adrin Jalali,Andreas Müller,Joel NothmanandGaël Varoquaux.
Decision Making Process#
Decisions about the future of the project are made through discussion with all
members of the community. All non-sensitive project management discussion takes
place on the project contributors’mailing listand theissue tracker.
Occasionally, sensitive discussion occurs on a private list.
Scikit-learn uses a “consensus seeking” process for making decisions. The group
tries to find a resolution that has no open objections among core contributors.
At any point during the discussion, any core contributor can call for a vote,
which will conclude one month from the call for the vote. Most votes have to be
backed by aSLEP. If no option can gather two thirds of the votes
cast, the decision is escalated to the TC, which in turn will use consensus
seeking with the fallback option of a simple majority vote if no consensus can
be found within a month. This is what we hereafter may refer to as “the
decision making process”.
Decisions (in addition to adding core contributors and TC membership as above)
are made according to the following rules:
Minor Documentation changes, such as typo fixes, or addition / correction
of a sentence, but no change of thescikit-learn.orglanding page or the
“about” page: Requires +1 by a maintainer, no -1 by a maintainer (lazy
consensus), happens on the issue or pull request page. Maintainers are
expected to give “reasonable time” to others to give their opinion on the
pull request if they’re not confident others would agree.
Minor Documentation changes, such as typo fixes, or addition / correction
of a sentence, but no change of thescikit-learn.orglanding page or the
“about” page: Requires +1 by a maintainer, no -1 by a maintainer (lazy
consensus), happens on the issue or pull request page. Maintainers are
expected to give “reasonable time” to others to give their opinion on the
pull request if they’re not confident others would agree.
scikit-learn.org
Code changes and major documentation changesrequire +1 by two maintainers, no -1 by a maintainer (lazy
consensus), happens on the issue of pull-request page.
Code changes and major documentation changesrequire +1 by two maintainers, no -1 by a maintainer (lazy
consensus), happens on the issue of pull-request page.
Changes to the API principles and changes to dependencies or supported
versionshappen viaEnhancement proposals (SLEPs)and follows the decision-making process
outlined above.
Changes to the API principles and changes to dependencies or supported
versionshappen viaEnhancement proposals (SLEPs)and follows the decision-making process
outlined above.
Changes to the governance modelfollow the process outlined inSLEP020.
Changes to the governance modelfollow the process outlined inSLEP020.
If a veto -1 vote is cast on a lazy consensus, the proposer can appeal to the
community and maintainers and the change can be approved or rejected using
the decision making procedure outlined above.
Governance Model Changes#
Governance model changes occur through an enhancement proposal or a GitHub Pull
Request. An enhancement proposal will go through “the decision-making process”
described in the previous section. Alternatively, an author may propose a change
directly to the governance model with a GitHub Pull Request. Logistically, an
author can open a Draft Pull Request for feedback and follow up with a new
revised Pull Request for voting. Once that author is happy with the state of the
Pull Request, they can call for a vote on the public mailing list. During the
one-month voting period, the Pull Request can not change. A Pull Request
Approval will count as a positive vote, and a “Request Changes” review will
count as a negative vote. If two-thirds of the cast votes are positive, then
the governance model change is accepted.
Enhancement proposals (SLEPs)#
For all votes, a proposal must have been made public and discussed before the
vote. Such proposal must be a consolidated document, in the form of a
“Scikit-Learn Enhancement Proposal” (SLEP), rather than a long discussion on an
issue. A SLEP must be submitted as a pull-request toenhancement proposalsusing theSLEP
template.SLEP000describes the process in more detail.
This Page
Show Source

GitHub
GitHub
About us#
History#
This project was started in 2007 as a Google Summer of Code project by
David Cournapeau. Later that year, Matthieu Brucher started working on this project
as part of his thesis.
In 2010 Fabian Pedregosa, Gael Varoquaux, Alexandre Gramfort and Vincent
Michel of INRIA took leadership of the project and made the first public
release, February the 1st 2010. Since then, several releases have appeared
following an approximately 3-month cycle, and a thriving international
community has been leading the development. As a result, INRIA holds the
copyright over the work done by people who were employed by INRIA at the
time of the contribution.
Governance#
The decision making process and governance structure of scikit-learn, like roles and responsibilities, is laid out in thegovernance document.
The people behind scikit-learn#
Scikit-learn is a community project, developed by a large group of
people, all across the world. A few core contributor teams, listed below, have
central roles, however a more complete list of contributors can be foundon
github.
Active Core Contributors#
The following people are currently maintainers, in charge of
consolidating scikit-learn’s development and maintenance:
Jérémie du Boisberranger
Loïc Estève
Thomas J. Fan
Alexandre Gramfort
Olivier Grisel
Tim Head
Nicolas Hug
Adrin Jalali
Julien Jerphanion
Guillaume Lemaitre
Adam Li
Lucy Liu
Christian Lorentzen
Andreas Mueller
Joel Nothman
Omar Salman
Gael Varoquaux
Yao Xiao
Meekail Zain
Note
Please do not email the authors directly to ask for assistance or report issues.
Instead, please seeWhat’s the best way to ask questions about scikit-learnin the FAQ.
See also
How you cancontribute to the project.
The following people help with documenting the project:
Arturo Amor
Lucy Liu
Maren Westermann
Yao Xiao
The following people are active contributors who also help withtriaging issues, PRs, and general
maintenance:
Virgil Chan
Juan Carlos Alfaro Jiménez
Lucy Liu
Maxwell Liu
Juan Martin Loyola
Sylvain Marié
Norbert Preining
Stefanie Senger
Reshama Shaikh
Albert Thomas
Maren Westermann
The following people help withcommunication around scikit-learn.
Lauren Burke-McCarthy
François Goupil
Emeritus Core Contributors#
The following people have been active contributors in the past, but are no
longer active in the project:
Mathieu Blondel
Mathieu Blondel
Joris Van den Bossche
Joris Van den Bossche
Matthieu Brucher
Matthieu Brucher
Lars Buitinck
Lars Buitinck
David Cournapeau
David Cournapeau
Noel Dawe
Noel Dawe
Vincent Dubourg
Vincent Dubourg
Edouard Duchesnay
Edouard Duchesnay
Alexander Fabisch
Alexander Fabisch
Virgile Fritsch
Virgile Fritsch
Satrajit Ghosh
Satrajit Ghosh
Angel Soler Gollonet
Angel Soler Gollonet
Chris Gorgolewski
Chris Gorgolewski
Jaques Grobler
Jaques Grobler
Yaroslav Halchenko
Yaroslav Halchenko
Brian Holt
Brian Holt
Arnaud Joly
Arnaud Joly
Thouis (Ray) Jones
Thouis (Ray) Jones
Kyle Kastner
Kyle Kastner
manoj kumar
manoj kumar
Robert Layton
Robert Layton
Wei Li
Wei Li
Paolo Losi
Paolo Losi
Gilles Louppe
Gilles Louppe
Jan Hendrik Metzen
Jan Hendrik Metzen
Vincent Michel
Vincent Michel
Jarrod Millman
Jarrod Millman
Vlad Niculae
Vlad Niculae
Alexandre Passos
Alexandre Passos
Fabian Pedregosa
Fabian Pedregosa
Peter Prettenhofer
Peter Prettenhofer
Hanmin Qin
Hanmin Qin
(Venkat) Raghav, Rajagopalan
(Venkat) Raghav, Rajagopalan
Jacob Schreiber
Jacob Schreiber
杜世橋 Du Shiqiao
杜世橋 Du Shiqiao
Bertrand Thirion
Bertrand Thirion
Tom Dupré la Tour
Tom Dupré la Tour
Jake Vanderplas
Jake Vanderplas
Nelle Varoquaux
Nelle Varoquaux
David Warde-Farley
David Warde-Farley
Ron Weiss
Ron Weiss
Roman Yurchak
Roman Yurchak
The following people have been active in the communication team in the
past, but no longer have communication responsibilities:
Reshama Shaikh
Reshama Shaikh
The following people have been active in the contributor experience team in the
past:
Chiara Marmo
Chiara Marmo
Citing scikit-learn#
If you use scikit-learn in a scientific publication, we would appreciate
citations to the following paper:
Scikit-learn: Machine Learning in Python, Pedregosaet al., JMLR 12, pp. 2825-2830, 2011.
Bibtex entry:
@article{scikit-learn,title={Scikit-learn:MachineLearningin{P}ython},author={Pedregosa,F.andVaroquaux,G.andGramfort,A.andMichel,V.andThirion,B.andGrisel,O.andBlondel,M.andPrettenhofer,P.andWeiss,R.andDubourg,V.andVanderplas,J.andPassos,A.andCournapeau,D.andBrucher,M.andPerrot,M.andDuchesnay,E.},journal={JournalofMachineLearningResearch},volume={12},pages={2825--2830},year={2011}}
If you want to cite scikit-learn for its API or design, you may also want to consider the
following paper:
API design for machine learning software: experiences from the scikit-learn
project, Buitincket al., 2013.
Bibtex entry:
@inproceedings{sklearn_api,author={LarsBuitinckandGillesLouppeandMathieuBlondelandFabianPedregosaandAndreasMuellerandOlivierGriselandVladNiculaeandPeterPrettenhoferandAlexandreGramfortandJaquesGroblerandRobertLaytonandJakeVanderPlasandArnaudJolyandBrianHoltandGa{\"{e}}l Varoquaux},title={{API}designformachinelearningsoftware:experiencesfromthescikit-learnproject},booktitle={ECMLPKDDWorkshop:LanguagesforDataMiningandMachineLearning},year={2013},pages={108--122},}
Artwork#
High quality PNG and SVG logos are available in thedoc/logos/source directory.
Funding#
Scikit-learn is a community driven project, however institutional and private
grants help to assure its sustainability.
The project would like to thank the following funders.
:probabl.employs Adrin Jalali, Arturo Amor,
François Goupil, Guillaume Lemaitre, Jérémie du Boisberranger, Loïc Estève,
Olivier Grisel, and Stefanie Senger.
TheMembersof
theScikit-learn Consortium at Inria Foundationhelp at maintaining and
improving the project through their financial support.
NVidiafunds Tim Head since 2022
and is part of the scikit-learn consortium at Inria.
Microsoftfunds Andreas Müller since 2020.
Quansight Labsfunds Lucy Liu since 2022.
The Chan-Zuckerberg InitiativeandWellcome Trustfund scikit-learn through theEssential Open Source Software for Science (EOSS)cycle 6.
It supports Lucy Liu and diversity & inclusion initiatives that will
be announced in the future.
Tideliftsupports the project via their service
agreement.
Past Sponsors#
Quansight Labsfunded Meekail Zain in 2022 and 2023,
and funded Thomas J. Fan from 2021 to 2023.
Columbia Universityfunded Andreas Müller
(2016-2020).
The University of Sydneyfunded Joel Nothman
(2017-2021).
Andreas Müller received a grant to improve scikit-learn from theAlfred P. Sloan Foundation.
This grant supported the position of Nicolas Hug and Thomas J. Fan.
INRIAactively supports this project. It has
provided funding for Fabian Pedregosa (2010-2012), Jaques Grobler
(2012-2013) and Olivier Grisel (2013-2017) to work on this project
full-time. It also hosts coding sprints and other events.
Paris-Saclay Center for Data Sciencefunded one year for a developer to work on the project full-time (2014-2015), 50%
of the time of Guillaume Lemaitre (2016-2017) and 50% of the time of Joris van den
Bossche (2017-2018).
NYU Moore-Sloan Data Science Environmentfunded Andreas Mueller (2014-2016) to work on this project. The Moore-Sloan
Data Science Environment also funds several students to work on the project
part-time.
Télécom Paristechfunded Manoj Kumar
(2014), Tom Dupré la Tour (2015), Raghav RV (2015-2017), Thierry Guillemot
(2016-2017) and Albert Thomas (2017) to work on scikit-learn.
The Labex DigiCosmefunded Nicolas Goix
(2015-2016), Tom Dupré la Tour (2015-2016 and 2017-2018), Mathurin Massias
(2018-2019) to work part time on scikit-learn during their PhDs. It also
funded a scikit-learn coding sprint in 2015.
The Chan-Zuckerberg Initiativefunded Nicolas
Hug to work full-time on scikit-learn in 2020.
The following students were sponsored byGoogleto work on scikit-learn through
theGoogle Summer of Codeprogram.
2007 - David Cournapeau
2007 - David Cournapeau
2011 -Vlad Niculae
2011 -Vlad Niculae
2012 -Vlad Niculae, Immanuel Bayer
2012 -Vlad Niculae, Immanuel Bayer
2013 - Kemal Eren, Nicolas Trésegnie
2013 - Kemal Eren, Nicolas Trésegnie
2014 - Hamzeh Alsalhi, Issam Laradji, Maheshakya Wijewardena, Manoj Kumar
2014 - Hamzeh Alsalhi, Issam Laradji, Maheshakya Wijewardena, Manoj Kumar
2015 -Raghav RV, Wei Xue
2015 -Raghav RV, Wei Xue
2016 -Nelson Liu,YenChen Lin
2016 -Nelson Liu,YenChen Lin
TheNeuroDebianproject providingDebianpackaging and contributions is supported byDr. James V. Haxby(Dartmouth
College).
The following organizations funded the scikit-learn consortium at Inria in
the past:
Coding Sprints#
The scikit-learn project has a long history ofopen source coding sprintswith over 50 sprint
events from 2010 to present day. There are scores of sponsors who contributed
to costs which include venue, food, travel, developer time and more. Seescikit-learn sprintsfor a full
list of events.
Donating to the project#
If you are interested in donating to the project or to one of our code-sprints,
please donate via theNumFOCUS Donations Page.
Help us,donate!
All donations will be handled byNumFOCUS, a non-profit
organization which is managed by a board ofScipy community members. NumFOCUS’s mission is to foster scientific
computing software, in particular in Python. As a fiscal home of scikit-learn, it
ensures that money is available when needed to keep the project funded and available
while in compliance with tax regulations.
The received donations for the scikit-learn project mostly will go towards covering
travel-expenses for code sprints, as well as towards the organization budget of the
project[1].
Notes
Regarding the organization budget, in particular, we might use some of
the donated funds to pay for other project expenses such as DNS,
hosting or continuous integration services.
Infrastructure support#
We would also like to thankMicrosoft Azure,Cirrus Cl,CircleClfor free CPU
time on their Continuous Integration servers, andAnaconda Inc.for the storage they provide for our staging and nightly builds.
This Page
Show Source

GitHub
GitHub
1.Supervised learning#
1.1. Linear Models1.1.1. Ordinary Least Squares1.1.2. Ridge regression and classification1.1.3. Lasso1.1.4. Multi-task Lasso1.1.5. Elastic-Net1.1.6. Multi-task Elastic-Net1.1.7. Least Angle Regression1.1.8. LARS Lasso1.1.9. Orthogonal Matching Pursuit (OMP)1.1.10. Bayesian Regression1.1.11. Logistic regression1.1.12. Generalized Linear Models1.1.13. Stochastic Gradient Descent - SGD1.1.14. Perceptron1.1.15. Passive Aggressive Algorithms1.1.16. Robustness regression: outliers and modeling errors1.1.17. Quantile Regression1.1.18. Polynomial regression: extending linear models with basis functions
1.1.1. Ordinary Least Squares
1.1.2. Ridge regression and classification
1.1.3. Lasso
1.1.4. Multi-task Lasso
1.1.5. Elastic-Net
1.1.6. Multi-task Elastic-Net
1.1.7. Least Angle Regression
1.1.8. LARS Lasso
1.1.9. Orthogonal Matching Pursuit (OMP)
1.1.10. Bayesian Regression
1.1.11. Logistic regression
1.1.12. Generalized Linear Models
1.1.13. Stochastic Gradient Descent - SGD
1.1.14. Perceptron
1.1.15. Passive Aggressive Algorithms
1.1.16. Robustness regression: outliers and modeling errors
1.1.17. Quantile Regression
1.1.18. Polynomial regression: extending linear models with basis functions
1.2. Linear and Quadratic Discriminant Analysis1.2.1. Dimensionality reduction using Linear Discriminant Analysis1.2.2. Mathematical formulation of the LDA and QDA classifiers1.2.3. Mathematical formulation of LDA dimensionality reduction1.2.4. Shrinkage and Covariance Estimator1.2.5. Estimation algorithms
1.2.1. Dimensionality reduction using Linear Discriminant Analysis
1.2.2. Mathematical formulation of the LDA and QDA classifiers
1.2.3. Mathematical formulation of LDA dimensionality reduction
1.2.4. Shrinkage and Covariance Estimator
1.2.5. Estimation algorithms
1.3. Kernel ridge regression
1.4. Support Vector Machines1.4.1. Classification1.4.2. Regression1.4.3. Density estimation, novelty detection1.4.4. Complexity1.4.5. Tips on Practical Use1.4.6. Kernel functions1.4.7. Mathematical formulation1.4.8. Implementation details
1.4.1. Classification
1.4.2. Regression
1.4.3. Density estimation, novelty detection
1.4.4. Complexity
1.4.5. Tips on Practical Use
1.4.6. Kernel functions
1.4.7. Mathematical formulation
1.4.8. Implementation details
1.5. Stochastic Gradient Descent1.5.1. Classification1.5.2. Regression1.5.3. Online One-Class SVM1.5.4. Stochastic Gradient Descent for sparse data1.5.5. Complexity1.5.6. Stopping criterion1.5.7. Tips on Practical Use1.5.8. Mathematical formulation1.5.9. Implementation details
1.5.1. Classification
1.5.2. Regression
1.5.3. Online One-Class SVM
1.5.4. Stochastic Gradient Descent for sparse data
1.5.5. Complexity
1.5.6. Stopping criterion
1.5.7. Tips on Practical Use
1.5.8. Mathematical formulation
1.5.9. Implementation details
1.6. Nearest Neighbors1.6.1. Unsupervised Nearest Neighbors1.6.2. Nearest Neighbors Classification1.6.3. Nearest Neighbors Regression1.6.4. Nearest Neighbor Algorithms1.6.5. Nearest Centroid Classifier1.6.6. Nearest Neighbors Transformer1.6.7. Neighborhood Components Analysis
1.6.1. Unsupervised Nearest Neighbors
1.6.2. Nearest Neighbors Classification
1.6.3. Nearest Neighbors Regression
1.6.4. Nearest Neighbor Algorithms
1.6.5. Nearest Centroid Classifier
1.6.6. Nearest Neighbors Transformer
1.6.7. Neighborhood Components Analysis
1.7. Gaussian Processes1.7.1. Gaussian Process Regression (GPR)1.7.2. Gaussian Process Classification (GPC)1.7.3. GPC examples1.7.4. Kernels for Gaussian Processes
1.7.1. Gaussian Process Regression (GPR)
1.7.2. Gaussian Process Classification (GPC)
1.7.3. GPC examples
1.7.4. Kernels for Gaussian Processes
1.8. Cross decomposition1.8.1. PLSCanonical1.8.2. PLSSVD1.8.3. PLSRegression1.8.4. Canonical Correlation Analysis
1.8.1. PLSCanonical
1.8.2. PLSSVD
1.8.3. PLSRegression
1.8.4. Canonical Correlation Analysis
1.9. Naive Bayes1.9.1. Gaussian Naive Bayes1.9.2. Multinomial Naive Bayes1.9.3. Complement Naive Bayes1.9.4. Bernoulli Naive Bayes1.9.5. Categorical Naive Bayes1.9.6. Out-of-core naive Bayes model fitting
1.9.1. Gaussian Naive Bayes
1.9.2. Multinomial Naive Bayes
1.9.3. Complement Naive Bayes
1.9.4. Bernoulli Naive Bayes
1.9.5. Categorical Naive Bayes
1.9.6. Out-of-core naive Bayes model fitting
1.10. Decision Trees1.10.1. Classification1.10.2. Regression1.10.3. Multi-output problems1.10.4. Complexity1.10.5. Tips on practical use1.10.6. Tree algorithms: ID3, C4.5, C5.0 and CART1.10.7. Mathematical formulation1.10.8. Missing Values Support1.10.9. Minimal Cost-Complexity Pruning
1.10.1. Classification
1.10.2. Regression
1.10.3. Multi-output problems
1.10.4. Complexity
1.10.5. Tips on practical use
1.10.6. Tree algorithms: ID3, C4.5, C5.0 and CART
1.10.7. Mathematical formulation
1.10.8. Missing Values Support
1.10.9. Minimal Cost-Complexity Pruning
1.11. Ensembles: Gradient boosting, random forests, bagging, voting, stacking1.11.1. Gradient-boosted trees1.11.2. Random forests and other randomized tree ensembles1.11.3. Bagging meta-estimator1.11.4. Voting Classifier1.11.5. Voting Regressor1.11.6. Stacked generalization1.11.7. AdaBoost
1.11.1. Gradient-boosted trees
1.11.2. Random forests and other randomized tree ensembles
1.11.3. Bagging meta-estimator
1.11.4. Voting Classifier
1.11.5. Voting Regressor
1.11.6. Stacked generalization
1.11.7. AdaBoost
1.12. Multiclass and multioutput algorithms1.12.1. Multiclass classification1.12.2. Multilabel classification1.12.3. Multiclass-multioutput classification1.12.4. Multioutput regression
1.12.1. Multiclass classification
1.12.2. Multilabel classification
1.12.3. Multiclass-multioutput classification
1.12.4. Multioutput regression
1.13. Feature selection1.13.1. Removing features with low variance1.13.2. Univariate feature selection1.13.3. Recursive feature elimination1.13.4. Feature selection using SelectFromModel1.13.5. Sequential Feature Selection1.13.6. Feature selection as part of a pipeline
1.13.1. Removing features with low variance
1.13.2. Univariate feature selection
1.13.3. Recursive feature elimination
1.13.4. Feature selection using SelectFromModel
1.13.5. Sequential Feature Selection
1.13.6. Feature selection as part of a pipeline
1.14. Semi-supervised learning1.14.1. Self Training1.14.2. Label Propagation
1.14.1. Self Training
1.14.2. Label Propagation
1.15. Isotonic regression
1.16. Probability calibration1.16.1. Calibration curves1.16.2. Calibrating a classifier1.16.3. Usage
1.16.1. Calibration curves
1.16.2. Calibrating a classifier
1.16.3. Usage
1.17. Neural network models (supervised)1.17.1. Multi-layer Perceptron1.17.2. Classification1.17.3. Regression1.17.4. Regularization1.17.5. Algorithms1.17.6. Complexity1.17.7. Tips on Practical Use1.17.8. More control with warm_start
1.17.1. Multi-layer Perceptron
1.17.2. Classification
1.17.3. Regression
1.17.4. Regularization
1.17.5. Algorithms
1.17.6. Complexity
1.17.7. Tips on Practical Use
1.17.8. More control with warm_start
This Page
Show Source

GitHub
GitHub
1.1.Linear Models#
The following are a set of methods intended for regression in which
the target value is expected to be a linear combination of the features.
In mathematical notation, if\(\hat{y}\)is the predicted
value.
Across the module, we designate the vector\(w = (w_1,
..., w_p)\)ascoef_and\(w_0\)asintercept_.
coef_
intercept_
To perform classification with generalized linear models, seeLogistic regression.
1.1.1.Ordinary Least Squares#
LinearRegressionfits a linear model with coefficients\(w = (w_1, ..., w_p)\)to minimize the residual sum
of squares between the observed targets in the dataset, and the
targets predicted by the linear approximation. Mathematically it
solves a problem of the form:
LinearRegression
LinearRegressionwill take in itsfitmethod arraysX,yand will store the coefficients\(w\)of the linear model in itscoef_member:
LinearRegression
fit
X
y
coef_
>>>fromsklearnimportlinear_model>>>reg=linear_model.LinearRegression()>>>reg.fit([[0,0],[1,1],[2,2]],[0,1,2])LinearRegression()>>>reg.coef_array([0.5, 0.5])
The coefficient estimates for Ordinary Least Squares rely on the
independence of the features. When features are correlated and the
columns of the design matrix\(X\)have an approximately linear
dependence, the design matrix becomes close to singular
and as a result, the least-squares estimate becomes highly sensitive
to random errors in the observed target, producing a large
variance. This situation ofmulticollinearitycan arise, for
example, when data are collected without an experimental design.
Examples
Ordinary Least Squares Example
Ordinary Least Squares Example
1.1.1.1.Non-Negative Least Squares#
It is possible to constrain all the coefficients to be non-negative, which may
be useful when they represent some physical or naturally non-negative
quantities (e.g., frequency counts or prices of goods).LinearRegressionaccepts a booleanpositiveparameter: when set toTrueNon-Negative Least Squaresare then applied.
LinearRegression
positive
True
Examples
Non-negative least squares
Non-negative least squares
1.1.1.2.Ordinary Least Squares Complexity#
The least squares solution is computed using the singular value
decomposition of X. If X is a matrix of shape(n_samples,n_features)this method has a cost of\(O(n_{\text{samples}} n_{\text{features}}^2)\), assuming that\(n_{\text{samples}} \geq n_{\text{features}}\).
(n_samples,n_features)
1.1.2.Ridge regression and classification#
1.1.2.1.Regression#
Ridgeregression addresses some of the problems ofOrdinary Least Squaresby imposing a penalty on the size of the
coefficients. The ridge coefficients minimize a penalized residual sum
of squares:
Ridge
The complexity parameter\(\alpha \geq 0\)controls the amount
of shrinkage: the larger the value of\(\alpha\), the greater the amount
of shrinkage and thus the coefficients become more robust to collinearity.
As with other linear models,Ridgewill take in itsfitmethod
arraysX,yand will store the coefficients\(w\)of the linear model in
itscoef_member:
Ridge
fit
X
y
coef_
>>>fromsklearnimportlinear_model>>>reg=linear_model.Ridge(alpha=.5)>>>reg.fit([[0,0],[0,0],[1,1]],[0,.1,1])Ridge(alpha=0.5)>>>reg.coef_array([0.34545455, 0.34545455])>>>reg.intercept_0.13636...
Note that the classRidgeallows for the user to specify that the
solver be automatically chosen by settingsolver="auto". When this option
is specified,Ridgewill choose between the"lbfgs","cholesky",
and"sparse_cg"solvers.Ridgewill begin checking the conditions
shown in the following table from top to bottom. If the condition is true,
the corresponding solver is chosen.
Ridge
solver="auto"
Ridge
"lbfgs"
"cholesky"
"sparse_cg"
Ridge
Solver
Condition
‘lbfgs’
Thepositive=Trueoption is specified.
positive=True
‘cholesky’
The input array X is not sparse.
‘sparse_cg’
None of the above conditions are fulfilled.
1.1.2.2.Classification#
TheRidgeregressor has a classifier variant:RidgeClassifier. This classifier first converts binary targets to{-1,1}and then treats the problem as a regression task, optimizing the
same objective as above. The predicted class corresponds to the sign of the
regressor’s prediction. For multiclass classification, the problem is
treated as multi-output regression, and the predicted class corresponds to
the output with the highest value.
Ridge
RidgeClassifier
{-1,1}
It might seem questionable to use a (penalized) Least Squares loss to fit a
classification model instead of the more traditional logistic or hinge
losses. However, in practice, all those models can lead to similar
cross-validation scores in terms of accuracy or precision/recall, while the
penalized least squares loss used by theRidgeClassifierallows for
a very different choice of the numerical solvers with distinct computational
performance profiles.
RidgeClassifier
TheRidgeClassifiercan be significantly faster than e.g.LogisticRegressionwith a high number of classes because it can
compute the projection matrix\((X^T X)^{-1} X^T\)only once.
RidgeClassifier
LogisticRegression
This classifier is sometimes referred to as aLeast Squares Support Vector
Machineswith
a linear kernel.
Examples
Plot Ridge coefficients as a function of the regularization
Plot Ridge coefficients as a function of the regularization
Classification of text documents using sparse features
Classification of text documents using sparse features
Common pitfalls in the interpretation of coefficients of linear models
Common pitfalls in the interpretation of coefficients of linear models
1.1.2.3.Ridge Complexity#
This method has the same order of complexity asOrdinary Least Squares.
1.1.2.4.Setting the regularization parameter: leave-one-out Cross-Validation#
RidgeCVandRidgeClassifierCVimplement ridge
regression/classification with built-in cross-validation of the alpha parameter.
They work in the same way asGridSearchCVexcept
that it defaults to efficient Leave-One-Outcross-validation.
When using the defaultcross-validation, alpha cannot be 0 due to the
formulation used to calculate Leave-One-Out error. See[RL2007]for details.
RidgeCV
RidgeClassifierCV
GridSearchCV
Usage example:
>>>importnumpyasnp>>>fromsklearnimportlinear_model>>>reg=linear_model.RidgeCV(alphas=np.logspace(-6,6,13))>>>reg.fit([[0,0],[0,0],[1,1]],[0,.1,1])RidgeCV(alphas=array([1.e-06, 1.e-05, 1.e-04, 1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01,1.e+02, 1.e+03, 1.e+04, 1.e+05, 1.e+06]))>>>reg.alpha_0.01
Specifying the value of thecvattribute will trigger the use of
cross-validation withGridSearchCV, for
examplecv=10for 10-fold cross-validation, rather than Leave-One-Out
Cross-Validation.
GridSearchCV
cv=10
“Notes on Regularized Least Squares”, Rifkin & Lippert (technical report,course slides).
1.1.3.Lasso#
TheLassois a linear model that estimates sparse coefficients.
It is useful in some contexts due to its tendency to prefer solutions
with fewer non-zero coefficients, effectively reducing the number of
features upon which the given solution is dependent. For this reason,
Lasso and its variants are fundamental to the field of compressed sensing.
Under certain conditions, it can recover the exact set of non-zero
coefficients (seeCompressive sensing: tomography reconstruction with L1 prior (Lasso)).
Lasso
Mathematically, it consists of a linear model with an added regularization term.
The objective function to minimize is:
The lasso estimate thus solves the minimization of the
least-squares penalty with\(\alpha ||w||_1\)added, where\(\alpha\)is a constant and\(||w||_1\)is the\(\ell_1\)-norm of
the coefficient vector.
The implementation in the classLassouses coordinate descent as
the algorithm to fit the coefficients. SeeLeast Angle Regressionfor another implementation:
Lasso
>>>fromsklearnimportlinear_model>>>reg=linear_model.Lasso(alpha=0.1)>>>reg.fit([[0,0],[1,1]],[0,1])Lasso(alpha=0.1)>>>reg.predict([[1,1]])array([0.8])
The functionlasso_pathis useful for lower-level tasks, as it
computes the coefficients along the full path of possible values.
lasso_path
Examples
L1-based models for Sparse Signals
L1-based models for Sparse Signals
Compressive sensing: tomography reconstruction with L1 prior (Lasso)
Compressive sensing: tomography reconstruction with L1 prior (Lasso)
Common pitfalls in the interpretation of coefficients of linear models
Common pitfalls in the interpretation of coefficients of linear models
Note
Feature selection with Lasso
As the Lasso regression yields sparse models, it can
thus be used to perform feature selection, as detailed inL1-based feature selection.
The following two references explain the iterations
used in the coordinate descent solver of scikit-learn, as well as
the duality gap computation used for convergence control.
“Regularization Path For Generalized linear Models by Coordinate Descent”,
Friedman, Hastie & Tibshirani, J Stat Softw, 2010 (Paper).
“Regularization Path For Generalized linear Models by Coordinate Descent”,
Friedman, Hastie & Tibshirani, J Stat Softw, 2010 (Paper).
“An Interior-Point Method for Large-Scale L1-Regularized Least Squares,”
S. J. Kim, K. Koh, M. Lustig, S. Boyd and D. Gorinevsky,
in IEEE Journal of Selected Topics in Signal Processing, 2007
(Paper)
“An Interior-Point Method for Large-Scale L1-Regularized Least Squares,”
S. J. Kim, K. Koh, M. Lustig, S. Boyd and D. Gorinevsky,
in IEEE Journal of Selected Topics in Signal Processing, 2007
(Paper)
1.1.3.1.Setting regularization parameter#
Thealphaparameter controls the degree of sparsity of the estimated
coefficients.
alpha
scikit-learn exposes objects that set the Lassoalphaparameter by
cross-validation:LassoCVandLassoLarsCV.LassoLarsCVis based on theLeast Angle Regressionalgorithm
explained below.
alpha
LassoCV
LassoLarsCV
LassoLarsCV
For high-dimensional datasets with many collinear features,LassoCVis most often preferable. However,LassoLarsCVhas
the advantage of exploring more relevant values ofalphaparameter, and
if the number of samples is very small compared to the number of
features, it is often faster thanLassoCV.
LassoCV
LassoLarsCV
alpha
LassoCV
Alternatively, the estimatorLassoLarsICproposes to use the
Akaike information criterion (AIC) and the Bayes Information criterion (BIC).
It is a computationally cheaper alternative to find the optimal value of alpha
as the regularization path is computed only once instead of k+1 times
when using k-fold cross-validation.
LassoLarsIC
Indeed, these criteria are computed on the in-sample training set. In short,
they penalize the over-optimistic scores of the different Lasso models by
their flexibility (cf. to “Mathematical details” section below).
However, such criteria need a proper estimation of the degrees of freedom of
the solution, are derived for large samples (asymptotic results) and assume the
correct model is candidates under investigation. They also tend to break when
the problem is badly conditioned (e.g. more features than samples).
Examples
Lasso model selection: AIC-BIC / cross-validation
Lasso model selection: AIC-BIC / cross-validation
Lasso model selection via information criteria
Lasso model selection via information criteria
The definition of AIC (and thus BIC) might differ in the literature. In this
section, we give more information regarding the criterion computed in
scikit-learn.
The AIC criterion is defined as:
where\(\hat{L}\)is the maximum likelihood of the model and\(d\)is the number of parameters (as well referred to as degrees of
freedom in the previous section).
The definition of BIC replace the constant\(2\)by\(\log(N)\):
where\(N\)is the number of samples.
For a linear Gaussian model, the maximum log-likelihood is defined as:
where\(\sigma^2\)is an estimate of the noise variance,\(y_i\)and\(\hat{y}_i\)are respectively the true and predicted
targets, and\(n\)is the number of samples.
Plugging the maximum log-likelihood in the AIC formula yields:
The first term of the above expression is sometimes discarded since it is a
constant when\(\sigma^2\)is provided. In addition,
it is sometimes stated that the AIC is equivalent to the\(C_p\)statistic[12]. In a strict sense, however, it is equivalent only up to some constant
and a multiplicative factor.
At last, we mentioned above that\(\sigma^2\)is an estimate of the
noise variance. InLassoLarsICwhen the parameternoise_varianceis
not provided (default), the noise variance is estimated via the unbiased
estimator[13]defined as:
LassoLarsIC
noise_variance
where\(p\)is the number of features and\(\hat{y}_i\)is the
predicted target using an ordinary least squares regression. Note, that this
formula is valid only whenn_samples>n_features.
n_samples>n_features
References
Zou, Hui, Trevor Hastie, and Robert Tibshirani.
“On the degrees of freedom of the lasso.”
The Annals of Statistics 35.5 (2007): 2173-2192.
Cherkassky, Vladimir, and Yunqian Ma.
“Comparison of model selection for regression.”
Neural computation 15.7 (2003): 1691-1714.
The equivalence betweenalphaand the regularization parameter of SVM,Cis given byalpha=1/Coralpha=1/(n_samples*C),
depending on the estimator and the exact objective function optimized by the
model.
alpha
C
alpha=1/C
alpha=1/(n_samples*C)
1.1.4.Multi-task Lasso#
TheMultiTaskLassois a linear model that estimates sparse
coefficients for multiple regression problems jointly:yis a 2D array,
of shape(n_samples,n_tasks). The constraint is that the selected
features are the same for all the regression problems, also called tasks.
MultiTaskLasso
y
(n_samples,n_tasks)
The following figure compares the location of the non-zero entries in the
coefficient matrix W obtained with a simple Lasso or a MultiTaskLasso.
The Lasso estimates yield scattered non-zeros while the non-zeros of
the MultiTaskLasso are full columns.
Fitting a time-series model, imposing that any active feature be active at all times.
Examples
Joint feature selection with multi-task Lasso
Joint feature selection with multi-task Lasso
Mathematically, it consists of a linear model trained with a mixed\(\ell_1\)\(\ell_2\)-norm for regularization.
The objective function to minimize is:
where\(\text{Fro}\)indicates the Frobenius norm
and\(\ell_1\)\(\ell_2\)reads
The implementation in the classMultiTaskLassouses
coordinate descent as the algorithm to fit the coefficients.
MultiTaskLasso
1.1.5.Elastic-Net#
ElasticNetis a linear regression model trained with both\(\ell_1\)and\(\ell_2\)-norm regularization of the coefficients.
This combination  allows for learning a sparse model where few of
the weights are non-zero likeLasso, while still maintaining
the regularization properties ofRidge. We control the convex
combination of\(\ell_1\)and\(\ell_2\)using thel1_ratioparameter.
ElasticNet
Lasso
Ridge
l1_ratio
Elastic-net is useful when there are multiple features that are
correlated with one another. Lasso is likely to pick one of these
at random, while elastic-net is likely to pick both.
A practical advantage of trading-off between Lasso and Ridge is that it
allows Elastic-Net to inherit some of Ridge’s stability under rotation.
The objective function to minimize is in this case
The classElasticNetCVcan be used to set the parametersalpha(\(\alpha\)) andl1_ratio(\(\rho\)) by cross-validation.
ElasticNetCV
alpha
l1_ratio
Examples
L1-based models for Sparse Signals
L1-based models for Sparse Signals
Lasso, Lasso-LARS, and Elastic Net paths
Lasso, Lasso-LARS, and Elastic Net paths
Fitting an Elastic Net with a precomputed Gram Matrix and Weighted Samples
Fitting an Elastic Net with a precomputed Gram Matrix and Weighted Samples
The following two references explain the iterations
used in the coordinate descent solver of scikit-learn, as well as
the duality gap computation used for convergence control.
“Regularization Path For Generalized linear Models by Coordinate Descent”,
Friedman, Hastie & Tibshirani, J Stat Softw, 2010 (Paper).
“Regularization Path For Generalized linear Models by Coordinate Descent”,
Friedman, Hastie & Tibshirani, J Stat Softw, 2010 (Paper).
“An Interior-Point Method for Large-Scale L1-Regularized Least Squares,”
S. J. Kim, K. Koh, M. Lustig, S. Boyd and D. Gorinevsky,
in IEEE Journal of Selected Topics in Signal Processing, 2007
(Paper)
“An Interior-Point Method for Large-Scale L1-Regularized Least Squares,”
S. J. Kim, K. Koh, M. Lustig, S. Boyd and D. Gorinevsky,
in IEEE Journal of Selected Topics in Signal Processing, 2007
(Paper)
1.1.6.Multi-task Elastic-Net#
TheMultiTaskElasticNetis an elastic-net model that estimates sparse
coefficients for multiple regression problems jointly:Yis a 2D array
of shape(n_samples,n_tasks). The constraint is that the selected
features are the same for all the regression problems, also called tasks.
MultiTaskElasticNet
Y
(n_samples,n_tasks)
Mathematically, it consists of a linear model trained with a mixed\(\ell_1\)\(\ell_2\)-norm and\(\ell_2\)-norm for regularization.
The objective function to minimize is:
The implementation in the classMultiTaskElasticNetuses coordinate descent as
the algorithm to fit the coefficients.
MultiTaskElasticNet
The classMultiTaskElasticNetCVcan be used to set the parametersalpha(\(\alpha\)) andl1_ratio(\(\rho\)) by cross-validation.
MultiTaskElasticNetCV
alpha
l1_ratio
1.1.7.Least Angle Regression#
Least-angle regression (LARS) is a regression algorithm for
high-dimensional data, developed by Bradley Efron, Trevor Hastie, Iain
Johnstone and Robert Tibshirani. LARS is similar to forward stepwise
regression. At each step, it finds the feature most correlated with the
target. When there are multiple features having equal correlation, instead
of continuing along the same feature, it proceeds in a direction equiangular
between the features.
The advantages of LARS are:
It is numerically efficient in contexts where the number of features
is significantly greater than the number of samples.
It is numerically efficient in contexts where the number of features
is significantly greater than the number of samples.
It is computationally just as fast as forward selection and has
the same order of complexity as ordinary least squares.
It is computationally just as fast as forward selection and has
the same order of complexity as ordinary least squares.
It produces a full piecewise linear solution path, which is
useful in cross-validation or similar attempts to tune the model.
It produces a full piecewise linear solution path, which is
useful in cross-validation or similar attempts to tune the model.
If two features are almost equally correlated with the target,
then their coefficients should increase at approximately the same
rate. The algorithm thus behaves as intuition would expect, and
also is more stable.
If two features are almost equally correlated with the target,
then their coefficients should increase at approximately the same
rate. The algorithm thus behaves as intuition would expect, and
also is more stable.
It is easily modified to produce solutions for other estimators,
like the Lasso.
It is easily modified to produce solutions for other estimators,
like the Lasso.
The disadvantages of the LARS method include:
Because LARS is based upon an iterative refitting of the
residuals, it would appear to be especially sensitive to the
effects of noise. This problem is discussed in detail by Weisberg
in the discussion section of the Efron et al. (2004) Annals of
Statistics article.
Because LARS is based upon an iterative refitting of the
residuals, it would appear to be especially sensitive to the
effects of noise. This problem is discussed in detail by Weisberg
in the discussion section of the Efron et al. (2004) Annals of
Statistics article.
The LARS model can be used via the estimatorLars, or its
low-level implementationlars_pathorlars_path_gram.
Lars
lars_path
lars_path_gram
1.1.8.LARS Lasso#
LassoLarsis a lasso model implemented using the LARS
algorithm, and unlike the implementation based on coordinate descent,
this yields the exact solution, which is piecewise linear as a
function of the norm of its coefficients.
LassoLars
>>>fromsklearnimportlinear_model>>>reg=linear_model.LassoLars(alpha=.1)>>>reg.fit([[0,0],[1,1]],[0,1])LassoLars(alpha=0.1)>>>reg.coef_array([0.6..., 0.        ])
Examples
Lasso, Lasso-LARS, and Elastic Net paths
Lasso, Lasso-LARS, and Elastic Net paths
The Lars algorithm provides the full path of the coefficients along
the regularization parameter almost for free, thus a common operation
is to retrieve the path with one of the functionslars_pathorlars_path_gram.
lars_path
lars_path_gram
The algorithm is similar to forward stepwise regression, but instead
of including features at each step, the estimated coefficients are
increased in a direction equiangular to each one’s correlations with
the residual.
Instead of giving a vector result, the LARS solution consists of a
curve denoting the solution for each value of the\(\ell_1\)norm of the
parameter vector. The full coefficients path is stored in the arraycoef_path_of shape(n_features,max_features+1). The first
column is always zero.
coef_path_
(n_features,max_features+1)
References
Original Algorithm is detailed in the paperLeast Angle Regressionby Hastie et al.
Original Algorithm is detailed in the paperLeast Angle Regressionby Hastie et al.
1.1.9.Orthogonal Matching Pursuit (OMP)#
OrthogonalMatchingPursuitandorthogonal_mpimplement the OMP
algorithm for approximating the fit of a linear model with constraints imposed
on the number of non-zero coefficients (ie. the\(\ell_0\)pseudo-norm).
OrthogonalMatchingPursuit
orthogonal_mp
Being a forward feature selection method likeLeast Angle Regression,
orthogonal matching pursuit can approximate the optimum solution vector with a
fixed number of non-zero elements:
Alternatively, orthogonal matching pursuit can target a specific error instead
of a specific number of non-zero coefficients. This can be expressed as:
OMP is based on a greedy algorithm that includes at each step the atom most
highly correlated with the current residual. It is similar to the simpler
matching pursuit (MP) method, but better in that at each iteration, the
residual is recomputed using an orthogonal projection on the space of the
previously chosen dictionary elements.
Examples
Orthogonal Matching Pursuit
Orthogonal Matching Pursuit
https://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf
https://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf
Matching pursuits with time-frequency dictionaries,
S. G. Mallat, Z. Zhang,
Matching pursuits with time-frequency dictionaries,
S. G. Mallat, Z. Zhang,
1.1.10.Bayesian Regression#
Bayesian regression techniques can be used to include regularization
parameters in the estimation procedure: the regularization parameter is
not set in a hard sense but tuned to the data at hand.
This can be done by introducinguninformative priorsover the hyper parameters of the model.
The\(\ell_{2}\)regularization used inRidge regression and classificationis
equivalent to finding a maximum a posteriori estimation under a Gaussian prior
over the coefficients\(w\)with precision\(\lambda^{-1}\).
Instead of settinglambdamanually, it is possible to treat it as a random
variable to be estimated from the data.
lambda
To obtain a fully probabilistic model, the output\(y\)is assumed
to be Gaussian distributed around\(X w\):
where\(\alpha\)is again treated as a random variable that is to be
estimated from the data.
The advantages of Bayesian Regression are:
It adapts to the data at hand.
It adapts to the data at hand.
It can be used to include regularization parameters in the
estimation procedure.
It can be used to include regularization parameters in the
estimation procedure.
The disadvantages of Bayesian regression include:
Inference of the model can be time consuming.
Inference of the model can be time consuming.
A good introduction to Bayesian methods is given in C. Bishop: Pattern
Recognition and Machine learning
A good introduction to Bayesian methods is given in C. Bishop: Pattern
Recognition and Machine learning
Original Algorithm is detailed in the  bookBayesianlearningforneuralnetworksby Radford M. Neal
Original Algorithm is detailed in the  bookBayesianlearningforneuralnetworksby Radford M. Neal
Bayesianlearningforneuralnetworks
1.1.10.1.Bayesian Ridge Regression#
BayesianRidgeestimates a probabilistic model of the
regression problem as described above.
The prior for the coefficient\(w\)is given by a spherical Gaussian:
BayesianRidge
The priors over\(\alpha\)and\(\lambda\)are chosen to begamma
distributions, the
conjugate prior for the precision of the Gaussian. The resulting model is
calledBayesian Ridge Regression, and is similar to the classicalRidge.
Ridge
The parameters\(w\),\(\alpha\)and\(\lambda\)are estimated
jointly during the fit of the model, the regularization parameters\(\alpha\)and\(\lambda\)being estimated by maximizing thelog marginal likelihood. The scikit-learn implementation
is based on the algorithm described in Appendix A of (Tipping, 2001)
where the update of the parameters\(\alpha\)and\(\lambda\)is done
as suggested in (MacKay, 1992). The initial value of the maximization procedure
can be set with the hyperparametersalpha_initandlambda_init.
alpha_init
lambda_init
There are four more hyperparameters,\(\alpha_1\),\(\alpha_2\),\(\lambda_1\)and\(\lambda_2\)of the gamma prior distributions over\(\alpha\)and\(\lambda\). These are usually chosen to benon-informative. By default\(\alpha_1 = \alpha_2 =  \lambda_1 = \lambda_2 = 10^{-6}\).
Bayesian Ridge Regression is used for regression:
>>>fromsklearnimportlinear_model>>>X=[[0.,0.],[1.,1.],[2.,2.],[3.,3.]]>>>Y=[0.,1.,2.,3.]>>>reg=linear_model.BayesianRidge()>>>reg.fit(X,Y)BayesianRidge()
After being fitted, the model can then be used to predict new values:
>>>reg.predict([[1,0.]])array([0.50000013])
The coefficients\(w\)of the model can be accessed:
>>>reg.coef_array([0.49999993, 0.49999993])
Due to the Bayesian framework, the weights found are slightly different to the
ones found byOrdinary Least Squares. However, Bayesian Ridge Regression
is more robust to ill-posed problems.
Examples
Curve Fitting with Bayesian Ridge Regression
Curve Fitting with Bayesian Ridge Regression
Section 3.3 in Christopher M. Bishop: Pattern Recognition and Machine Learning, 2006
Section 3.3 in Christopher M. Bishop: Pattern Recognition and Machine Learning, 2006
David J. C. MacKay,Bayesian Interpolation, 1992.
David J. C. MacKay,Bayesian Interpolation, 1992.
Michael E. Tipping,Sparse Bayesian Learning and the Relevance Vector Machine, 2001.
Michael E. Tipping,Sparse Bayesian Learning and the Relevance Vector Machine, 2001.
1.1.10.2.Automatic Relevance Determination - ARD#
The Automatic Relevance Determination (as being implemented inARDRegression) is a kind of linear model which is very similar to theBayesian Ridge Regression, but that leads to sparser coefficients\(w\)[1][2].
ARDRegression
ARDRegressionposes a different prior over\(w\): it drops
the spherical Gaussian distribution for a centered elliptic Gaussian
distribution. This means each coefficient\(w_{i}\)can itself be drawn from
a Gaussian distribution, centered on zero and with a precision\(\lambda_{i}\):
ARDRegression
with\(A\)being a positive definite diagonal matrix and\(\text{diag}(A) = \lambda = \{\lambda_{1},...,\lambda_{p}\}\).
In contrast to theBayesian Ridge Regression, each coordinate of\(w_{i}\)has its own standard deviation\(\frac{1}{\lambda_i}\). The
prior over all\(\lambda_i\)is chosen to be the same gamma distribution
given by the hyperparameters\(\lambda_1\)and\(\lambda_2\).
ARD is also known in the literature asSparse Bayesian LearningandRelevance
Vector Machine[3][4]. For a worked-out comparison between ARD andBayesian
Ridge Regression, see the example below.
Examples
Comparing Linear Bayesian Regressors
Comparing Linear Bayesian Regressors
References
Christopher M. Bishop: Pattern Recognition and Machine Learning, Chapter 7.2.1
David Wipf and Srikantan Nagarajan:A New View of Automatic Relevance Determination
Michael E. Tipping:Sparse Bayesian Learning and the Relevance Vector Machine
Tristan Fletcher:Relevance Vector Machines Explained
1.1.11.Logistic regression#
The logistic regression is implemented inLogisticRegression. Despite
its name, it is implemented as a linear model for classification rather than
regression in terms of the scikit-learn/ML nomenclature. The logistic
regression is also known in the literature as logit regression,
maximum-entropy classification (MaxEnt) or the log-linear classifier. In this
model, the probabilities describing the possible outcomes of a single trial
are modeled using alogistic function.
LogisticRegression
This implementation can fit binary, One-vs-Rest, or multinomial logistic
regression with optional\(\ell_1\),\(\ell_2\)or Elastic-Net
regularization.
Note
Regularization
Regularization is applied by default, which is common in machine
learning but not in statistics. Another advantage of regularization is
that it improves numerical stability. No regularization amounts to
setting C to a very high value.
Note
Logistic Regression as a special case of the Generalized Linear Models (GLM)
Logistic regression is a special case ofGeneralized Linear Modelswith a Binomial / Bernoulli conditional
distribution and a Logit link. The numerical output of the logistic
regression, which is the predicted probability, can be used as a classifier
by applying a threshold (by default 0.5) to it. This is how it is
implemented in scikit-learn, so it expects a categorical target, making
the Logistic Regression a classifier.
Examples
L1 Penalty and Sparsity in Logistic Regression
L1 Penalty and Sparsity in Logistic Regression
Regularization path of L1- Logistic Regression
Regularization path of L1- Logistic Regression
Decision Boundaries of Multinomial and One-vs-Rest Logistic Regression
Decision Boundaries of Multinomial and One-vs-Rest Logistic Regression
Multiclass sparse logistic regression on 20newgroups
Multiclass sparse logistic regression on 20newgroups
MNIST classification using multinomial logistic + L1
MNIST classification using multinomial logistic + L1
Plot classification probability
Plot classification probability
1.1.11.1.Binary Case#
For notational ease, we assume that the target\(y_i\)takes values in the
set\(\{0, 1\}\)for data point\(i\).
Once fitted, thepredict_probamethod ofLogisticRegressionpredicts
the probability of the positive class\(P(y_i=1|X_i)\)as
predict_proba
LogisticRegression
As an optimization problem, binary
class logistic regression with regularization term\(r(w)\)minimizes the
following cost function:
where\({s_i}\)corresponds to the weights assigned by the user to a
specific training sample (the vector\(s\)is formed by element-wise
multiplication of the class weights and sample weights),
and the sum\(S = \sum_{i=1}^n s_i\).
We currently provide four choices for the regularization term\(r(w)\)via
thepenaltyargument:
penalty
penalty
\(r(w)\)
None
None
\(0\)
\(\ell_1\)
\(\|w\|_1\)
\(\ell_2\)
\(\frac{1}{2}\|w\|_2^2 = \frac{1}{2}w^T w\)
ElasticNet
ElasticNet
\(\frac{1 - \rho}{2}w^T w + \rho \|w\|_1\)
For ElasticNet,\(\rho\)(which corresponds to thel1_ratioparameter)
controls the strength of\(\ell_1\)regularization vs.\(\ell_2\)regularization. Elastic-Net is equivalent to\(\ell_1\)when\(\rho = 1\)and equivalent to\(\ell_2\)when\(\rho=0\).
l1_ratio
Note that the scale of the class weights and the sample weights will influence
the optimization problem. For instance, multiplying the sample weights by a
constant\(b>0\)is equivalent to multiplying the (inverse) regularization
strengthCby\(b\).
C
1.1.11.2.Multinomial Case#
The binary case can be extended to\(K\)classes leading to the multinomial
logistic regression, see alsolog-linear model.
Note
It is possible to parameterize a\(K\)-class classification model
using only\(K-1\)weight vectors, leaving one class probability fully
determined by the other class probabilities by leveraging the fact that all
class probabilities must sum to one. We deliberately choose to overparameterize the model
using\(K\)weight vectors for ease of implementation and to preserve the
symmetrical inductive bias regarding ordering of classes, see[16]. This effect becomes
especially important when using regularization. The choice of overparameterization can be
detrimental for unpenalized models since then the solution may not be unique, as shown in[16].
Let\(y_i \in {1, \ldots, K}\)be the label (ordinal) encoded target variable for observation\(i\).
Instead of a single coefficient vector, we now have
a matrix of coefficients\(W\)where each row vector\(W_k\)corresponds to class\(k\). We aim at predicting the class probabilities\(P(y_i=k|X_i)\)viapredict_probaas:
predict_proba
The objective for the optimization becomes
where\([P]\)represents the Iverson bracket which evaluates to\(0\)if\(P\)is false, otherwise it evaluates to\(1\).
Again,\(s_{ik}\)are the weights assigned by the user (multiplication of sample
weights and class weights) with their sum\(S = \sum_{i=1}^n \sum_{k=0}^{K-1} s_{ik}\).
We currently provide four choices
for the regularization term\(r(W)\)via thepenaltyargument, where\(m\)is the number of features:
penalty
penalty
\(r(W)\)
None
None
\(0\)
\(\ell_1\)
\(\|W\|_{1,1} = \sum_{i=1}^m\sum_{j=1}^{K}|W_{i,j}|\)
\(\ell_2\)
\(\frac{1}{2}\|W\|_F^2 = \frac{1}{2}\sum_{i=1}^m\sum_{j=1}^{K} W_{i,j}^2\)
ElasticNet
ElasticNet
\(\frac{1 - \rho}{2}\|W\|_F^2 + \rho \|W\|_{1,1}\)
1.1.11.3.Solvers#
The solvers implemented in the classLogisticRegressionare “lbfgs”, “liblinear”, “newton-cg”, “newton-cholesky”, “sag” and “saga”:
LogisticRegression
The following table summarizes the penalties and multinomial multiclass supported by each solver:
Solvers
Penalties
‘lbfgs’
‘liblinear’
‘newton-cg’
‘newton-cholesky’
‘sag’
‘saga’
L2 penalty
yes
no
yes
no
yes
yes
L1 penalty
no
yes
no
no
no
yes
Elastic-Net (L1 + L2)
no
no
no
no
no
yes
No penalty (‘none’)
yes
no
yes
yes
yes
yes
Multiclass support
multinomial multiclass
yes
no
yes
no
yes
yes
Behaviors
Penalize the intercept (bad)
no
yes
no
no
no
no
Faster for large datasets
no
no
no
no
yes
yes
Robust to unscaled datasets
yes
yes
yes
yes
no
no
The “lbfgs” solver is used by default for its robustness. For large datasets
the “saga” solver is usually faster.
For large dataset, you may also consider usingSGDClassifierwithloss="log_loss", which might be even faster but requires more tuning.
SGDClassifier
loss="log_loss"
There might be a difference in the scores obtained betweenLogisticRegressionwithsolver=liblinearorLinearSVCand the external liblinear library directly,
whenfit_intercept=Falseand the fitcoef_(or) the data to be predicted
are zeroes. This is because for the sample(s) withdecision_functionzero,LogisticRegressionandLinearSVCpredict the
negative class, while liblinear predicts the positive class. Note that a model
withfit_intercept=Falseand having many samples withdecision_functionzero, is likely to be a underfit, bad model and you are advised to setfit_intercept=Trueand increase theintercept_scaling.
LogisticRegression
solver=liblinear
LinearSVC
fit_intercept=False
coef_
decision_function
LogisticRegression
LinearSVC
fit_intercept=False
decision_function
fit_intercept=True
intercept_scaling
The solver “liblinear” uses a coordinate descent (CD) algorithm, and relies
on the excellent C++LIBLINEAR library, which is shipped with
scikit-learn. However, the CD algorithm implemented in liblinear cannot learn
a true multinomial (multiclass) model; instead, the optimization problem is
decomposed in a “one-vs-rest” fashion so separate binary classifiers are
trained for all classes. This happens under the hood, soLogisticRegressioninstances using this solver behave as multiclass
classifiers. For\(\ell_1\)regularizationsklearn.svm.l1_min_callows to
calculate the lower bound for C in order to get a non “null” (all feature
weights to zero) model.
The solver “liblinear” uses a coordinate descent (CD) algorithm, and relies
on the excellent C++LIBLINEAR library, which is shipped with
scikit-learn. However, the CD algorithm implemented in liblinear cannot learn
a true multinomial (multiclass) model; instead, the optimization problem is
decomposed in a “one-vs-rest” fashion so separate binary classifiers are
trained for all classes. This happens under the hood, soLogisticRegressioninstances using this solver behave as multiclass
classifiers. For\(\ell_1\)regularizationsklearn.svm.l1_min_callows to
calculate the lower bound for C in order to get a non “null” (all feature
weights to zero) model.
LogisticRegression
sklearn.svm.l1_min_c
The “lbfgs”, “newton-cg” and “sag” solvers only support\(\ell_2\)regularization or no regularization, and are found to converge faster for some
high-dimensional data. Settingmulti_classto “multinomial” with these solvers
learns a true multinomial logistic regression model[5], which means that its
probability estimates should be better calibrated than the default “one-vs-rest”
setting.
The “lbfgs”, “newton-cg” and “sag” solvers only support\(\ell_2\)regularization or no regularization, and are found to converge faster for some
high-dimensional data. Settingmulti_classto “multinomial” with these solvers
learns a true multinomial logistic regression model[5], which means that its
probability estimates should be better calibrated than the default “one-vs-rest”
setting.
multi_class
The “sag” solver uses Stochastic Average Gradient descent[6]. It is faster
than other solvers for large datasets, when both the number of samples and the
number of features are large.
The “sag” solver uses Stochastic Average Gradient descent[6]. It is faster
than other solvers for large datasets, when both the number of samples and the
number of features are large.
The “saga” solver[7]is a variant of “sag” that also supports the
non-smoothpenalty="l1". This is therefore the solver of choice for sparse
multinomial logistic regression. It is also the only solver that supportspenalty="elasticnet".
The “saga” solver[7]is a variant of “sag” that also supports the
non-smoothpenalty="l1". This is therefore the solver of choice for sparse
multinomial logistic regression. It is also the only solver that supportspenalty="elasticnet".
penalty="l1"
penalty="elasticnet"
The “lbfgs” is an optimization algorithm that approximates the
Broyden–Fletcher–Goldfarb–Shanno algorithm[8], which belongs to
quasi-Newton methods. As such, it can deal with a wide range of different training
data and is therefore the default solver. Its performance, however, suffers on poorly
scaled datasets and on datasets with one-hot encoded categorical features with rare
categories.
The “lbfgs” is an optimization algorithm that approximates the
Broyden–Fletcher–Goldfarb–Shanno algorithm[8], which belongs to
quasi-Newton methods. As such, it can deal with a wide range of different training
data and is therefore the default solver. Its performance, however, suffers on poorly
scaled datasets and on datasets with one-hot encoded categorical features with rare
categories.
The “newton-cholesky” solver is an exact Newton solver that calculates the hessian
matrix and solves the resulting linear system. It is a very good choice forn_samples>>n_features, but has a few shortcomings: Only\(\ell_2\)regularization is supported. Furthermore, because the hessian matrix is explicitly
computed, the memory usage has a quadratic dependency onn_featuresas well as onn_classes. As a consequence, only the one-vs-rest scheme is implemented for the
multiclass case.
The “newton-cholesky” solver is an exact Newton solver that calculates the hessian
matrix and solves the resulting linear system. It is a very good choice forn_samples>>n_features, but has a few shortcomings: Only\(\ell_2\)regularization is supported. Furthermore, because the hessian matrix is explicitly
computed, the memory usage has a quadratic dependency onn_featuresas well as onn_classes. As a consequence, only the one-vs-rest scheme is implemented for the
multiclass case.
n_samples
n_features
n_features
n_classes
For a comparison of some of these solvers, see[9].
References
Christopher M. Bishop: Pattern Recognition and Machine Learning, Chapter 4.3.4
Mark Schmidt, Nicolas Le Roux, and Francis Bach:Minimizing Finite Sums with the Stochastic Average Gradient.
Aaron Defazio, Francis Bach, Simon Lacoste-Julien:SAGA: A Fast Incremental Gradient Method With Support for
Non-Strongly Convex Composite Objectives.
https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm
Thomas P. Minka“A comparison of numerical optimizers for logistic regression”
Simon, Noah, J. Friedman and T. Hastie.
“A Blockwise Descent Algorithm for Group-penalized Multiresponse and
Multinomial Regression.”
Note
Feature selection with sparse logistic regression
A logistic regression with\(\ell_1\)penalty yields sparse models, and can
thus be used to perform feature selection, as detailed inL1-based feature selection.
Note
P-value estimation
It is possible to obtain the p-values and confidence intervals for
coefficients in cases of regression without penalization. Thestatsmodels
packagenatively supports this.
Within sklearn, one could use bootstrapping instead as well.
LogisticRegressionCVimplements Logistic Regression with built-in
cross-validation support, to find the optimalCandl1_ratioparameters
according to thescoringattribute. The “newton-cg”, “sag”, “saga” and
“lbfgs” solvers are found to be faster for high-dimensional dense data, due
to warm-starting (seeGlossary).
LogisticRegressionCV
C
l1_ratio
scoring
1.1.12.Generalized Linear Models#
Generalized Linear Models (GLM) extend linear models in two ways[10]. First, the predicted values\(\hat{y}\)are linked to a linear
combination of the input variables\(X\)via an inverse link function\(h\)as
Secondly, the squared loss function is replaced by the unit deviance\(d\)of a distribution in the exponential family (or more precisely, a
reproductive exponential dispersion model (EDM)[11]).
The minimization problem becomes:
where\(\alpha\)is the L2 regularization penalty. When sample weights are
provided, the average becomes a weighted average.
The following table lists some specific EDMs and their unit deviance :
Distribution
Target Domain
Unit Deviance\(d(y, \hat{y})\)
Normal
\(y \in (-\infty, \infty)\)
\((y-\hat{y})^2\)
Bernoulli
\(y \in \{0, 1\}\)
\(2({y}\log\frac{y}{\hat{y}}+({1}-{y})\log\frac{{1}-{y}}{{1}-\hat{y}})\)
Categorical
\(y \in \{0, 1, ..., k\}\)
\(2\sum_{i \in \{0, 1, ..., k\}} I(y = i) y_\text{i}\log\frac{I(y = i)}{\hat{I(y = i)}}\)
Poisson
\(y \in [0, \infty)\)
\(2(y\log\frac{y}{\hat{y}}-y+\hat{y})\)
Gamma
\(y \in (0, \infty)\)
\(2(\log\frac{\hat{y}}{y}+\frac{y}{\hat{y}}-1)\)
Inverse Gaussian
\(y \in (0, \infty)\)
\(\frac{(y-\hat{y})^2}{y\hat{y}^2}\)
The Probability Density Functions (PDF) of these distributions are illustrated
in the following figure,
PDF of a random variable Y following Poisson, Tweedie (power=1.5) and Gamma
distributions with different mean values (\(\mu\)). Observe the point
mass at\(Y=0\)for the Poisson distribution and the Tweedie (power=1.5)
distribution, but not for the Gamma distribution which has a strictly
positive target domain.#
The Bernoulli distribution is a discrete probability distribution modelling a
Bernoulli trial - an event that has only two mutually exclusive outcomes.
The Categorical distribution is a generalization of the Bernoulli distribution
for a categorical random variable. While a random variable in a Bernoulli
distribution has two possible outcomes, a Categorical random variable can take
on one of K possible categories, with the probability of each category
specified separately.
The choice of the distribution depends on the problem at hand:
If the target values\(y\)are counts (non-negative integer valued) or
relative frequencies (non-negative), you might use a Poisson distribution
with a log-link.
If the target values\(y\)are counts (non-negative integer valued) or
relative frequencies (non-negative), you might use a Poisson distribution
with a log-link.
If the target values are positive valued and skewed, you might try a Gamma
distribution with a log-link.
If the target values are positive valued and skewed, you might try a Gamma
distribution with a log-link.
If the target values seem to be heavier tailed than a Gamma distribution, you
might try an Inverse Gaussian distribution (or even higher variance powers of
the Tweedie family).
If the target values seem to be heavier tailed than a Gamma distribution, you
might try an Inverse Gaussian distribution (or even higher variance powers of
the Tweedie family).
If the target values\(y\)are probabilities, you can use the Bernoulli
distribution. The Bernoulli distribution with a logit link can be used for
binary classification. The Categorical distribution with a softmax link can be
used for multiclass classification.
If the target values\(y\)are probabilities, you can use the Bernoulli
distribution. The Bernoulli distribution with a logit link can be used for
binary classification. The Categorical distribution with a softmax link can be
used for multiclass classification.
Agriculture / weather modeling:  number of rain events per year (Poisson),
amount of rainfall per event (Gamma), total rainfall per year (Tweedie /
Compound Poisson Gamma).
Agriculture / weather modeling:  number of rain events per year (Poisson),
amount of rainfall per event (Gamma), total rainfall per year (Tweedie /
Compound Poisson Gamma).
Risk modeling / insurance policy pricing:  number of claim events /
policyholder per year (Poisson), cost per event (Gamma), total cost per
policyholder per year (Tweedie / Compound Poisson Gamma).
Risk modeling / insurance policy pricing:  number of claim events /
policyholder per year (Poisson), cost per event (Gamma), total cost per
policyholder per year (Tweedie / Compound Poisson Gamma).
Credit Default: probability that a loan can’t be paid back (Bernoulli).
Credit Default: probability that a loan can’t be paid back (Bernoulli).
Fraud Detection: probability that a financial transaction like a cash transfer
is a fraudulent transaction (Bernoulli).
Fraud Detection: probability that a financial transaction like a cash transfer
is a fraudulent transaction (Bernoulli).
Predictive maintenance: number of production interruption events per year
(Poisson), duration of interruption (Gamma), total interruption time per year
(Tweedie / Compound Poisson Gamma).
Predictive maintenance: number of production interruption events per year
(Poisson), duration of interruption (Gamma), total interruption time per year
(Tweedie / Compound Poisson Gamma).
Medical Drug Testing: probability of curing a patient in a set of trials or
probability that a patient will experience side effects (Bernoulli).
Medical Drug Testing: probability of curing a patient in a set of trials or
probability that a patient will experience side effects (Bernoulli).
News Classification: classification of news articles into three categories
namely Business News, Politics and Entertainment news (Categorical).
News Classification: classification of news articles into three categories
namely Business News, Politics and Entertainment news (Categorical).
References
McCullagh, Peter; Nelder, John (1989). Generalized Linear Models,
Second Edition. Boca Raton: Chapman and Hall/CRC. ISBN 0-412-31760-5.
Jørgensen, B. (1992). The theory of exponential dispersion models
and analysis of deviance. Monografias de matemática, no. 51.  See alsoExponential dispersion model.
1.1.12.1.Usage#
TweedieRegressorimplements a generalized linear model for the
Tweedie distribution, that allows to model any of the above mentioned
distributions using the appropriatepowerparameter. In particular:
TweedieRegressor
power
power=0: Normal distribution. Specific estimators such asRidge,ElasticNetare generally more appropriate in
this case.
power=0: Normal distribution. Specific estimators such asRidge,ElasticNetare generally more appropriate in
this case.
power=0
Ridge
ElasticNet
power=1: Poisson distribution.PoissonRegressoris exposed
for convenience. However, it is strictly equivalent toTweedieRegressor(power=1,link='log').
power=1: Poisson distribution.PoissonRegressoris exposed
for convenience. However, it is strictly equivalent toTweedieRegressor(power=1,link='log').
power=1
PoissonRegressor
TweedieRegressor(power=1,link='log')
power=2: Gamma distribution.GammaRegressoris exposed for
convenience. However, it is strictly equivalent toTweedieRegressor(power=2,link='log').
power=2: Gamma distribution.GammaRegressoris exposed for
convenience. However, it is strictly equivalent toTweedieRegressor(power=2,link='log').
power=2
GammaRegressor
TweedieRegressor(power=2,link='log')
power=3: Inverse Gaussian distribution.
power=3: Inverse Gaussian distribution.
power=3
The link function is determined by thelinkparameter.
link
Usage example:
>>>fromsklearn.linear_modelimportTweedieRegressor>>>reg=TweedieRegressor(power=1,alpha=0.5,link='log')>>>reg.fit([[0,0],[0,1],[2,2]],[0,1,2])TweedieRegressor(alpha=0.5, link='log', power=1)>>>reg.coef_array([0.2463..., 0.4337...])>>>reg.intercept_-0.7638...
Examples
Poisson regression and non-normal loss
Poisson regression and non-normal loss
Tweedie regression on insurance claims
Tweedie regression on insurance claims
The feature matrixXshould be standardized before fitting. This ensures
that the penalty treats features equally.
X
Since the linear predictor\(Xw\)can be negative and Poisson,
Gamma and Inverse Gaussian distributions don’t support negative values, it
is necessary to apply an inverse link function that guarantees the
non-negativeness. For example withlink='log', the inverse link function
becomes\(h(Xw)=\exp(Xw)\).
link='log'
If you want to model a relative frequency, i.e. counts per exposure (time,
volume, …) you can do so by using a Poisson distribution and passing\(y=\frac{\mathrm{counts}}{\mathrm{exposure}}\)as target values
together with\(\mathrm{exposure}\)as sample weights. For a concrete
example see e.g.Tweedie regression on insurance claims.
When performing cross-validation for thepowerparameter ofTweedieRegressor, it is advisable to specify an explicitscoringfunction,
because the default scorerTweedieRegressor.scoreis a function ofpoweritself.
power
TweedieRegressor
scoring
TweedieRegressor.score
power
1.1.13.Stochastic Gradient Descent - SGD#
Stochastic gradient descent is a simple yet very efficient approach
to fit linear models. It is particularly useful when the number of samples
(and the number of features) is very large.
Thepartial_fitmethod allows online/out-of-core learning.
partial_fit
The classesSGDClassifierandSGDRegressorprovide
functionality to fit linear models for classification and regression
using different (convex) loss functions and different penalties.
E.g., withloss="log",SGDClassifierfits a logistic regression model,
while withloss="hinge"it fits a linear support vector machine (SVM).
SGDClassifier
SGDRegressor
loss="log"
SGDClassifier
loss="hinge"
You can refer to the dedicatedStochastic Gradient Descentdocumentation section for more details.
1.1.14.Perceptron#
ThePerceptronis another simple classification algorithm suitable for
large scale learning. By default:
Perceptron
It does not require a learning rate.
It does not require a learning rate.
It is not regularized (penalized).
It is not regularized (penalized).
It updates its model only on mistakes.
It updates its model only on mistakes.
The last characteristic implies that the Perceptron is slightly faster to
train than SGD with the hinge loss and that the resulting models are
sparser.
In fact, thePerceptronis a wrapper around theSGDClassifierclass using a perceptron loss and a constant learning rate. Refer tomathematical sectionof the SGD procedure
for more details.
Perceptron
SGDClassifier
1.1.15.Passive Aggressive Algorithms#
The passive-aggressive algorithms are a family of algorithms for large-scale
learning. They are similar to the Perceptron in that they do not require a
learning rate. However, contrary to the Perceptron, they include a
regularization parameterC.
C
For classification,PassiveAggressiveClassifiercan be used withloss='hinge'(PA-I) orloss='squared_hinge'(PA-II).  For regression,PassiveAggressiveRegressorcan be used withloss='epsilon_insensitive'(PA-I) orloss='squared_epsilon_insensitive'(PA-II).
PassiveAggressiveClassifier
loss='hinge'
loss='squared_hinge'
PassiveAggressiveRegressor
loss='epsilon_insensitive'
loss='squared_epsilon_insensitive'
“Online Passive-Aggressive Algorithms”K. Crammer, O. Dekel, J. Keshat, S. Shalev-Shwartz, Y. Singer - JMLR 7 (2006)
“Online Passive-Aggressive Algorithms”K. Crammer, O. Dekel, J. Keshat, S. Shalev-Shwartz, Y. Singer - JMLR 7 (2006)
1.1.16.Robustness regression: outliers and modeling errors#
Robust regression aims to fit a regression model in the
presence of corrupt data: either outliers, or error in the model.
1.1.16.1.Different scenario and useful concepts#
There are different things to keep in mind when dealing with data
corrupted by outliers:
Outliers in X or in y?Outliers in the y directionOutliers in the X direction
Outliers in X or in y?
Outliers in the y direction
Outliers in the X direction
Fraction of outliers versus amplitude of errorThe number of outlying points matters, but also how much they are
outliers.Small outliersLarge outliers
Fraction of outliers versus amplitude of error
The number of outlying points matters, but also how much they are
outliers.
Small outliers
Large outliers
An important notion of robust fitting is that of breakdown point: the
fraction of data that can be outlying for the fit to start missing the
inlying data.
Note that in general, robust fitting in high-dimensional setting (largen_features) is very hard. The robust models here will probably not work
in these settings.
n_features
Trade-offs: which estimator ?
Scikit-learn provides 3 robust regression estimators:RANSAC,Theil SenandHuberRegressor.
HuberRegressorshould be faster thanRANSACandTheil Senunless the number of samples are very large, i.e.n_samples>>n_features.
This is becauseRANSACandTheil Senfit on smaller subsets of the data. However, bothTheil SenandRANSACare unlikely to be as robust asHuberRegressorfor the default parameters.
HuberRegressorshould be faster thanRANSACandTheil Senunless the number of samples are very large, i.e.n_samples>>n_features.
This is becauseRANSACandTheil Senfit on smaller subsets of the data. However, bothTheil SenandRANSACare unlikely to be as robust asHuberRegressorfor the default parameters.
n_samples
n_features
RANSACis faster thanTheil Senand scales much better with the number of samples.
RANSACis faster thanTheil Senand scales much better with the number of samples.
RANSACwill deal better with large
outliers in the y direction (most common situation).
RANSACwill deal better with large
outliers in the y direction (most common situation).
Theil Senwill cope better with
medium-size outliers in the X direction, but this property will
disappear in high-dimensional settings.
Theil Senwill cope better with
medium-size outliers in the X direction, but this property will
disappear in high-dimensional settings.
When in doubt, useRANSAC.
1.1.16.2.RANSAC: RANdom SAmple Consensus#
RANSAC (RANdom SAmple Consensus) fits a model from random subsets of
inliers from the complete data set.
RANSAC is a non-deterministic algorithm producing only a reasonable result with
a certain probability, which is dependent on the number of iterations (seemax_trialsparameter). It is typically used for linear and non-linear
regression problems and is especially popular in the field of photogrammetric
computer vision.
max_trials
The algorithm splits the complete input sample data into a set of inliers,
which may be subject to noise, and outliers, which are e.g. caused by erroneous
measurements or invalid hypotheses about the data. The resulting model is then
estimated only from the determined inliers.
Examples
Robust linear model estimation using RANSAC
Robust linear model estimation using RANSAC
Robust linear estimator fitting
Robust linear estimator fitting
Each iteration performs the following steps:
Selectmin_samplesrandom samples from the original data and check
whether the set of data is valid (seeis_data_valid).
Selectmin_samplesrandom samples from the original data and check
whether the set of data is valid (seeis_data_valid).
min_samples
is_data_valid
Fit a model to the random subset (estimator.fit) and check
whether the estimated model is valid (seeis_model_valid).
Fit a model to the random subset (estimator.fit) and check
whether the estimated model is valid (seeis_model_valid).
estimator.fit
is_model_valid
Classify all data as inliers or outliers by calculating the residuals
to the estimated model (estimator.predict(X)-y) - all data
samples with absolute residuals smaller than or equal to theresidual_thresholdare considered as inliers.
Classify all data as inliers or outliers by calculating the residuals
to the estimated model (estimator.predict(X)-y) - all data
samples with absolute residuals smaller than or equal to theresidual_thresholdare considered as inliers.
estimator.predict(X)-y
residual_threshold
Save fitted model as best model if number of inlier samples is
maximal. In case the current estimated model has the same number of
inliers, it is only considered as the best model if it has better score.
Save fitted model as best model if number of inlier samples is
maximal. In case the current estimated model has the same number of
inliers, it is only considered as the best model if it has better score.
These steps are performed either a maximum number of times (max_trials) or
until one of the special stop criteria are met (seestop_n_inliersandstop_score). The final model is estimated using all inlier samples (consensus
set) of the previously determined best model.
max_trials
stop_n_inliers
stop_score
Theis_data_validandis_model_validfunctions allow to identify and reject
degenerate combinations of random sub-samples. If the estimated model is not
needed for identifying degenerate cases,is_data_validshould be used as it
is called prior to fitting the model and thus leading to better computational
performance.
is_data_valid
is_model_valid
is_data_valid
https://en.wikipedia.org/wiki/RANSAC
https://en.wikipedia.org/wiki/RANSAC
“Random Sample Consensus: A Paradigm for Model Fitting with Applications to
Image Analysis and Automated Cartography”Martin A. Fischler and Robert C. Bolles - SRI International (1981)
“Random Sample Consensus: A Paradigm for Model Fitting with Applications to
Image Analysis and Automated Cartography”Martin A. Fischler and Robert C. Bolles - SRI International (1981)
“Performance Evaluation of RANSAC Family”Sunglok Choi, Taemin Kim and Wonpil Yu - BMVC (2009)
“Performance Evaluation of RANSAC Family”Sunglok Choi, Taemin Kim and Wonpil Yu - BMVC (2009)
1.1.16.3.Theil-Sen estimator: generalized-median-based estimator#
TheTheilSenRegressorestimator uses a generalization of the median in
multiple dimensions. It is thus robust to multivariate outliers. Note however
that the robustness of the estimator decreases quickly with the dimensionality
of the problem. It loses its robustness properties and becomes no
better than an ordinary least squares in high dimension.
TheilSenRegressor
Examples
Theil-Sen Regression
Theil-Sen Regression
Robust linear estimator fitting
Robust linear estimator fitting
TheilSenRegressoris comparable to theOrdinary Least Squares
(OLS)in terms of asymptotic efficiency and as an
unbiased estimator. In contrast to OLS, Theil-Sen is a non-parametric
method which means it makes no assumption about the underlying
distribution of the data. Since Theil-Sen is a median-based estimator, it
is more robust against corrupted data aka outliers. In univariate
setting, Theil-Sen has a breakdown point of about 29.3% in case of a
simple linear regression which means that it can tolerate arbitrary
corrupted data of up to 29.3%.
TheilSenRegressor
The implementation ofTheilSenRegressorin scikit-learn follows a
generalization to a multivariate linear regression model[14]using the
spatial median which is a generalization of the median to multiple
dimensions[15].
TheilSenRegressor
In terms of time and space complexity, Theil-Sen scales according to
which makes it infeasible to be applied exhaustively to problems with a
large number of samples and features. Therefore, the magnitude of a
subpopulation can be chosen to limit the time and space complexity by
considering only a random subset of all possible combinations.
References
Xin Dang, Hanxiang Peng, Xueqin Wang and Heping Zhang:Theil-Sen Estimators in a Multiple Linear Regression Model.
Kärkkäinen and S. Äyrämö:On Computation of Spatial Median for Robust Data Mining.
Kärkkäinen and S. Äyrämö:On Computation of Spatial Median for Robust Data Mining.
Also see theWikipedia page
1.1.16.4.Huber Regression#
TheHuberRegressoris different fromRidgebecause it applies a
linear loss to samples that are defined as outliers by theepsilonparameter.
A sample is classified as an inlier if the absolute error of that sample is
lesser than the thresholdepsilon. It differs fromTheilSenRegressorandRANSACRegressorbecause it does not ignore the effect of the outliers
but gives a lesser weight to them.
HuberRegressor
Ridge
epsilon
epsilon
TheilSenRegressor
RANSACRegressor
Examples
HuberRegressor vs Ridge on dataset with strong outliers
HuberRegressor vs Ridge on dataset with strong outliers
HuberRegressorminimizes
HuberRegressor
where the loss function is given by
It is advised to set the parameterepsilonto 1.35 to achieve 95%
statistical efficiency.
epsilon
References
Peter J. Huber, Elvezio M. Ronchetti: Robust Statistics, Concomitant scale
estimates, p. 172.
Peter J. Huber, Elvezio M. Ronchetti: Robust Statistics, Concomitant scale
estimates, p. 172.
TheHuberRegressordiffers from usingSGDRegressorwith loss set tohuberin the following ways.
HuberRegressor
SGDRegressor
huber
HuberRegressoris scaling invariant. Onceepsilonis set, scalingXandydown or up by different values would produce the same robustness to outliers as before.
as compared toSGDRegressorwhereepsilonhas to be set again whenXandyare
scaled.
HuberRegressoris scaling invariant. Onceepsilonis set, scalingXandydown or up by different values would produce the same robustness to outliers as before.
as compared toSGDRegressorwhereepsilonhas to be set again whenXandyare
scaled.
HuberRegressor
epsilon
X
y
SGDRegressor
epsilon
X
y
HuberRegressorshould be more efficient to use on data with small number of
samples whileSGDRegressorneeds a number of passes on the training data to
produce the same robustness.
HuberRegressorshould be more efficient to use on data with small number of
samples whileSGDRegressorneeds a number of passes on the training data to
produce the same robustness.
HuberRegressor
SGDRegressor
Note that this estimator is different from theR implementation of Robust
Regressionbecause the R
implementation does a weighted least squares implementation with weights given to each
sample on the basis of how much the residual is greater than a certain threshold.
1.1.17.Quantile Regression#
Quantile regression estimates the median or other quantiles of\(y\)conditional on\(X\), while ordinary least squares (OLS) estimates the
conditional mean.
Quantile regression may be useful if one is interested in predicting an
interval instead of point prediction. Sometimes, prediction intervals are
calculated based on the assumption that prediction error is distributed
normally with zero mean and constant variance. Quantile regression provides
sensible prediction intervals even for errors with non-constant (but
predictable) variance or non-normal distribution.
Based on minimizing the pinball loss, conditional quantiles can also be
estimated by models other than linear models. For example,GradientBoostingRegressorcan predict conditional
quantiles if its parameterlossis set to"quantile"and parameteralphais set to the quantile that should be predicted. See the example inPrediction Intervals for Gradient Boosting Regression.
GradientBoostingRegressor
loss
"quantile"
alpha
Most implementations of quantile regression are based on linear programming
problem. The current implementation is based onscipy.optimize.linprog.
scipy.optimize.linprog
Examples
Quantile regression
Quantile regression
As a linear model, theQuantileRegressorgives linear predictions\(\hat{y}(w, X) = Xw\)for the\(q\)-th quantile,\(q \in (0, 1)\).
The weights or coefficients\(w\)are then found by the following
minimization problem:
QuantileRegressor
This consists of the pinball loss (also known as linear loss),
see alsomean_pinball_loss,
mean_pinball_loss
and the L1 penalty controlled by parameteralpha, similar toLasso.
alpha
Lasso
As the pinball loss is only linear in the residuals, quantile regression is
much more robust to outliers than squared error based estimation of the mean.
Somewhat in between is theHuberRegressor.
HuberRegressor
Koenker, R., & Bassett Jr, G. (1978).Regression quantiles.Econometrica: journal of the Econometric Society, 33-50.
Koenker, R., & Bassett Jr, G. (1978).Regression quantiles.Econometrica: journal of the Econometric Society, 33-50.
Portnoy, S., & Koenker, R. (1997).The Gaussian hare and the Laplacian
tortoise: computability of squared-error versus absolute-error estimators.
Statistical Science, 12, 279-300.
Portnoy, S., & Koenker, R. (1997).The Gaussian hare and the Laplacian
tortoise: computability of squared-error versus absolute-error estimators.
Statistical Science, 12, 279-300.
Koenker, R. (2005).Quantile Regression.
Cambridge University Press.
Koenker, R. (2005).Quantile Regression.
Cambridge University Press.
1.1.18.Polynomial regression: extending linear models with basis functions#
One common pattern within machine learning is to use linear models trained
on nonlinear functions of the data.  This approach maintains the generally
fast performance of linear methods, while allowing them to fit a much wider
range of data.
For example, a simple linear regression can be extended by constructingpolynomial featuresfrom the coefficients.  In the standard linear
regression case, you might have a model that looks like this for
two-dimensional data:
If we want to fit a paraboloid to the data instead of a plane, we can combine
the features in second-order polynomials, so that the model looks like this:
The (sometimes surprising) observation is that this isstill a linear model:
to see this, imagine creating a new set of features
With this re-labeling of the data, our problem can be written
We see that the resultingpolynomial regressionis in the same class of
linear models we considered above (i.e. the model is linear in\(w\))
and can be solved by the same techniques.  By considering linear fits within
a higher-dimensional space built with these basis functions, the model has the
flexibility to fit a much broader range of data.
Here is an example of applying this idea to one-dimensional data, using
polynomial features of varying degrees:
This figure is created using thePolynomialFeaturestransformer, which
transforms an input data matrix into a new data matrix of a given degree.
It can be used as follows:
PolynomialFeatures
>>>fromsklearn.preprocessingimportPolynomialFeatures>>>importnumpyasnp>>>X=np.arange(6).reshape(3,2)>>>Xarray([[0, 1],[2, 3],[4, 5]])>>>poly=PolynomialFeatures(degree=2)>>>poly.fit_transform(X)array([[ 1.,  0.,  1.,  0.,  0.,  1.],[ 1.,  2.,  3.,  4.,  6.,  9.],[ 1.,  4.,  5., 16., 20., 25.]])
The features ofXhave been transformed from\([x_1, x_2]\)to\([1, x_1, x_2, x_1^2, x_1 x_2, x_2^2]\), and can now be used within
any linear model.
X
This sort of preprocessing can be streamlined with thePipelinetools. A single object representing a simple
polynomial regression can be created and used as follows:
>>>fromsklearn.preprocessingimportPolynomialFeatures>>>fromsklearn.linear_modelimportLinearRegression>>>fromsklearn.pipelineimportPipeline>>>importnumpyasnp>>>model=Pipeline([('poly',PolynomialFeatures(degree=3)),...('linear',LinearRegression(fit_intercept=False))])>>># fit to an order-3 polynomial data>>>x=np.arange(5)>>>y=3-2*x+x**2-x**3>>>model=model.fit(x[:,np.newaxis],y)>>>model.named_steps['linear'].coef_array([ 3., -2.,  1., -1.])
The linear model trained on polynomial features is able to exactly recover
the input polynomial coefficients.
In some cases it’s not necessary to include higher powers of any single feature,
but only the so-calledinteraction featuresthat multiply together at most\(d\)distinct features.
These can be gotten fromPolynomialFeatureswith the settinginteraction_only=True.
PolynomialFeatures
interaction_only=True
For example, when dealing with boolean features,\(x_i^n = x_i\)for all\(n\)and is therefore useless;
but\(x_i x_j\)represents the conjunction of two booleans.
This way, we can solve the XOR problem with a linear classifier:
>>>fromsklearn.linear_modelimportPerceptron>>>fromsklearn.preprocessingimportPolynomialFeatures>>>importnumpyasnp>>>X=np.array([[0,0],[0,1],[1,0],[1,1]])>>>y=X[:,0]^X[:,1]>>>yarray([0, 1, 1, 0])>>>X=PolynomialFeatures(interaction_only=True).fit_transform(X).astype(int)>>>Xarray([[1, 0, 0, 0],[1, 0, 1, 0],[1, 1, 0, 0],[1, 1, 1, 1]])>>>clf=Perceptron(fit_intercept=False,max_iter=10,tol=None,...shuffle=False).fit(X,y)
And the classifier “predictions” are perfect:
>>>clf.predict(X)array([0, 1, 1, 0])>>>clf.score(X,y)1.0
This Page
Show Source

GitHub
GitHub
1.2.Linear and Quadratic Discriminant Analysis#
Linear Discriminant Analysis
(LinearDiscriminantAnalysis) and Quadratic
Discriminant Analysis
(QuadraticDiscriminantAnalysis) are two classic
classifiers, with, as their names suggest, a linear and a quadratic decision
surface, respectively.
LinearDiscriminantAnalysis
QuadraticDiscriminantAnalysis
These classifiers are attractive because they have closed-form solutions that
can be easily computed, are inherently multiclass, have proven to work well in
practice, and have no hyperparameters to tune.
The plot shows decision boundaries for Linear Discriminant Analysis and
Quadratic Discriminant Analysis. The bottom row demonstrates that Linear
Discriminant Analysis can only learn linear boundaries, while Quadratic
Discriminant Analysis can learn quadratic boundaries and is therefore more
flexible.
Examples
Linear and Quadratic Discriminant Analysis with covariance ellipsoid: Comparison of LDA and
QDA on synthetic data.
Linear and Quadratic Discriminant Analysis with covariance ellipsoid: Comparison of LDA and
QDA on synthetic data.
1.2.1.Dimensionality reduction using Linear Discriminant Analysis#
LinearDiscriminantAnalysiscan be used to
perform supervised dimensionality reduction, by projecting the input data to a
linear subspace consisting of the directions which maximize the separation
between classes (in a precise sense discussed in the mathematics section
below). The dimension of the output is necessarily less than the number of
classes, so this is in general a rather strong dimensionality reduction, and
only makes sense in a multiclass setting.
LinearDiscriminantAnalysis
This is implemented in thetransformmethod. The desired dimensionality can
be set using then_componentsparameter. This parameter has no influence
on thefitandpredictmethods.
transform
n_components
fit
predict
Examples
Comparison of LDA and PCA 2D projection of Iris dataset: Comparison of LDA and
PCA for dimensionality reduction of the Iris dataset
Comparison of LDA and PCA 2D projection of Iris dataset: Comparison of LDA and
PCA for dimensionality reduction of the Iris dataset
1.2.2.Mathematical formulation of the LDA and QDA classifiers#
Both LDA and QDA can be derived from simple probabilistic models which model
the class conditional distribution of the data\(P(X|y=k)\)for each class\(k\). Predictions can then be obtained by using Bayes’ rule, for each
training sample\(x \in \mathcal{R}^d\):
and we select the class\(k\)which maximizes this posterior probability.
More specifically, for linear and quadratic discriminant analysis,\(P(x|y)\)is modeled as a multivariate Gaussian distribution with
density:
where\(d\)is the number of features.
1.2.2.1.QDA#
According to the model above, the log of the posterior is:
where the constant term\(Cst\)corresponds to the denominator\(P(x)\), in addition to other constant terms from the Gaussian. The
predicted class is the one that maximises this log-posterior.
Note
Relation with Gaussian Naive Bayes
If in the QDA model one assumes that the covariance matrices are diagonal,
then the inputs are assumed to be conditionally independent in each class,
and the resulting classifier is equivalent to the Gaussian Naive Bayes
classifiernaive_bayes.GaussianNB.
naive_bayes.GaussianNB
1.2.2.2.LDA#
LDA is a special case of QDA, where the Gaussians for each class are assumed
to share the same covariance matrix:\(\Sigma_k = \Sigma\)for all\(k\). This reduces the log posterior to:
The term\((x-\mu_k)^t \Sigma^{-1} (x-\mu_k)\)corresponds to theMahalanobis Distancebetween the sample\(x\)and the mean\(\mu_k\). The Mahalanobis
distance tells how close\(x\)is from\(\mu_k\), while also
accounting for the variance of each feature. We can thus interpret LDA as
assigning\(x\)to the class whose mean is the closest in terms of
Mahalanobis distance, while also accounting for the class prior
probabilities.
The log-posterior of LDA can also be written[3]as:
where\(\omega_k = \Sigma^{-1} \mu_k\)and\(\omega_{k0} =
-\frac{1}{2} \mu_k^t\Sigma^{-1}\mu_k + \log P (y = k)\). These quantities
correspond to thecoef_andintercept_attributes, respectively.
coef_
intercept_
From the above formula, it is clear that LDA has a linear decision surface.
In the case of QDA, there are no assumptions on the covariance matrices\(\Sigma_k\)of the Gaussians, leading to quadratic decision surfaces.
See[1]for more details.
1.2.3.Mathematical formulation of LDA dimensionality reduction#
First note that the K means\(\mu_k\)are vectors in\(\mathcal{R}^d\), and they lie in an affine subspace\(H\)of
dimension at most\(K - 1\)(2 points lie on a line, 3 points lie on a
plane, etc.).
As mentioned above, we can interpret LDA as assigning\(x\)to the class
whose mean\(\mu_k\)is the closest in terms of Mahalanobis distance,
while also accounting for the class prior probabilities. Alternatively, LDA
is equivalent to firstspheringthe data so that the covariance matrix is
the identity, and then assigning\(x\)to the closest mean in terms of
Euclidean distance (still accounting for the class priors).
Computing Euclidean distances in this d-dimensional space is equivalent to
first projecting the data points into\(H\), and computing the distances
there (since the other dimensions will contribute equally to each class in
terms of distance). In other words, if\(x\)is closest to\(\mu_k\)in the original space, it will also be the case in\(H\).
This shows that, implicit in the LDA
classifier, there is a dimensionality reduction by linear projection onto a\(K-1\)dimensional space.
We can reduce the dimension even more, to a chosen\(L\), by projecting
onto the linear subspace\(H_L\)which maximizes the variance of the\(\mu^*_k\)after projection (in effect, we are doing a form of PCA for the
transformed class means\(\mu^*_k\)). This\(L\)corresponds to then_componentsparameter used in thetransformmethod. See[1]for more details.
n_components
transform
1.2.4.Shrinkage and Covariance Estimator#
Shrinkage is a form of regularization used to improve the estimation of
covariance matrices in situations where the number of training samples is
small compared to the number of features.
In this scenario, the empirical sample covariance is a poor
estimator, and shrinkage helps improving the generalization performance of
the classifier.
Shrinkage LDA can be used by setting theshrinkageparameter of
theLinearDiscriminantAnalysisclass to ‘auto’.
This automatically determines the optimal shrinkage parameter in an analytic
way following the lemma introduced by Ledoit and Wolf[2]. Note that
currently shrinkage only works when setting thesolverparameter to ‘lsqr’
or ‘eigen’.
shrinkage
LinearDiscriminantAnalysis
solver
Theshrinkageparameter can also be manually set between 0 and 1. In
particular, a value of 0 corresponds to no shrinkage (which means the empirical
covariance matrix will be used) and a value of 1 corresponds to complete
shrinkage (which means that the diagonal matrix of variances will be used as
an estimate for the covariance matrix). Setting this parameter to a value
between these two extrema will estimate a shrunk version of the covariance
matrix.
shrinkage
The shrunk Ledoit and Wolf estimator of covariance may not always be the
best choice. For example if the distribution of the data
is normally distributed, the
Oracle Approximating Shrinkage estimatorsklearn.covariance.OASyields a smaller Mean Squared Error than the one given by Ledoit and Wolf’s
formula used with shrinkage=”auto”. In LDA, the data are assumed to be gaussian
conditionally to the class. If these assumptions hold, using LDA with
the OAS estimator of covariance will yield a better classification
accuracy than if Ledoit and Wolf or the empirical covariance estimator is used.
sklearn.covariance.OAS
The covariance estimator can be chosen using with thecovariance_estimatorparameter of thediscriminant_analysis.LinearDiscriminantAnalysisclass. A covariance estimator should have afitmethod and acovariance_attribute like all covariance estimators in thesklearn.covariancemodule.
covariance_estimator
discriminant_analysis.LinearDiscriminantAnalysis
covariance_
sklearn.covariance
Examples
Normal, Ledoit-Wolf and OAS Linear Discriminant Analysis for classification: Comparison of LDA classifiers
with Empirical, Ledoit Wolf and OAS covariance estimator.
Normal, Ledoit-Wolf and OAS Linear Discriminant Analysis for classification: Comparison of LDA classifiers
with Empirical, Ledoit Wolf and OAS covariance estimator.
1.2.5.Estimation algorithms#
Using LDA and QDA requires computing the log-posterior which depends on the
class priors\(P(y=k)\), the class means\(\mu_k\), and the
covariance matrices.
The ‘svd’ solver is the default solver used forLinearDiscriminantAnalysis, and it is
the only available solver forQuadraticDiscriminantAnalysis.
It can perform both classification and transform (for LDA).
As it does not rely on the calculation of the covariance matrix, the ‘svd’
solver may be preferable in situations where the number of features is large.
The ‘svd’ solver cannot be used with shrinkage.
For QDA, the use of the SVD solver relies on the fact that the covariance
matrix\(\Sigma_k\)is, by definition, equal to\(\frac{1}{n - 1}
X_k^tX_k = \frac{1}{n - 1} V S^2 V^t\)where\(V\)comes from the SVD of the (centered)
matrix:\(X_k = U S V^t\). It turns out that we can compute the
log-posterior above without having to explicitly compute\(\Sigma\):
computing\(S\)and\(V\)via the SVD of\(X\)is enough. For
LDA, two SVDs are computed: the SVD of the centered input matrix\(X\)and the SVD of the class-wise mean vectors.
LinearDiscriminantAnalysis
QuadraticDiscriminantAnalysis
The ‘lsqr’ solver is an efficient algorithm that only works for
classification. It needs to explicitly compute the covariance matrix\(\Sigma\), and supports shrinkage and custom covariance estimators.
This solver computes the coefficients\(\omega_k = \Sigma^{-1}\mu_k\)by solving for\(\Sigma \omega =
\mu_k\), thus avoiding the explicit computation of the inverse\(\Sigma^{-1}\).
The ‘eigen’ solver is based on the optimization of the between class scatter to
within class scatter ratio. It can be used for both classification and
transform, and it supports shrinkage. However, the ‘eigen’ solver needs to
compute the covariance matrix, so it might not be suitable for situations with
a high number of features.
References
“The Elements of Statistical Learning”, Hastie T., Tibshirani R.,
Friedman J., Section 4.3, p.106-119, 2008.
Ledoit O, Wolf M. Honey, I Shrunk the Sample Covariance Matrix.
The Journal of Portfolio Management 30(4), 110-119, 2004.
R. O. Duda, P. E. Hart, D. G. Stork. Pattern Classification
(Second Edition), section 2.6.2.
This Page
Show Source

GitHub
GitHub
1.3.Kernel ridge regression#
Kernel ridge regression (KRR)[M2012]combinesRidge regression and classification(linear least squares with l2-norm regularization) with thekernel trick. It thus learns a linear
function in the space induced by the respective kernel and the data. For
non-linear kernels, this corresponds to a non-linear function in the original
space.
The form of the model learned byKernelRidgeis identical to support
vector regression (SVR). However, different loss
functions are used: KRR uses squared error loss while support vector
regression uses\(\epsilon\)-insensitive loss, both combined with l2
regularization. In contrast toSVR, fittingKernelRidgecan be done in closed-form and is typically faster for
medium-sized datasets. On the other hand, the learned model is non-sparse and
thus slower thanSVR, which learns a sparse model for\(\epsilon > 0\), at prediction-time.
KernelRidge
SVR
SVR
KernelRidge
SVR
The following figure comparesKernelRidgeandSVRon an artificial dataset, which consists of a
sinusoidal target function and strong noise added to every fifth datapoint.
The learned model ofKernelRidgeandSVRis
plotted, where both complexity/regularization and bandwidth of the RBF kernel
have been optimized using grid-search. The learned functions are very
similar; however, fittingKernelRidgeis approximately seven times
faster than fittingSVR(both with grid-search).
However, prediction of 100000 target values is more than three times faster
withSVRsince it has learned a sparse model using only
approximately 1/3 of the 100 training datapoints as support vectors.
KernelRidge
SVR
KernelRidge
SVR
KernelRidge
SVR
SVR
The next figure compares the time for fitting and prediction ofKernelRidgeandSVRfor different sizes of the
training set. FittingKernelRidgeis faster thanSVRfor medium-sized training sets (less than 1000
samples); however, for larger training setsSVRscales
better. With regard to prediction time,SVRis faster
thanKernelRidgefor all sizes of the training set because of the
learned sparse solution. Note that the degree of sparsity and thus the
prediction time depends on the parameters\(\epsilon\)and\(C\)of
theSVR;\(\epsilon = 0\)would correspond to a
dense model.
KernelRidge
SVR
KernelRidge
SVR
SVR
SVR
KernelRidge
SVR
Examples
Comparison of kernel ridge regression and SVR
Comparison of kernel ridge regression and SVR
References
“Machine Learning: A Probabilistic Perspective”
Murphy, K. P. - chapter 14.4.3, pp. 492-493, The MIT Press, 2012
This Page
Show Source

GitHub
GitHub
1.4.Support Vector Machines#
Support vector machines (SVMs)are a set of supervised learning
methods used forclassification,regressionandoutliers detection.
The advantages of support vector machines are:
Effective in high dimensional spaces.
Effective in high dimensional spaces.
Still effective in cases where number of dimensions is greater
than the number of samples.
Still effective in cases where number of dimensions is greater
than the number of samples.
Uses a subset of training points in the decision function (called
support vectors), so it is also memory efficient.
Uses a subset of training points in the decision function (called
support vectors), so it is also memory efficient.
Versatile: differentKernel functionscan be
specified for the decision function. Common kernels are
provided, but it is also possible to specify custom kernels.
Versatile: differentKernel functionscan be
specified for the decision function. Common kernels are
provided, but it is also possible to specify custom kernels.
The disadvantages of support vector machines include:
If the number of features is much greater than the number of
samples, avoid over-fitting in choosingKernel functionsand regularization
term is crucial.
If the number of features is much greater than the number of
samples, avoid over-fitting in choosingKernel functionsand regularization
term is crucial.
SVMs do not directly provide probability estimates, these are
calculated using an expensive five-fold cross-validation
(seeScores and probabilities, below).
SVMs do not directly provide probability estimates, these are
calculated using an expensive five-fold cross-validation
(seeScores and probabilities, below).
The support vector machines in scikit-learn support both dense
(numpy.ndarrayand convertible to that bynumpy.asarray) and
sparse (anyscipy.sparse) sample vectors as input. However, to use
an SVM to make predictions for sparse data, it must have been fit on such
data. For optimal performance, use C-orderednumpy.ndarray(dense) orscipy.sparse.csr_matrix(sparse) withdtype=float64.
numpy.ndarray
numpy.asarray
scipy.sparse
numpy.ndarray
scipy.sparse.csr_matrix
dtype=float64
1.4.1.Classification#
SVC,NuSVCandLinearSVCare classes
capable of performing binary and multi-class classification on a dataset.
SVC
NuSVC
LinearSVC
SVCandNuSVCare similar methods, but accept slightly
different sets of parameters and have different mathematical formulations (see
sectionMathematical formulation). On the other hand,LinearSVCis another (faster) implementation of Support Vector
Classification for the case of a linear kernel. It also
lacks some of the attributes ofSVCandNuSVC, likesupport_.LinearSVCusessquared_hingeloss and due to its
implementation inliblinearit also regularizes the intercept, if considered.
This effect can however be reduced by carefully fine tuning itsintercept_scalingparameter, which allows the intercept term to have a
different regularization behavior compared to the other features. The
classification results and score can therefore differ from the other two
classifiers.
SVC
NuSVC
LinearSVC
SVC
NuSVC
support_
LinearSVC
squared_hinge
liblinear
intercept_scaling
As other classifiers,SVC,NuSVCandLinearSVCtake as input two arrays: an arrayXof shape(n_samples,n_features)holding the training samples, and an arrayyof
class labels (strings or integers), of shape(n_samples):
SVC
NuSVC
LinearSVC
X
(n_samples,n_features)
y
(n_samples)
>>>fromsklearnimportsvm>>>X=[[0,0],[1,1]]>>>y=[0,1]>>>clf=svm.SVC()>>>clf.fit(X,y)SVC()
After being fitted, the model can then be used to predict new values:
>>>clf.predict([[2.,2.]])array([1])
SVMs decision function (detailed in theMathematical formulation)
depends on some subset of the training data, called the support vectors. Some
properties of these support vectors can be found in attributessupport_vectors_,support_andn_support_:
support_vectors_
support_
n_support_
>>># get support vectors>>>clf.support_vectors_array([[0., 0.],[1., 1.]])>>># get indices of support vectors>>>clf.support_array([0, 1]...)>>># get number of support vectors for each class>>>clf.n_support_array([1, 1]...)
Examples
SVM: Maximum margin separating hyperplane
SVM: Maximum margin separating hyperplane
SVM-Anova: SVM with univariate feature selection
SVM-Anova: SVM with univariate feature selection
Plot classification probability
Plot classification probability
1.4.1.1.Multi-class classification#
SVCandNuSVCimplement the “one-versus-one”
approach for multi-class classification. In total,n_classes*(n_classes-1)/2classifiers are constructed and each one trains data from two classes.
To provide a consistent interface with other classifiers, thedecision_function_shapeoption allows to monotonically transform the
results of the “one-versus-one” classifiers to a “one-vs-rest” decision
function of shape(n_samples,n_classes), which is the default setting
of the parameter (default=’ovr’).
SVC
NuSVC
n_classes*(n_classes-1)/2
decision_function_shape
(n_samples,n_classes)
>>>X=[[0],[1],[2],[3]]>>>Y=[0,1,2,3]>>>clf=svm.SVC(decision_function_shape='ovo')>>>clf.fit(X,Y)SVC(decision_function_shape='ovo')>>>dec=clf.decision_function([[1]])>>>dec.shape[1]# 6 classes: 4*3/2 = 66>>>clf.decision_function_shape="ovr">>>dec=clf.decision_function([[1]])>>>dec.shape[1]# 4 classes4
On the other hand,LinearSVCimplements “one-vs-the-rest”
multi-class strategy, thus trainingn_classesmodels.
LinearSVC
n_classes
>>>lin_clf=svm.LinearSVC()>>>lin_clf.fit(X,Y)LinearSVC()>>>dec=lin_clf.decision_function([[1]])>>>dec.shape[1]4
SeeMathematical formulationfor a complete description of
the decision function.
Note that theLinearSVCalso implements an alternative multi-class
strategy, the so-called multi-class SVM formulated by Crammer and Singer[16], by using the optionmulti_class='crammer_singer'. In practice,
one-vs-rest classification is usually preferred, since the results are mostly
similar, but the runtime is significantly less.
LinearSVC
multi_class='crammer_singer'
For “one-vs-rest”LinearSVCthe attributescoef_andintercept_have the shape(n_classes,n_features)and(n_classes,)respectively.
Each row of the coefficients corresponds to one of then_classes“one-vs-rest” classifiers and similar for the intercepts, in the
order of the “one” class.
LinearSVC
coef_
intercept_
(n_classes,n_features)
(n_classes,)
n_classes
In the case of “one-vs-one”SVCandNuSVC, the layout of
the attributes is a little more involved. In the case of a linear
kernel, the attributescoef_andintercept_have the shape(n_classes*(n_classes-1)/2,n_features)and(n_classes*(n_classes-1)/2)respectively. This is similar to the layout forLinearSVCdescribed above, with each row now corresponding
to a binary classifier. The order for classes
0 to n is “0 vs 1”, “0 vs 2” , … “0 vs n”, “1 vs 2”, “1 vs 3”, “1 vs n”, . .
. “n-1 vs n”.
SVC
NuSVC
coef_
intercept_
(n_classes*(n_classes-1)/2,n_features)
(n_classes*(n_classes-1)/2)
LinearSVC
The shape ofdual_coef_is(n_classes-1,n_SV)with
a somewhat hard to grasp layout.
The columns correspond to the support vectors involved in any
of then_classes*(n_classes-1)/2“one-vs-one” classifiers.
Each support vectorvhas a dual coefficient in each of then_classes-1classifiers comparing the class ofvagainst another class.
Note that some, but not all, of these dual coefficients, may be zero.
Then_classes-1entries in each column are these dual coefficients,
ordered by the opposing class.
dual_coef_
(n_classes-1,n_SV)
n_classes*(n_classes-1)/2
v
n_classes-1
v
n_classes-1
This might be clearer with an example: consider a three class problem with
class 0 having three support vectors\(v^{0}_0, v^{1}_0, v^{2}_0\)and class 1 and 2 having two support vectors\(v^{0}_1, v^{1}_1\)and\(v^{0}_2, v^{1}_2\)respectively.  For each
support vector\(v^{j}_i\), there are two dual coefficients.  Let’s call
the coefficient of support vector\(v^{j}_i\)in the classifier between
classes\(i\)and\(k\)\(\alpha^{j}_{i,k}\).
Thendual_coef_looks like this:
dual_coef_
\(\alpha^{0}_{0,1}\)
\(\alpha^{1}_{0,1}\)
\(\alpha^{2}_{0,1}\)
\(\alpha^{0}_{1,0}\)
\(\alpha^{1}_{1,0}\)
\(\alpha^{0}_{2,0}\)
\(\alpha^{1}_{2,0}\)
\(\alpha^{0}_{0,2}\)
\(\alpha^{1}_{0,2}\)
\(\alpha^{2}_{0,2}\)
\(\alpha^{0}_{1,2}\)
\(\alpha^{1}_{1,2}\)
\(\alpha^{0}_{2,1}\)
\(\alpha^{1}_{2,1}\)
Coefficients
for SVs of class 0
Coefficients
for SVs of class 1
Coefficients
for SVs of class 2
Examples
Plot different SVM classifiers in the iris dataset
Plot different SVM classifiers in the iris dataset
1.4.1.2.Scores and probabilities#
Thedecision_functionmethod ofSVCandNuSVCgives
per-class scores for each sample (or a single score per sample in the binary
case). When the constructor optionprobabilityis set toTrue,
class membership probability estimates (from the methodspredict_probaandpredict_log_proba) are enabled. In the binary case, the probabilities are
calibrated using Platt scaling[9]: logistic regression on the SVM’s scores,
fit by an additional cross-validation on the training data.
In the multiclass case, this is extended as per[10].
decision_function
SVC
NuSVC
probability
True
predict_proba
predict_log_proba
Note
The same probability calibration procedure is available for all estimators
via theCalibratedClassifierCV(seeProbability calibration). In the case ofSVCandNuSVC, this
procedure is builtin inlibsvmwhich is used under the hood, so it does
not rely on scikit-learn’sCalibratedClassifierCV.
CalibratedClassifierCV
SVC
NuSVC
CalibratedClassifierCV
The cross-validation involved in Platt scaling
is an expensive operation for large datasets.
In addition, the probability estimates may be inconsistent with the scores:
the “argmax” of the scores may not be the argmax of the probabilities
the “argmax” of the scores may not be the argmax of the probabilities
in binary classification, a sample may be labeled bypredictas
belonging to the positive class even if the output ofpredict_probais
less than 0.5; and similarly, it could be labeled as negative even if the
output ofpredict_probais more than 0.5.
in binary classification, a sample may be labeled bypredictas
belonging to the positive class even if the output ofpredict_probais
less than 0.5; and similarly, it could be labeled as negative even if the
output ofpredict_probais more than 0.5.
predict
predict_proba
predict_proba
Platt’s method is also known to have theoretical issues.
If confidence scores are required, but these do not have to be probabilities,
then it is advisable to setprobability=Falseand usedecision_functioninstead ofpredict_proba.
probability=False
decision_function
predict_proba
Please note that whendecision_function_shape='ovr'andn_classes>2,
unlikedecision_function, thepredictmethod does not try to break ties
by default. You can setbreak_ties=Truefor the output ofpredictto be
the same asnp.argmax(clf.decision_function(...),axis=1), otherwise the
first class among the tied classes will always be returned; but have in mind
that it comes with a computational cost. SeeSVM Tie Breaking Examplefor an example on
tie breaking.
decision_function_shape='ovr'
n_classes>2
decision_function
predict
break_ties=True
predict
np.argmax(clf.decision_function(...),axis=1)
1.4.1.3.Unbalanced problems#
In problems where it is desired to give more importance to certain
classes or certain individual samples, the parametersclass_weightandsample_weightcan be used.
class_weight
sample_weight
SVC(but notNuSVC) implements the parameterclass_weightin thefitmethod. It’s a dictionary of the form{class_label:value}, where value is a floating point number > 0
that sets the parameterCof classclass_labeltoC*value.
The figure below illustrates the decision boundary of an unbalanced problem,
with and without weight correction.
SVC
NuSVC
class_weight
fit
{class_label:value}
C
class_label
C*value
SVC,NuSVC,SVR,NuSVR,LinearSVC,LinearSVRandOneClassSVMimplement also weights for
individual samples in thefitmethod through thesample_weightparameter.
Similar toclass_weight, this sets the parameterCfor the i-th
example toC*sample_weight[i], which will encourage the classifier to
get these samples right. The figure below illustrates the effect of sample
weighting on the decision boundary. The size of the circles is proportional
to the sample weights:
SVC
NuSVC
SVR
NuSVR
LinearSVC
LinearSVR
OneClassSVM
fit
sample_weight
class_weight
C
C*sample_weight[i]
Examples
SVM: Separating hyperplane for unbalanced classes
SVM: Separating hyperplane for unbalanced classes
SVM: Weighted samples
SVM: Weighted samples
1.4.2.Regression#
The method of Support Vector Classification can be extended to solve
regression problems. This method is called Support Vector Regression.
The model produced by support vector classification (as described
above) depends only on a subset of the training data, because the cost
function for building the model does not care about training points
that lie beyond the margin. Analogously, the model produced by Support
Vector Regression depends only on a subset of the training data,
because the cost function ignores samples whose prediction is close to their
target.
There are three different implementations of Support Vector Regression:SVR,NuSVRandLinearSVR.LinearSVRprovides a faster implementation thanSVRbut only considers the
linear kernel, whileNuSVRimplements a slightly different formulation
thanSVRandLinearSVR. Due to its implementation inliblinearLinearSVRalso regularizes the intercept, if considered.
This effect can however be reduced by carefully fine tuning itsintercept_scalingparameter, which allows the intercept term to have a
different regularization behavior compared to the other features. The
classification results and score can therefore differ from the other two
classifiers. SeeImplementation detailsfor further details.
SVR
NuSVR
LinearSVR
LinearSVR
SVR
NuSVR
SVR
LinearSVR
liblinear
LinearSVR
intercept_scaling
As with classification classes, the fit method will take as
argument vectors X, y, only that in this case y is expected to have
floating point values instead of integer values:
>>>fromsklearnimportsvm>>>X=[[0,0],[2,2]]>>>y=[0.5,2.5]>>>regr=svm.SVR()>>>regr.fit(X,y)SVR()>>>regr.predict([[1,1]])array([1.5])
Examples
Support Vector Regression (SVR) using linear and non-linear kernels
Support Vector Regression (SVR) using linear and non-linear kernels
1.4.3.Density estimation, novelty detection#
The classOneClassSVMimplements a One-Class SVM which is used in
outlier detection.
OneClassSVM
SeeNovelty and Outlier Detectionfor the description and usage of OneClassSVM.
1.4.4.Complexity#
Support Vector Machines are powerful tools, but their compute and
storage requirements increase rapidly with the number of training
vectors. The core of an SVM is a quadratic programming problem (QP),
separating support vectors from the rest of the training data. The QP
solver used by thelibsvm-based implementation scales between\(O(n_{features} \times n_{samples}^2)\)and\(O(n_{features} \times n_{samples}^3)\)depending on how efficiently
thelibsvmcache is used in practice (dataset dependent). If the data
is very sparse\(n_{features}\)should be replaced by the average number
of non-zero features in a sample vector.
For the linear case, the algorithm used inLinearSVCby theliblinearimplementation is much more
efficient than itslibsvm-basedSVCcounterpart and can
scale almost linearly to millions of samples and/or features.
LinearSVC
SVC
1.4.5.Tips on Practical Use#
Avoiding data copy: ForSVC,SVR,NuSVCandNuSVR, if the data passed to certain methods is not C-ordered
contiguous and double precision, it will be copied before calling the
underlying C implementation. You can check whether a given numpy array is
C-contiguous by inspecting itsflagsattribute.ForLinearSVC(andLogisticRegression) any input passed as a numpy
array will be copied and converted to theliblinearinternal sparse data
representation (double precision floats and int32 indices of non-zero
components). If you want to fit a large-scale linear classifier without
copying a dense numpy C-contiguous double precision array as input, we
suggest to use theSGDClassifierclass instead.  The objective
function can be configured to be almost the same as theLinearSVCmodel.
Avoiding data copy: ForSVC,SVR,NuSVCandNuSVR, if the data passed to certain methods is not C-ordered
contiguous and double precision, it will be copied before calling the
underlying C implementation. You can check whether a given numpy array is
C-contiguous by inspecting itsflagsattribute.
SVC
SVR
NuSVC
NuSVR
flags
ForLinearSVC(andLogisticRegression) any input passed as a numpy
array will be copied and converted to theliblinearinternal sparse data
representation (double precision floats and int32 indices of non-zero
components). If you want to fit a large-scale linear classifier without
copying a dense numpy C-contiguous double precision array as input, we
suggest to use theSGDClassifierclass instead.  The objective
function can be configured to be almost the same as theLinearSVCmodel.
LinearSVC
LogisticRegression
SGDClassifier
LinearSVC
Kernel cache size: ForSVC,SVR,NuSVCandNuSVR, the size of the kernel cache has a strong impact on run
times for larger problems.  If you have enough RAM available, it is
recommended to setcache_sizeto a higher value than the default of
200(MB), such as 500(MB) or 1000(MB).
Kernel cache size: ForSVC,SVR,NuSVCandNuSVR, the size of the kernel cache has a strong impact on run
times for larger problems.  If you have enough RAM available, it is
recommended to setcache_sizeto a higher value than the default of
200(MB), such as 500(MB) or 1000(MB).
SVC
SVR
NuSVC
NuSVR
cache_size
Setting C:Cis1by default and it’s a reasonable default
choice.  If you have a lot of noisy observations you should decrease it:
decreasing C corresponds to more regularization.LinearSVCandLinearSVRare less sensitive toCwhen
it becomes large, and prediction results stop improving after a certain
threshold. Meanwhile, largerCvalues will take more time to train,
sometimes up to 10 times longer, as shown in[11].
Setting C:Cis1by default and it’s a reasonable default
choice.  If you have a lot of noisy observations you should decrease it:
decreasing C corresponds to more regularization.
C
1
LinearSVCandLinearSVRare less sensitive toCwhen
it becomes large, and prediction results stop improving after a certain
threshold. Meanwhile, largerCvalues will take more time to train,
sometimes up to 10 times longer, as shown in[11].
LinearSVC
LinearSVR
C
C
Support Vector Machine algorithms are not scale invariant, soit
is highly recommended to scale your data. For example, scale each
attribute on the input vector X to [0,1] or [-1,+1], or standardize it
to have mean 0 and variance 1. Note that thesamescaling must be
applied to the test vector to obtain meaningful results. This can be done
easily by using aPipeline:>>>fromsklearn.pipelineimportmake_pipeline>>>fromsklearn.preprocessingimportStandardScaler>>>fromsklearn.svmimportSVC>>>clf=make_pipeline(StandardScaler(),SVC())See sectionPreprocessing datafor more details on scaling and
normalization.
Support Vector Machine algorithms are not scale invariant, soit
is highly recommended to scale your data. For example, scale each
attribute on the input vector X to [0,1] or [-1,+1], or standardize it
to have mean 0 and variance 1. Note that thesamescaling must be
applied to the test vector to obtain meaningful results. This can be done
easily by using aPipeline:
Pipeline
>>>fromsklearn.pipelineimportmake_pipeline>>>fromsklearn.preprocessingimportStandardScaler>>>fromsklearn.svmimportSVC>>>clf=make_pipeline(StandardScaler(),SVC())
See sectionPreprocessing datafor more details on scaling and
normalization.
Regarding theshrinkingparameter, quoting[12]:We found that if the
number of iterations is large, then shrinking can shorten the training
time. However, if we loosely solve the optimization problem (e.g., by
using a large stopping tolerance), the code without using shrinking may
be much faster
Regarding theshrinkingparameter, quoting[12]:We found that if the
number of iterations is large, then shrinking can shorten the training
time. However, if we loosely solve the optimization problem (e.g., by
using a large stopping tolerance), the code without using shrinking may
be much faster
shrinking
ParameternuinNuSVC/OneClassSVM/NuSVRapproximates the fraction of training errors and support vectors.
ParameternuinNuSVC/OneClassSVM/NuSVRapproximates the fraction of training errors and support vectors.
nu
NuSVC
OneClassSVM
NuSVR
InSVC, if the data is unbalanced (e.g. many
positive and few negative), setclass_weight='balanced'and/or try
different penalty parametersC.
InSVC, if the data is unbalanced (e.g. many
positive and few negative), setclass_weight='balanced'and/or try
different penalty parametersC.
SVC
class_weight='balanced'
C
Randomness of the underlying implementations: The underlying
implementations ofSVCandNuSVCuse a random number
generator only to shuffle the data for probability estimation (whenprobabilityis set toTrue). This randomness can be controlled
with therandom_stateparameter. Ifprobabilityis set toFalsethese estimators are not random andrandom_statehas no effect on the
results. The underlyingOneClassSVMimplementation is similar to
the ones ofSVCandNuSVC. As no probability estimation
is provided forOneClassSVM, it is not random.The underlyingLinearSVCimplementation uses a random number
generator to select features when fitting the model with a dual coordinate
descent (i.e. whendualis set toTrue). It is thus not uncommon
to have slightly different results for the same input data. If that
happens, try with a smallertolparameter. This randomness can also be
controlled with therandom_stateparameter. Whendualis
set toFalsethe underlying implementation ofLinearSVCis
not random andrandom_statehas no effect on the results.
Randomness of the underlying implementations: The underlying
implementations ofSVCandNuSVCuse a random number
generator only to shuffle the data for probability estimation (whenprobabilityis set toTrue). This randomness can be controlled
with therandom_stateparameter. Ifprobabilityis set toFalsethese estimators are not random andrandom_statehas no effect on the
results. The underlyingOneClassSVMimplementation is similar to
the ones ofSVCandNuSVC. As no probability estimation
is provided forOneClassSVM, it is not random.
SVC
NuSVC
probability
True
random_state
probability
False
random_state
OneClassSVM
SVC
NuSVC
OneClassSVM
The underlyingLinearSVCimplementation uses a random number
generator to select features when fitting the model with a dual coordinate
descent (i.e. whendualis set toTrue). It is thus not uncommon
to have slightly different results for the same input data. If that
happens, try with a smallertolparameter. This randomness can also be
controlled with therandom_stateparameter. Whendualis
set toFalsethe underlying implementation ofLinearSVCis
not random andrandom_statehas no effect on the results.
LinearSVC
dual
True
tol
random_state
dual
False
LinearSVC
random_state
Using L1 penalization as provided byLinearSVC(penalty='l1',dual=False)yields a sparse solution, i.e. only a subset of feature
weights is different from zero and contribute to the decision function.
IncreasingCyields a more complex model (more features are selected).
TheCvalue that yields a “null” model (all weights equal to zero) can
be calculated usingl1_min_c.
Using L1 penalization as provided byLinearSVC(penalty='l1',dual=False)yields a sparse solution, i.e. only a subset of feature
weights is different from zero and contribute to the decision function.
IncreasingCyields a more complex model (more features are selected).
TheCvalue that yields a “null” model (all weights equal to zero) can
be calculated usingl1_min_c.
LinearSVC(penalty='l1',dual=False)
C
C
l1_min_c
1.4.6.Kernel functions#
Thekernel functioncan be any of the following:
linear:\(\langle x, x'\rangle\).
linear:\(\langle x, x'\rangle\).
polynomial:\((\gamma \langle x, x'\rangle + r)^d\), where\(d\)is specified by parameterdegree,\(r\)bycoef0.
polynomial:\((\gamma \langle x, x'\rangle + r)^d\), where\(d\)is specified by parameterdegree,\(r\)bycoef0.
degree
coef0
rbf:\(\exp(-\gamma \|x-x'\|^2)\), where\(\gamma\)is
specified by parametergamma, must be greater than 0.
rbf:\(\exp(-\gamma \|x-x'\|^2)\), where\(\gamma\)is
specified by parametergamma, must be greater than 0.
gamma
sigmoid\(\tanh(\gamma \langle x,x'\rangle + r)\),
where\(r\)is specified bycoef0.
sigmoid\(\tanh(\gamma \langle x,x'\rangle + r)\),
where\(r\)is specified bycoef0.
coef0
Different kernels are specified by thekernelparameter:
kernel
>>>linear_svc=svm.SVC(kernel='linear')>>>linear_svc.kernel'linear'>>>rbf_svc=svm.SVC(kernel='rbf')>>>rbf_svc.kernel'rbf'
See alsoKernel Approximationfor a solution to use RBF kernels that is much faster and more scalable.
1.4.6.1.Parameters of the RBF Kernel#
When training an SVM with theRadial Basis Function(RBF) kernel, two
parameters must be considered:Candgamma.  The parameterC,
common to all SVM kernels, trades off misclassification of training examples
against simplicity of the decision surface. A lowCmakes the decision
surface smooth, while a highCaims at classifying all training examples
correctly.gammadefines how much influence a single training example has.
The largergammais, the closer other examples must be to be affected.
C
gamma
C
C
C
gamma
gamma
Proper choice ofCandgammais critical to the SVM’s performance.  One
is advised to useGridSearchCVwithCandgammaspaced exponentially far apart to choose good values.
C
gamma
GridSearchCV
C
gamma
Examples
RBF SVM parameters
RBF SVM parameters
Scaling the regularization parameter for SVCs
Scaling the regularization parameter for SVCs
1.4.6.2.Custom Kernels#
You can define your own kernels by either giving the kernel as a
python function or by precomputing the Gram matrix.
Classifiers with custom kernels behave the same way as any other
classifiers, except that:
Fieldsupport_vectors_is now empty, only indices of support
vectors are stored insupport_
Fieldsupport_vectors_is now empty, only indices of support
vectors are stored insupport_
support_vectors_
support_
A reference (and not a copy) of the first argument in thefit()method is stored for future reference. If that array changes between the
use offit()andpredict()you will have unexpected results.
A reference (and not a copy) of the first argument in thefit()method is stored for future reference. If that array changes between the
use offit()andpredict()you will have unexpected results.
fit()
fit()
predict()
You can use your own defined kernels by passing a function to thekernelparameter.
kernel
Your kernel must take as arguments two matrices of shape(n_samples_1,n_features),(n_samples_2,n_features)and return a kernel matrix of shape(n_samples_1,n_samples_2).
(n_samples_1,n_features)
(n_samples_2,n_features)
(n_samples_1,n_samples_2)
The following code defines a linear kernel and creates a classifier
instance that will use that kernel:
>>>importnumpyasnp>>>fromsklearnimportsvm>>>defmy_kernel(X,Y):...returnnp.dot(X,Y.T)...>>>clf=svm.SVC(kernel=my_kernel)
You can pass pre-computed kernels by using thekernel='precomputed'option. You should then pass Gram matrix instead of X to thefitandpredictmethods. The kernel values betweenalltraining vectors and the
test vectors must be provided:
kernel='precomputed'
fit
predict
>>>importnumpyasnp>>>fromsklearn.datasetsimportmake_classification>>>fromsklearn.model_selectionimporttrain_test_split>>>fromsklearnimportsvm>>>X,y=make_classification(n_samples=10,random_state=0)>>>X_train,X_test,y_train,y_test=train_test_split(X,y,random_state=0)>>>clf=svm.SVC(kernel='precomputed')>>># linear kernel computation>>>gram_train=np.dot(X_train,X_train.T)>>>clf.fit(gram_train,y_train)SVC(kernel='precomputed')>>># predict on training examples>>>gram_test=np.dot(X_test,X_train.T)>>>clf.predict(gram_test)array([0, 1, 0])
Examples
SVM with custom kernel
SVM with custom kernel
1.4.7.Mathematical formulation#
A support vector machine constructs a hyper-plane or set of hyper-planes in a
high or infinite dimensional space, which can be used for
classification, regression or other tasks. Intuitively, a good
separation is achieved by the hyper-plane that has the largest distance
to the nearest training data points of any class (so-called functional
margin), since in general the larger the margin the lower the
generalization error of the classifier. The figure below shows the decision
function for a linearly separable problem, with three samples on the
margin boundaries, called “support vectors”:
In general, when the problem isn’t linearly separable, the support vectors
are the sampleswithinthe margin boundaries.
We recommend[13]and[14]as good references for the theory and
practicalities of SVMs.
1.4.7.1.SVC#
Given training vectors\(x_i \in \mathbb{R}^p\), i=1,…, n, in two classes, and a
vector\(y \in \{1, -1\}^n\), our goal is to find\(w \in
\mathbb{R}^p\)and\(b \in \mathbb{R}\)such that the prediction given by\(\text{sign} (w^T\phi(x) + b)\)is correct for most samples.
SVC solves the following primal problem:
Intuitively, we’re trying to maximize the margin (by minimizing\(||w||^2 = w^Tw\)), while incurring a penalty when a sample is
misclassified or within the margin boundary. Ideally, the value\(y_i
(w^T \phi (x_i) + b)\)would be\(\geq 1\)for all samples, which
indicates a perfect prediction. But problems are usually not always perfectly
separable with a hyperplane, so we allow some samples to be at a distance\(\zeta_i\)from
their correct margin boundary. The penalty termCcontrols the strength of
this penalty, and as a result, acts as an inverse regularization parameter
(see note below).
C
The dual problem to the primal is
where\(e\)is the vector of all ones,
and\(Q\)is an\(n\)by\(n\)positive semidefinite matrix,\(Q_{ij} \equiv y_i y_j K(x_i, x_j)\), where\(K(x_i, x_j) = \phi (x_i)^T \phi (x_j)\)is the kernel. The terms\(\alpha_i\)are called the dual coefficients,
and they are upper-bounded by\(C\).
This dual representation highlights the fact that training vectors are
implicitly mapped into a higher (maybe infinite)
dimensional space by the function\(\phi\): seekernel trick.
Once the optimization problem is solved, the output ofdecision_functionfor a given sample\(x\)becomes:
and the predicted class correspond to its sign. We only need to sum over the
support vectors (i.e. the samples that lie within the margin) because the
dual coefficients\(\alpha_i\)are zero for the other samples.
These parameters can be accessed through the attributesdual_coef_which holds the product\(y_i \alpha_i\),support_vectors_which
holds the support vectors, andintercept_which holds the independent
term\(b\)
dual_coef_
support_vectors_
intercept_
Note
While SVM models derived fromlibsvmandliblinearuseCas
regularization parameter, most other estimators usealpha. The exact
equivalence between the amount of regularization of two models depends on
the exact objective function optimized by the model. For example, when the
estimator used isRidgeregression,
the relation between them is given as\(C = \frac{1}{alpha}\).
C
alpha
Ridge
The primal problem can be equivalently formulated as
where we make use of thehinge loss. This is the form that is
directly optimized byLinearSVC, but unlike the dual form, this one
does not involve inner products between samples, so the famous kernel trick
cannot be applied. This is why only the linear kernel is supported byLinearSVC(\(\phi\)is the identity function).
LinearSVC
LinearSVC
The\(\nu\)-SVC formulation[15]is a reparameterization of the\(C\)-SVC and therefore mathematically equivalent.
We introduce a new parameter\(\nu\)(instead of\(C\)) which
controls the number of support vectors andmargin errors:\(\nu \in (0, 1]\)is an upper bound on the fraction of margin errors and
a lower bound of the fraction of support vectors. A margin error corresponds
to a sample that lies on the wrong side of its margin boundary: it is either
misclassified, or it is correctly classified but does not lie beyond the
margin.
1.4.7.2.SVR#
Given training vectors\(x_i \in \mathbb{R}^p\), i=1,…, n, and a
vector\(y \in \mathbb{R}^n\)\(\varepsilon\)-SVR solves the following primal problem:
Here, we are penalizing samples whose prediction is at least\(\varepsilon\)away from their true target. These samples penalize the objective by\(\zeta_i\)or\(\zeta_i^*\), depending on whether their predictions
lie above or below the\(\varepsilon\)tube.
The dual problem is
where\(e\)is the vector of all ones,\(Q\)is an\(n\)by\(n\)positive semidefinite matrix,\(Q_{ij} \equiv K(x_i, x_j) = \phi (x_i)^T \phi (x_j)\)is the kernel. Here training vectors are implicitly mapped into a higher
(maybe infinite) dimensional space by the function\(\phi\).
The prediction is:
These parameters can be accessed through the attributesdual_coef_which holds the difference\(\alpha_i - \alpha_i^*\),support_vectors_which
holds the support vectors, andintercept_which holds the independent
term\(b\)
dual_coef_
support_vectors_
intercept_
The primal problem can be equivalently formulated as
where we make use of the epsilon-insensitive loss, i.e. errors of less than\(\varepsilon\)are ignored. This is the form that is directly optimized
byLinearSVR.
LinearSVR
1.4.8.Implementation details#
Internally, we uselibsvm[12]andliblinear[11]to handle all
computations. These libraries are wrapped using C and Cython.
For a description of the implementation and details of the algorithms
used, please refer to their respective papers.
References
Platt“Probabilistic outputs for SVMs and comparisons to
regularized likelihood methods”.
Wu, Lin and Weng,“Probability estimates for multi-class
classification by pairwise coupling”,
JMLR 5:975-1005, 2004.
Fan, Rong-En, et al.,“LIBLINEAR: A library for large linear classification.”,
Journal of machine learning research 9.Aug (2008): 1871-1874.
Chang and Lin,LIBSVM: A Library for Support Vector Machines.
Bishop,Pattern recognition and machine learning,
chapter 7 Sparse Kernel Machines
“A Tutorial on Support Vector Regression”Alex J. Smola, Bernhard Schölkopf - Statistics and Computing archive
Volume 14 Issue 3, August 2004, p. 199-222.
Schölkopf et. alNew Support Vector Algorithms
Crammer and SingerOn the Algorithmic Implementation ofMulticlass
Kernel-based Vector Machines, JMLR 2001.
This Page
Show Source